{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp machine_learning.note_linking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# machine_learning.note_linking\n",
    "> Functions for gathering note linking data and to use models trained with said data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import ast\n",
    "from abc import ABC, abstractmethod\n",
    "import copy\n",
    "from datasets import Dataset\n",
    "from enum import Enum\n",
    "from itertools import combinations\n",
    "from pathlib import Path\n",
    "from os import PathLike\n",
    "import random\n",
    "import re\n",
    "from typing import Literal, Optional, TypedDict, TypeVar, Union\n",
    "\n",
    "from fastcore.basics import patch\n",
    "import torch\n",
    "from transformers import Pipeline\n",
    "from jarowinkler import jarowinkler_similarity \n",
    "\n",
    "\n",
    "from trouver.helper import latex_str_is_likely_in_latex_str, latex_str_in_latex_str_fuzz_metric\n",
    "from trouver.helper.numbers import modify_int_by_at_most_at_most_offset, modify_int_by_at_most_at_most_value\n",
    "from trouver.helper.regex import find_regex_in_text, latex_indices\n",
    "from trouver.obsidian.file import MarkdownFile\n",
    "\n",
    "from trouver.helper.latex.augment import (\n",
    "    augment_text, choose_modification_methods_at_random, remove_font_styles_at_random, change_font_styles_at_random, change_greek_letters_at_random, remove_math_keywords, random_latex_command_removal, random_word_removal, dollar_sign_manipulation, random_char_modification\n",
    "    )\n",
    "from trouver.obsidian.footnotes import identify_available_footnote_numbers\n",
    "from trouver.obsidian.links import links_from_text, LinkType, ObsidianLink, MARKDOWNLINK_CAPTURE_PATTERN\n",
    "from trouver.personal_vault.information_notes import index_note_of_note\n",
    "from trouver.machine_learning.note_data import (\n",
    "    NoteLinkEnum, NoteData, note_data_order_cmp, randomly_modify, InfoNoteData, NotatNoteData, note_data_from_index_note, note_data_from_reference, find_reverse_links, get_main_note_content_of_notat_note_data, _note_data_from_vault_note_on_the_fly, _update_dict\n",
    "    )\n",
    "from trouver.notation.in_standard_info_note import notation_notes_linked_in_see_also_section\n",
    "from trouver.notation.parse import NotationNoteParsed, parse_notation_note, notation_in_note, main_of_notation\n",
    "from trouver.personal_vault.note_processing import process_standard_information_note, ProcessNoteError\n",
    "from trouver.personal_vault.note_type import (\n",
    "    PersonalNoteTypeEnum, assert_note_is_of_type, note_is_of_type, type_of_note\n",
    ")\n",
    "\n",
    "\n",
    "from trouver.personal_vault.notes import (\n",
    "    notes_linked_in_note,  notes_linked_in_notes_linked_in_note)\n",
    "from trouver.personal_vault.reference import index_note_for_reference, all_paths_to_notes_in_reference_folder\n",
    "from trouver.obsidian.vault import VaultNote\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sieve instances of pairs for building a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Sieving and Negative Sampling**\n",
    "\n",
    "A convenient feature of `Obsidian.md` is that notes can have linkts and embedded links to other notes. For mathematical text, such links help to remind oneself, for example, of the meaning of various definitions and notations that a particular note might depend on. We attempt to train and use machine learning [ML] models for link prediction, which should largely consist of two prediction tasks:\n",
    "1. Understand if one note should link to another (and what the general \"rationale\" for doing so is)\n",
    "2. Find where in the note a link should best be positioned.  \n",
    "\n",
    "When it comes to training a machine learning model for link prediction, creating a dataset is an asymmetry problem:\n",
    "\n",
    "1. Positive Instances are easy: If Note A links to Note B, that is a positive sample.\n",
    "2. Negative Instances are hard: If Note A does not link to Note B, does that mean they shouldn't link, or simply that the author hasn't added the link yet?\n",
    "\n",
    "To solve this, we use a Heuristic Sieving Strategy:\n",
    "1. The \"Well-Connected\" Heuristic\n",
    "\n",
    "We assume that notes with many existing links (both incoming and outgoing) are \"mature\" or \"well-focused.\" If a mature note does not link to another note, it is high-probability evidence of a true negative.\n",
    "\n",
    "- High Count Notes: High incoming links ($>4$) and valid outgoing links ($>2$). These are the \"anchors\" of our dataset.\n",
    "- Mid Count Notes: Moderate incoming links ($>2$) and outgoing links ($>1$).\n",
    "\n",
    "2. Hard Negative Mining (Similarity)\n",
    "\n",
    "Models often struggle to distinguish between distinct concepts that share similar names or notation (e.g., distinguishing $\\mathcal{F}$ the sheaf from $F$ the field).\n",
    "\n",
    "- Similar Notation Injection: We calculate the Jaro-Winkler distance between notation strings.\n",
    "\n",
    "- We deliberately inject pairs of notes with similar notations but no actual link into the dataset. This forces the model to learn context beyond just surface-level string similarity.\n",
    "\n",
    "3. Admissibility Filtering\n",
    "\n",
    "Finally, we filter pairs based on _auto tags. If a note has already been processed by an automated linker (_auto/links_added), we respect its current state to avoid retraining on model-generated noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- In practice, it is difficult to manually create all links that ought to be linked. In particular, while it can be easy to extract \"positive\" instances of links (by virtue of simply finding explicit links), it is more difficult to obtain \"negative\" instances of links with certainty. The general method for obtaining \"negative\" instances is nevertheless to randomly sample pairs of notes and consider such a pair as \"negative\" if there is no link between them; some notes are \"well focused\" on in practice (and in particular has many links to other notes); if there are no links between two such notes, then it is likely that there are not supposed to be links between them. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class NotePairData(TypedDict):\n",
    "    origin_note: NoteData\n",
    "    relied_note: NoteData\n",
    "    # linked_type: NoteLinkEnum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def link_types_for_note_pair_data(\n",
    "        pair_data: NotePairData\n",
    "        ) -> set[NoteLinkEnum]:\n",
    "    relied_note_name: str = pair_data['relied_note'].note_name\n",
    "    directly_linked_notes_from_origin = pair_data['origin_note'].directly_linked_notes\n",
    "    if relied_note_name in directly_linked_notes_from_origin:\n",
    "        return set(directly_linked_notes_from_origin[relied_note_name])\n",
    "    else:\n",
    "        return set([NoteLinkEnum.NO_LINK])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _high_count_note_data(\n",
    "        info_note_data: dict[str, InfoNoteData],\n",
    "        notat_note_data: dict[str, NotatNoteData],\n",
    "        ) -> tuple[set[str], set[str]]:\n",
    "    \"\"\"\n",
    "    Helper function to `sieve_note_data_pairs`.\n",
    "    \"\"\"\n",
    "\n",
    "    high_count_info_notes: set[str] = set([\n",
    "        name for name, data_point in info_note_data.items()\n",
    "        if len(data_point.reverse_linked_notes) > 4\n",
    "        and len(info_note_data[name].directly_linked_notes) > 2])\n",
    "    high_count_notat_notes: set[str] = set([\n",
    "        name for name, data_point in notat_note_data.items()\n",
    "        if len(data_point.reverse_linked_notes) > 4])\n",
    "\n",
    "    return (high_count_info_notes, high_count_notat_notes)\n",
    "\n",
    "\n",
    "def _mid_count_note_data(\n",
    "        info_note_data: dict[str, InfoNoteData],\n",
    "        notat_note_data: dict[str, NotatNoteData],\n",
    "        high_count_info_notes: set[str],\n",
    "        high_count_notat_notes: set[str],\n",
    "        ) -> tuple[set[str], set[str]]:\n",
    "    \"\"\"\n",
    "    Helper function to `sieve_note_data_pairs`.\n",
    "    \"\"\"\n",
    "\n",
    "    mid_count_info_notes: set[str] = set([\n",
    "        name for name, data_point in info_note_data.items()\n",
    "        if len(data_point.reverse_linked_notes) > 2\n",
    "        and len(info_note_data[name].directly_linked_notes) > 1\n",
    "        and name not in high_count_info_notes])\n",
    "    mid_count_notat_notes: list[str] = set([\n",
    "        name for name, data_point in notat_note_data.items()\n",
    "        if len(data_point.reverse_linked_notes) > 2\n",
    "        and name not in high_count_notat_notes])\n",
    "\n",
    "    return (mid_count_info_notes, mid_count_notat_notes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _positive_instances_from_high_or_mid_count_notes(\n",
    "        high_count_notes: set[str],\n",
    "        mid_count_notes: set[str],\n",
    "        note_data: dict[str, NoteData],\n",
    "        ) -> list[tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Helper function to `sieve_note_data_pairs`.\n",
    "    \"\"\"\n",
    "    chosen_pairs: list[tuple[str, str]] = []\n",
    "    # Get all \"positive\" note links from high count notes to high or mid count notes.\n",
    "    for high_count_note_name in list(high_count_notes):\n",
    "    # for high_count_note_name, high_count_data_point in high_count_notes.items():\n",
    "        high_count_data_point = note_data[high_count_note_name]\n",
    "        for other_note, _ in high_count_data_point.directly_linked_notes.items():\n",
    "            if other_note in high_count_notes or other_note in mid_count_notes:\n",
    "                chosen_pairs.append((high_count_note_name, other_note))\n",
    "    # Get all \"positive\" note links from mid count notes to high count notes.\n",
    "    for mid_count_note_name in list(mid_count_notes):\n",
    "        mid_count_data_point = note_data[mid_count_note_name]\n",
    "    # for mid_count_note_name, mid_count_data_point in mid_count_notes.items():\n",
    "        for other_note, _ in mid_count_data_point.directly_linked_notes.items():\n",
    "            if other_note in high_count_notes:\n",
    "                chosen_pairs.append((mid_count_note_name, other_note))\n",
    "    return chosen_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _negative_instances_from_high_or_mid_count_notes(\n",
    "        high_count_notes: set[str],\n",
    "        mid_count_notes: set[str],\n",
    "        note_data: dict[str, NoteData],\n",
    "        num_pairs: int # The approximate number of pairs to sample.\n",
    "        ) -> list[tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Get \"negative\" pair instances from high or mid count notes, i.e. pairs where\n",
    "    the origin note seem to not link to relied note.\n",
    "    \"\"\"\n",
    "    high_count_weights = [\n",
    "        (len(note_data[note_name].reverse_linked_notes)**0.5)\n",
    "        for note_name in list(high_count_notes)]\n",
    "    mid_count_weights = [\n",
    "        (len(note_data[note_name].reverse_linked_notes)**0.5)\n",
    "        for note_name in list(mid_count_notes)]\n",
    "    high_to_high_samples = int(0.5 * num_pairs)\n",
    "    high_to_mid_samples = int(0.25 * num_pairs)\n",
    "    mid_to_high_samples = int(0.25 * num_pairs)\n",
    "    high_count_notes_list = list(high_count_notes)\n",
    "    mid_count_notes_list = list(mid_count_notes)\n",
    "\n",
    "    sample_pairs: set[tuple[str, str]] = set()\n",
    "\n",
    "    origin_notes = random.choices(\n",
    "        high_count_notes_list, weights=high_count_weights, k=high_to_high_samples\n",
    "        ) if high_count_notes_list else []\n",
    "    relied_notes = random.choices(\n",
    "        high_count_notes_list, weights=high_count_weights, k=high_to_high_samples\n",
    "        ) if high_count_notes_list else []\n",
    "    for origin_note, relied_note in zip(origin_notes, relied_notes):\n",
    "        if (origin_note == relied_note\n",
    "                or relied_note in note_data[origin_note].directly_linked_notes):\n",
    "            continue\n",
    "        else:\n",
    "            sample_pairs.add((origin_note, relied_note))\n",
    "\n",
    "    origin_notes = random.choices(\n",
    "        high_count_notes_list, weights=high_count_weights, k=high_to_mid_samples\n",
    "        ) if high_count_notes_list else []\n",
    "    relied_notes = random.choices(\n",
    "        mid_count_notes_list, weights=mid_count_weights, k=high_to_mid_samples\n",
    "        ) if mid_count_notes_list else []\n",
    "    for origin_note, relied_note in zip(origin_notes, relied_notes):\n",
    "        if (origin_note == relied_note\n",
    "                or relied_note in note_data[origin_note].directly_linked_notes):\n",
    "            continue\n",
    "        else:\n",
    "            sample_pairs.add((origin_note, relied_note))\n",
    "\n",
    "    origin_notes = random.choices(\n",
    "        mid_count_notes_list, weights=mid_count_weights, k=mid_to_high_samples\n",
    "        ) if mid_count_notes_list else []\n",
    "    relied_notes = random.choices(\n",
    "        high_count_notes_list, weights=high_count_weights, k=mid_to_high_samples\n",
    "        ) if high_count_notes_list else []\n",
    "    for origin_note, relied_note in zip(origin_notes, relied_notes):\n",
    "        if (origin_note == relied_note\n",
    "                or relied_note in note_data[origin_note].directly_linked_notes):\n",
    "            continue\n",
    "        else:\n",
    "            sample_pairs.add((origin_note, relied_note))\n",
    "\n",
    "    return list(sample_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _similar_notation_pairs(\n",
    "        notat_note_data: dict[str, NotatNoteData],\n",
    "        # ) -> list[tuple[str, str]]:\n",
    "        ) -> dict[str, set[str]]: # The keys are names of notation notes and the values are sets of names of notation notes whose notations are similar to the one explained in the key notation note.\n",
    "    \"\"\"\n",
    "    Identify pairs of names of notation notes whose notations are similar.\n",
    "\n",
    "    Helper function to sieve_note_data_pairs.\n",
    "\n",
    "    The similarity is measured by Jaro-Winkler, which works well on short\n",
    "    strings.\n",
    "    \"\"\"\n",
    "    # jarowinkler = JaroWinkler()\n",
    "    # similar_notation_pairs: list[tuple[str, str]] = []\n",
    "    similar_notation_dict: dict[str, set[str]] = {}\n",
    "    for notat_name_1, notat_name_2 in combinations(notat_note_data, 2):\n",
    "        notat_data_1, notat_data_2 = notat_note_data[notat_name_1], notat_note_data[notat_name_2]\n",
    "        notat_str_1 = notat_data_1.parsed.notation_str\n",
    "        notat_str_2 = notat_data_2.parsed.notation_str\n",
    "        similarity = jarowinkler_similarity(notat_str_1, notat_str_2)\n",
    "        reverse_similarity = jarowinkler_similarity(notat_str_1[::-1], notat_str_2[::-1]) \n",
    "        if similarity > 0.9 or reverse_similarity > 0.9:\n",
    "            _update_dict(similar_notation_dict, notat_name_1, notat_name_2)\n",
    "            _update_dict(similar_notation_dict, notat_name_2, notat_name_1)\n",
    "    return similar_notation_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _random_pair_replacing_notation_notes_with_similar_notation_notes(\n",
    "        original_pair: tuple[str, str],\n",
    "        similar_notation_dict: set[str, set[str]]\n",
    "        ) -> tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Helper function to `_random_pair_replacing_notation_notes_with_similar_notation_notes`.\n",
    "    \"\"\"\n",
    "    origin_note_name = original_pair[0]\n",
    "    relied_note_name = original_pair[1]\n",
    "    if random.random() > 0.5:\n",
    "        if origin_note_name in similar_notation_dict:\n",
    "            origin_note_name = random.choice(list(similar_notation_dict[origin_note_name]))\n",
    "    if random.random() > 0.5:\n",
    "        if relied_note_name in similar_notation_dict:\n",
    "            relied_note_name = random.choice(list(similar_notation_dict[relied_note_name]))\n",
    "    return (origin_note_name, relied_note_name)\n",
    "    \n",
    "\n",
    "    \n",
    "def _pairs_with_notation_notes_replaced_with_similar_notation_notes(\n",
    "        sieved_pairs: set[tuple[str, str]],\n",
    "        count: int, # The approximate number of pairs to attempt to obtain.\n",
    "        similar_notation_dict: set[str, set[str]], # An output of `_similar_notation_pairs`\n",
    "        # notat_note_data: dict[str, NotatNoteData],\n",
    "        ) -> list[tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Return modified versions of entries of `sieved_pairs` drawn at random\n",
    "    where notation note names are replaced by names of notation notes whose \n",
    "    introduced notations are similar, in accordance to `similar_notation_dict`.\n",
    "\n",
    "    Helper function to `sieve_note_data_pairs`.\n",
    "    \"\"\"\n",
    "    sieved_pairs_list = list(sieved_pairs)\n",
    "    new_pairs: list[tuple[str, str]] = []\n",
    "    for _ in range(count):\n",
    "        original_pair = random.choice(sieved_pairs_list)\n",
    "        new_pair = _random_pair_replacing_notation_notes_with_similar_notation_notes(\n",
    "            original_pair, similar_notation_dict)\n",
    "        new_pairs.append(new_pair)\n",
    "    return new_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _pair_is_admissible(\n",
    "        origin_note: str,\n",
    "        relied_note: str,\n",
    "        note_data: dict[str, NoteData],\n",
    "        info_note_data: dict[str, InfoNoteData],\n",
    "        notat_note_data: dict[str, InfoNoteData],\n",
    "        ) -> bool:\n",
    "    origin_note_has_tags = note_data[origin_note].tags is not None\n",
    "    if not origin_note_has_tags:\n",
    "        return True\n",
    "    if (('_auto/links_added' in note_data[origin_note].tags and relied_note in info_note_data)\n",
    "            or ('_auto/notations_added' in note_data[origin_note].tags and relied_note in notat_note_data)):\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| export\n",
    "# def sieve_note_data_pairs(\n",
    "#         info_note_data: dict[str, InfoNoteData],\n",
    "#         notat_note_data: dict[str, NotatNoteData],\n",
    "#         ) -> list[NotePairData]:\n",
    "#     note_data: dict[str, NoteData] = {}\n",
    "#     note_data.update(info_note_data)\n",
    "#     note_data.update(notat_note_data)\n",
    "#     high_count_info_notes, high_count_notat_notes = _high_count_note_data(\n",
    "#         info_note_data, notat_note_data) # set[str]\n",
    "#     high_count_notes: set[str] = high_count_info_notes.union(high_count_notat_notes)\n",
    "#     mid_count_info_notes, mid_count_notat_notes = _mid_count_note_data(\n",
    "#         info_note_data, notat_note_data, high_count_info_notes, high_count_notat_notes)\n",
    "#     mid_count_notes: set[str] = mid_count_info_notes.union(mid_count_notat_notes)\n",
    "\n",
    "#     positive_pairs: list[tuple[str, str]] = _positive_instances_from_high_or_mid_count_notes(\n",
    "#         high_count_notes, mid_count_notes, note_data)\n",
    "#     negative_pairs: list[tuple[str, str]] = _negative_instances_from_high_or_mid_count_notes(\n",
    "#         high_count_notes, mid_count_notes, note_data, len(positive_pairs)*3)\n",
    "#     sieved_pairs: set[tuple[str, str]] = set(positive_pairs)\n",
    "#     sieved_pairs.update(negative_pairs)\n",
    "\n",
    "#     similar_notation_dict: dict[str, set[str]] = _similar_notation_pairs(notat_note_data)\n",
    "#     # similar_notation_names: list[tuple[str, str]] = _similar_notation_pairs(notat_note_data)\n",
    "#     sieved_pairs.update(_pairs_with_notation_notes_replaced_with_similar_notation_notes(\n",
    "#         sieved_pairs, len(positive_pairs), similar_notation_dict))\n",
    "#     sieved_pairs_list = list(sieved_pairs)\n",
    "#     note_pair_data_list: list[NotePairData] = []\n",
    "#     for origin_note, relied_note in sieved_pairs_list:\n",
    "#         if not _pair_is_admissible(\n",
    "#                 origin_note, relied_note, note_data, info_note_data, notat_note_data):\n",
    "#             continue\n",
    "#         note_pair_data_list.append(\n",
    "#             NotePairData(\n",
    "#                 origin_note=note_data[origin_note],\n",
    "#                 relied_note=note_data[relied_note]))\n",
    "#     return note_pair_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#| export\n",
    "def _classify_anchor_notes(\n",
    "        info_note_data: dict[str, InfoNoteData],\n",
    "        notat_note_data: dict[str, NotatNoteData]\n",
    "        ) -> tuple[set[str], set[str], set[str], set[str]]:\n",
    "    \"\"\"Hidden helper: Classify notes into High/Mid counts for sieving.\"\"\"\n",
    "    high_info, high_notat = _high_count_note_data(info_note_data, notat_note_data)\n",
    "    mid_info, mid_notat = _mid_count_note_data(info_note_data, notat_note_data, high_info, high_notat)\n",
    "    return high_info | high_notat, mid_info | mid_notat, high_info, high_notat\n",
    "\n",
    "def _gather_raw_pairs(\n",
    "        high_notes: set[str],\n",
    "        mid_notes: set[str],\n",
    "        note_data: dict[str, NoteData],\n",
    "        notat_note_data: dict[str, NotatNoteData]\n",
    "        ) -> set[tuple[str, str]]:\n",
    "    \"\"\"Hidden helper: Collect positive, negative, and similar-notation pairs.\"\"\"\n",
    "    pos_pairs = _positive_instances_from_high_or_mid_count_notes(high_notes, mid_notes, note_data)\n",
    "    neg_pairs = _negative_instances_from_high_or_mid_count_notes(\n",
    "        high_notes, mid_notes, note_data, num_pairs=len(pos_pairs)*3)\n",
    "    \n",
    "    sieved_pairs = set(pos_pairs) | set(neg_pairs)\n",
    "    \n",
    "    # Add hard negatives (similar notation)\n",
    "    sim_dict = _similar_notation_pairs(notat_note_data)\n",
    "    sim_pairs = _pairs_with_notation_notes_replaced_with_similar_notation_notes(\n",
    "        sieved_pairs, len(pos_pairs), sim_dict)\n",
    "    \n",
    "    return sieved_pairs | set(sim_pairs)\n",
    "\n",
    "def _filter_admissible_pairs(\n",
    "        candidate_pairs: set[tuple[str, str]],\n",
    "        note_data: dict[str, NoteData],\n",
    "        info_data: dict[str, InfoNoteData],\n",
    "        notat_data: dict[str, NotatNoteData]\n",
    "        ) -> list[NotePairData]:\n",
    "    \"\"\"Hidden helper: Convert valid raw pairs into NotePairData objects.\"\"\"\n",
    "    final_data = []\n",
    "    for origin, relied in candidate_pairs:\n",
    "        if _pair_is_admissible(origin, relied, note_data, info_data, notat_data):\n",
    "            final_data.append(NotePairData(\n",
    "                origin_note=note_data[origin], \n",
    "                relied_note=note_data[relied]))\n",
    "    return final_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def sieve_note_data_pairs(\n",
    "        info_note_data: dict[str, InfoNoteData], # Data for all standard information notes.\n",
    "        notat_note_data: dict[str, NotatNoteData] # Data for all notation notes.\n",
    "        ) -> list[NotePairData]: # A balanced list of note pairs for training.\n",
    "    \"\"\"\n",
    "    Constructs a balanced training dataset of note pairs by sampling positive links, \n",
    "    inferred negative links, and 'hard negative' pairs with similar notation.\n",
    "    \"\"\"\n",
    "    note_data = {**info_note_data, **notat_note_data}\n",
    "    \n",
    "    # 1. Identify \"Anchor\" notes (well-connected notes suitable for sampling)\n",
    "    high_notes, mid_notes, _, _ = _classify_anchor_notes(info_note_data, notat_note_data)\n",
    "    \n",
    "    # 2. Gather raw candidate pairs (Positives + Negatives + Similar Notation)\n",
    "    raw_pairs = _gather_raw_pairs(high_notes, mid_notes, note_data, notat_note_data)\n",
    "    \n",
    "    # 3. Filter for admissibility and format\n",
    "    return _filter_admissible_pairs(raw_pairs, note_data, info_note_data, notat_note_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sieving Note Pairs for Training Data\n",
    "\n",
    "The sieve_note_data_pairs function is the core pipeline for assembling a balanced dataset of note pairs. It solves the problem of \"implicit negatives\" by intelligently sampling pairs that likely shouldn't be linked.\n",
    "\n",
    "The Sieving Process:\n",
    "\n",
    "    1. Identify \"Anchor\" Notes: It classifies notes into \"High Count\" (well-connected) and \"Mid Count\" categories based on their link density. We assume these notes are mature enough that missing links are true negatives.\n",
    "\n",
    "    2. Gather Positives: It collects all existing valid links between these anchor notes.\n",
    "\n",
    "    3. Sample Negatives: It randomly samples pairs of anchor notes that are not currently linked. To balance classes, it samples ~3x as many negatives as positives.\n",
    "\n",
    "    4. Inject Hard Negatives: It adds pairs of notes with visually similar notation (e.g., Gal(L/K) vs Gal(F/E)) but no actual link. This forces the model to learn context, not just string similarity.\n",
    "\n",
    "    5. Filter Admissibility: Finally, it removes pairs where the origin note has already been auto-processed (_auto/links_added), preventing the model from training on its own prior predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 9 training pairs.\n",
      "------------------------------\n",
      "Positive Instances (Existing Links):\n",
      "  Info_Mid -> Info_High\n",
      "  Info_High -> Info_Mid\n",
      "  Info_High -> Notat_A\n",
      "\n",
      "Negative/Inferred Instances (No Link):\n",
      "  Notat_B -> Info_High\n",
      "  Notat_A -> Info_Mid\n",
      "  Notat_B -> Info_Mid\n"
     ]
    }
   ],
   "source": [
    "#| example\n",
    "from unittest.mock import MagicMock\n",
    "\n",
    "# 1. Setup Helper (Same as before, but ensure it sets necessary attributes)\n",
    "def mock_note_data(name, reverse_count=0, direct_links=None, tags=None, notation_str=\"\"):\n",
    "    m = MagicMock()\n",
    "    m.note_name = name\n",
    "    # Satisfy count thresholds with dummy sets\n",
    "    m.reverse_linked_notes = {f\"in_{i}\" for i in range(reverse_count)}\n",
    "    m.directly_linked_notes = direct_links if direct_links else {}\n",
    "    m.tags = tags\n",
    "    \n",
    "    # Setup Notation Data specifics\n",
    "    m.parsed = MagicMock()\n",
    "    m.parsed.notation_str = notation_str if notation_str else name\n",
    "    return m\n",
    "\n",
    "# 2. Create a \"Mature\" Dataset that satisfies Sieve Thresholds\n",
    "#\n",
    "# Thresholds Reminder:\n",
    "# - High Count Info:  >4 Reverse Links AND >2 Direct Links\n",
    "# - High Count Notat: >4 Reverse Links\n",
    "# - Mid Count Info:   >2 Reverse Links AND >1 Direct Link\n",
    "\n",
    "info_mock = {\n",
    "    # High Count Info Note (Anchor)\n",
    "    # 5 incoming, 3 outgoing -> Qualifies as High Count\n",
    "    \"Info_High\": mock_note_data(\n",
    "        \"Info_High\", \n",
    "        reverse_count=5, \n",
    "        direct_links={\"Notat_A\": 1, \"Notat_B\": 1, \"Info_Mid\": 1}\n",
    "    ),\n",
    "    \n",
    "    # Mid Count Info Note\n",
    "    # 3 incoming, 2 outgoing -> Qualifies as Mid Count\n",
    "    \"Info_Mid\": mock_note_data(\n",
    "        \"Info_Mid\", \n",
    "        reverse_count=3, \n",
    "        direct_links={\"Info_High\": 1, \"Notat_A\": 1}\n",
    "    )\n",
    "}\n",
    "\n",
    "notat_mock = {\n",
    "    # High Count Notation Note\n",
    "    # 5 incoming -> Qualifies as High Count\n",
    "    \"Notat_A\": mock_note_data(\n",
    "        \"Notat_A\", \n",
    "        reverse_count=5, \n",
    "        notation_str=\"Gal(L/K)\"\n",
    "    ),\n",
    "    \n",
    "    # Another Notation Note (For Negative Sampling)\n",
    "    # 5 incoming -> Qualifies as High Count\n",
    "    \"Notat_B\": mock_note_data(\n",
    "        \"Notat_B\", \n",
    "        reverse_count=5, \n",
    "        notation_str=\"Gal(F/E)\" # Similar string to Notat_A\n",
    "    )\n",
    "}\n",
    "\n",
    "# 3. Run the Sieve\n",
    "# This should now find:\n",
    "# - Positive pairs (e.g., Info_High -> Notat_A)\n",
    "# - Negative pairs (e.g., Notat_A -> Notat_B, since no link exists)\n",
    "# - Similar pairs (Notat_A <-> Notat_B due to string similarity)\n",
    "training_pairs = sieve_note_data_pairs(info_mock, notat_mock)\n",
    "\n",
    "# 4. Inspect Results\n",
    "print(f\"Generated {len(training_pairs)} training pairs.\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Group by type for clarity\n",
    "positives = []\n",
    "negatives = []\n",
    "\n",
    "for pair in training_pairs:\n",
    "    orig = pair['origin_note'].note_name\n",
    "    dest = pair['relied_note'].note_name\n",
    "    \n",
    "    # check if it was an existing link in our mock data\n",
    "    orig_data = info_mock.get(orig) or notat_mock.get(orig)\n",
    "    if dest in orig_data.directly_linked_notes:\n",
    "        positives.append(f\"{orig} -> {dest}\")\n",
    "    else:\n",
    "        negatives.append(f\"{orig} -> {dest}\")\n",
    "\n",
    "print(\"Positive Instances (Existing Links):\")\n",
    "for p in positives[:3]: print(f\"  {p}\")\n",
    "\n",
    "print(\"\\nNegative/Inferred Instances (No Link):\")\n",
    "for n in negatives[:3]: print(f\"  {n}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from unittest.mock import MagicMock, patch\n",
    "from fastcore.test import *\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Test 1: Link Type Extraction\n",
    "# ---------------------------------------------------------\n",
    "def test_link_types_extraction():\n",
    "    # Adjusted ENUM name to match your file's likely convention (no underscores)\n",
    "    origin = mock_note_data(\"Origin\", direct_links={\"Target\": {NoteLinkEnum.INFO_TO_INFO_IN_CONTENT}})\n",
    "    target = mock_note_data(\"Target\")\n",
    "    \n",
    "    pair_data = {\"origin_note\": origin, \"relied_note\": target}\n",
    "    \n",
    "    # Case 1: Link exists\n",
    "    types = link_types_for_note_pair_data(pair_data)\n",
    "    test_eq(types, {NoteLinkEnum.INFO_TO_INFO_IN_CONTENT})\n",
    "    \n",
    "    # Case 2: No link exists\n",
    "    pair_data['relied_note'].note_name = \"NonExistent\"\n",
    "    types = link_types_for_note_pair_data(pair_data)\n",
    "    test_eq(types, {NoteLinkEnum.NO_LINK})\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Test 2: High/Mid Count Classification\n",
    "# ---------------------------------------------------------\n",
    "def test_note_counting_logic():\n",
    "    # Setup data\n",
    "    info_data = {\n",
    "        \"A\": mock_note_data(\"A\", reverse_count=5, direct_links={\"x\":1, \"y\":2, \"z\":3}),\n",
    "        \"B\": mock_note_data(\"B\", reverse_count=3, direct_links={\"x\":1, \"y\":2}),\n",
    "        \"C\": mock_note_data(\"C\", reverse_count=1)\n",
    "    }\n",
    "    notat_data = {} \n",
    "\n",
    "    # Test High Count\n",
    "    high_info, _ = _high_count_note_data(info_data, notat_data)\n",
    "    test_eq(high_info, {\"A\"})\n",
    "    \n",
    "    # Test Mid Count (Must exclude High counts)\n",
    "    mid_info, _ = _mid_count_note_data(info_data, notat_data, high_info, set())\n",
    "    test_eq(mid_info, {\"B\"})\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Test 3: Positive Instance Extraction\n",
    "# ---------------------------------------------------------\n",
    "def test_positive_instance_extraction():\n",
    "    notes = {\n",
    "        \"A\": mock_note_data(\"A\", direct_links={\"B\": 1, \"C\": 1}),\n",
    "        \"B\": mock_note_data(\"B\", direct_links={\"A\": 1}),\n",
    "        \"C\": mock_note_data(\"C\")\n",
    "    }\n",
    "    \n",
    "    high_set = {\"A\"}\n",
    "    mid_set = {\"B\"}\n",
    "    \n",
    "    pairs = _positive_instances_from_high_or_mid_count_notes(high_set, mid_set, notes)\n",
    "    pairs.sort() \n",
    "    expected = [(\"A\", \"B\"), (\"B\", \"A\")]\n",
    "    test_eq(pairs, expected)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Test 4: Negative Sampling (with Mocked Randomness)\n",
    "# ---------------------------------------------------------\n",
    "@patch('random.choices')\n",
    "def test_negative_instance_sampling(mock_choices):\n",
    "    notes = {\n",
    "        \"A\": mock_note_data(\"A\", reverse_count=5, direct_links={\"B\": 1}),\n",
    "        \"B\": mock_note_data(\"B\", reverse_count=5, direct_links={}) \n",
    "    }\n",
    "    \n",
    "    # Mock random.choices to return specific sequences:\n",
    "    # 1. High->High: [B], [A]. Link B->A does NOT exist. (Should be ADDED)\n",
    "    # 2. High->Mid: Returns empty (no mids in this mock)\n",
    "    # 3. Mid->High: Returns empty\n",
    "    \n",
    "    # Note: Logic inside function calls random.choices 3 times (High->High, High->Mid, Mid->High)\n",
    "    mock_choices.side_effect = [\n",
    "        [\"B\"], [\"A\"], # High -> High call (origin list, relied list)\n",
    "        [], [],       # High -> Mid call\n",
    "        [], []        # Mid -> High call\n",
    "    ]\n",
    "    \n",
    "    high_set = {\"A\", \"B\"}\n",
    "    mid_set = set()\n",
    "    \n",
    "    results = _negative_instances_from_high_or_mid_count_notes(\n",
    "        high_set, mid_set, notes, num_pairs=10\n",
    "    )\n",
    "    \n",
    "    test_eq(results, [(\"B\", \"A\")])\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Test 5: Admissibility (Tag Filtering)\n",
    "# ---------------------------------------------------------\n",
    "def test_pair_admissibility():\n",
    "    info_data = {\"InfoNote\": mock_note_data(\"InfoNote\")}\n",
    "    notat_data = {\"NotatNote\": mock_note_data(\"NotatNote\")}\n",
    "    \n",
    "    all_data = {**info_data, **notat_data}\n",
    "    \n",
    "    # Case 1: Origin has no tags -> Admissible\n",
    "    all_data[\"CleanOrigin\"] = mock_note_data(\"CleanOrigin\", tags=None)\n",
    "    assert _pair_is_admissible(\"CleanOrigin\", \"InfoNote\", all_data, info_data, notat_data)\n",
    "    \n",
    "    # Case 2: Origin has auto-links tag, Target is InfoNote -> Inadmissible\n",
    "    all_data[\"AutoLinkedOrigin\"] = mock_note_data(\"AutoLinkedOrigin\", tags={'_auto/links_added'})\n",
    "    assert not _pair_is_admissible(\"AutoLinkedOrigin\", \"InfoNote\", all_data, info_data, notat_data)\n",
    "    \n",
    "    # Case 3: Origin has auto-links tag, Target is NotatNote -> Admissible \n",
    "    # (because relied_note 'NotatNote' is NOT in info_data, it's in notat_data)\n",
    "    assert _pair_is_admissible(\"AutoLinkedOrigin\", \"NotatNote\", all_data, info_data, notat_data)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Test 6: Similar Notation Logic (UPDATED for jarowinkler_similarity)\n",
    "# ---------------------------------------------------------\n",
    "def test_similar_notation_logic():\n",
    "    n1 = mock_note_data(\"N1\"); n1.parsed.notation_str = \"Gal(L/K)\"\n",
    "    n2 = mock_note_data(\"N2\"); n2.parsed.notation_str = \"Gal(F/E)\"\n",
    "    n3 = mock_note_data(\"N3\"); n3.parsed.notation_str = \"Spec(R)\"\n",
    "    \n",
    "    data = {\"N1\": n1, \"N2\": n2, \"N3\": n3}\n",
    "    \n",
    "    # Patch the function directly where it is imported in your main module\n",
    "    # Assuming your main module is called \"__main__\" in the notebook context\n",
    "    with patch('__main__.jarowinkler_similarity') as mock_similarity:\n",
    "        # Define side effects: High similarity (>0.9) for N1-N2, Low for others\n",
    "        def sim_side_effect(s1, s2):\n",
    "            if \"Gal\" in s1 and \"Gal\" in s2: return 0.95\n",
    "            return 0.1\n",
    "        \n",
    "        mock_similarity.side_effect = sim_side_effect\n",
    "        \n",
    "        sim_pairs = _similar_notation_pairs(data)\n",
    "        \n",
    "        # Should find N1->N2 and N2->N1\n",
    "        assert \"N2\" in sim_pairs[\"N1\"]\n",
    "        assert \"N1\" in sim_pairs[\"N2\"]\n",
    "        assert \"N3\" not in sim_pairs\n",
    "\n",
    "# Run all\n",
    "test_link_types_extraction()\n",
    "test_note_counting_logic()\n",
    "test_positive_instance_extraction()\n",
    "test_negative_instance_sampling()\n",
    "test_pair_admissibility()\n",
    "test_similar_notation_logic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting a note pair into a string and data augmentation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparing Inputs for NLP Models**\n",
    "\n",
    "Once note pairs are selected, they must be converted into a single text string suitable for transformer models (BERT, T5). This involves:\n",
    "\n",
    "    Formatting: Concatenating the \"Origin\" and \"Relied\" note data with a specific separator token ([SEP] for BERT, </s> for T5).\n",
    "\n",
    "    Augmentation: To make the model robust, we randomly perturb the text (e.g. removing LaTeX commands, making bad formatting) and occasionally erase positional metadata (section numbers, etc.) to prevent overfitting to specific document structures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _erase_position_metadata(\n",
    "        augmentation: Literal['high', 'mid', 'low'] | None\n",
    "        ) -> bool:\n",
    "    \"\"\"\n",
    "    Randomly determines whether to erase positional metadata based on augmentation intensity.\n",
    "    \n",
    "    - 'high': 30% chance\n",
    "    - 'mid':  20% chance\n",
    "    - 'low':  10% chance\n",
    "    \"\"\"\n",
    "    if augmentation is None: return False\n",
    "    \n",
    "    rand_value = random.random()\n",
    "    thresholds = {'high': 0.3, 'mid': 0.2, 'low': 0.1}\n",
    "    return rand_value < thresholds.get(augmentation, 0.0)\n",
    "\n",
    "#| export\n",
    "def string_from_note_pair(\n",
    "        pair_data: NotePairData, # The pair of notes to convert.\n",
    "        format: Literal['bert', 't5'] # The model architecture determines the separator token.\n",
    "        ) -> str: # A single string combining both notes.\n",
    "    \"\"\"\n",
    "    Formats a pair of notes into a single input string for NLP models.\n",
    "    \n",
    "    Combines the `data_string` of both notes, separated by a model-specific delimiter.\n",
    "    \"\"\"\n",
    "    origin_data = pair_data['origin_note']\n",
    "    relied_data = pair_data['relied_note']\n",
    "    origin_data_string = origin_data.data_string(format)\n",
    "    relied_data_string = relied_data.data_string(format)\n",
    "    \n",
    "    if format == 'bert':\n",
    "        return f'{origin_data_string}\\n\\n[SEP]\\n\\n{relied_data_string}'\n",
    "    else:\n",
    "        # T5 typically uses </s> or specific sentinel tokens depending on pre-training\n",
    "        return f'{origin_data_string}\\n\\n</s>\\n\\n{relied_data_string}'\n",
    "\n",
    "#| export\n",
    "def augment_note_pair(\n",
    "        pair_data: NotePairData, # The original note pair.\n",
    "        augmentation: Optional[Literal['high', 'mid', 'low']] = None, # Intensity of augmentation.\n",
    "        include_position_data_for_origin: bool = True, # Force inclusion/exclusion of origin metadata.\n",
    "        include_position_data_for_relied: bool = True # Force inclusion/exclusion of relied metadata.\n",
    "        ) -> NotePairData: # A new, modified NotePairData object.\n",
    "    \"\"\"\n",
    "    Creates an augmented copy of a note pair for training data variety.\n",
    "    \n",
    "    Applies text perturbations (via `randomly_modify`) and optionally erases \n",
    "    positional metadata (section numbers, etc.) to force the model to focus on content.\n",
    "    \"\"\"\n",
    "    origin_data = pair_data['origin_note'].deepcopy()\n",
    "    relied_data = pair_data['relied_note'].deepcopy()\n",
    "    \n",
    "    # Determine if we should erase metadata (probabilistic OR forced)\n",
    "    erase_origin = _erase_position_metadata(augmentation) or not include_position_data_for_origin\n",
    "    erase_relied = _erase_position_metadata(augmentation) or not include_position_data_for_relied\n",
    "    \n",
    "    if augmentation is not None:\n",
    "        origin_data.randomly_modify(augmentation, erase_position_metadata=erase_origin)\n",
    "        relied_data.randomly_modify(augmentation, erase_position_metadata=erase_relied)\n",
    "        \n",
    "    return NotePairData(origin_note=origin_data, relied_note=relied_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unittest.mock import MagicMock, patch\n",
    "\n",
    "def test_string_formatting():\n",
    "    # Setup Mocks\n",
    "    origin = MagicMock(); origin.data_string.return_value = \"ORIGIN_TEXT\"\n",
    "    relied = MagicMock(); relied.data_string.return_value = \"RELIED_TEXT\"\n",
    "    pair = {\"origin_note\": origin, \"relied_note\": relied}\n",
    "    \n",
    "    # BERT format\n",
    "    assert string_from_note_pair(pair, \"bert\") == \"ORIGIN_TEXT\\n\\n[SEP]\\n\\nRELIED_TEXT\"\n",
    "    \n",
    "    # T5 format\n",
    "    assert string_from_note_pair(pair, \"t5\") == \"ORIGIN_TEXT\\n\\n</s>\\n\\nRELIED_TEXT\"\n",
    "\n",
    "@patch('random.random')\n",
    "def test_metadata_erasure_probability(mock_rand):\n",
    "    # Test 'high' probability (0.3)\n",
    "    mock_rand.return_value = 0.25 # Below 0.3\n",
    "    assert _erase_position_metadata('high') is True\n",
    "    \n",
    "    mock_rand.return_value = 0.35 # Above 0.3\n",
    "    assert _erase_position_metadata('high') is False\n",
    "    \n",
    "    # Test None\n",
    "    assert _erase_position_metadata(None) is False\n",
    "\n",
    "def test_augmentation_logic():\n",
    "    # Setup recursive mocks for deepcopy/randomly_modify\n",
    "    origin = MagicMock()\n",
    "    origin.deepcopy.return_value = origin # Simplify for test\n",
    "    relied = MagicMock()\n",
    "    relied.deepcopy.return_value = relied\n",
    "    \n",
    "    pair = {\"origin_note\": origin, \"relied_note\": relied}\n",
    "    \n",
    "    # Run Augmentation\n",
    "    with patch('__main__._erase_position_metadata', return_value=True):\n",
    "        augment_note_pair(pair, 'mid')\n",
    "        \n",
    "    # Verify modification calls\n",
    "    origin.randomly_modify.assert_called_with('mid', erase_position_metadata=True)\n",
    "    relied.randomly_modify.assert_called_with('mid', erase_position_metadata=True)\n",
    "\n",
    "test_string_formatting()\n",
    "test_metadata_erasure_probability()\n",
    "test_augmentation_logic()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| export\n",
    "# def _erase_position_metadata(\n",
    "#         augmentation: Literal['high', 'mid',' low'] | None,\n",
    "#         ) -> bool:\n",
    "#     \"\"\"\n",
    "\n",
    "#     \"\"\"\n",
    "#     rand_value = random.random()\n",
    "#     if augmentation == 'high':\n",
    "#         return rand_value < 0.3\n",
    "#     elif augmentation == 'mid':\n",
    "#         return rand_value < 0.2\n",
    "#     elif augmentation == 'low':\n",
    "#         return rand_value < 0.1\n",
    "#     return False\n",
    "    \n",
    "    \n",
    "# def string_from_note_pair(\n",
    "#             pair_data: NotePairData,\n",
    "#             format: Literal['bert', 't5'],\n",
    "#             # note_data: dict[str, NoteData],\n",
    "#         ) -> str:\n",
    "#     origin_data = pair_data['origin_note']\n",
    "#     relied_data = pair_data['relied_note']\n",
    "#     origin_data_string = origin_data.data_string(format)\n",
    "#     relied_data_string = relied_data.data_string(format)\n",
    "#     if format == 'bert':\n",
    "#         return f'{origin_data_string}\\n\\n[SEP]\\n\\n{relied_data_string}'\n",
    "#     else:\n",
    "#         return f'{origin_data_string}\\n\\n</s>\\n\\n{relied_data_string}'\n",
    "\n",
    "\n",
    "# def augment_note_pair(\n",
    "#         pair_data: NotePairData,\n",
    "#         augmentation: Optional[Literal['high', 'mid', 'low']] = None,\n",
    "#         include_position_data_for_origin: bool = True,\n",
    "#         include_position_data_for_relied: bool = True,\n",
    "#         ) -> NotePairData:\n",
    "#     \"\"\"\n",
    "#     Return an augmented copy of `pair_data`.\n",
    "#     \"\"\"\n",
    "#     origin_data = pair_data['origin_note'].deepcopy()\n",
    "#     relied_data = pair_data['relied_note'].deepcopy()\n",
    "#     erase_position_data_for_origin_data = _erase_position_metadata(augmentation) or not include_position_data_for_origin\n",
    "#     erase_position_data_for_relied_data = _erase_position_metadata(augmentation) or not include_position_data_for_relied\n",
    "#     if augmentation is not None:\n",
    "#         origin_data.randomly_modify(augmentation, erase_position_data_for_origin_data)\n",
    "#         relied_data.randomly_modify(augmentation, erase_position_data_for_relied_data)\n",
    "#     return NotePairData(\n",
    "#         origin_note=origin_data, relied_note=relied_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForSeq2SeqLM, AutoModelForTokenClassification, AutoTokenizer, pipeline\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained('hyunjongkimmath/notation_summarizations_model')\n",
    "# tokenizer = AutoTokenizer.from_pretrained('hyunjongkimmath/notation_summarizations_model')\n",
    "# summarizer = pipeline('summarization', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Constructing the Final Dataset**\n",
    "\n",
    "We compile the processed pairs into a standard format (`NoteLinkingDataPoint`) compatible with HuggingFace Dataset objects. This format includes:\n",
    "\n",
    "1. Input Text: The augmented, concatenated string.\n",
    "2. Labels: A list of NoteLinkEnum names representing the valid relationships between the notes (e.g., ['INFO_TO_INFO_IN_CONTENT', 'INFO_TO_INFO_IN_SEE_ALSO']).\n",
    "\n",
    "The `dataset_from_note_data` function orchestrates the entire pipeline: sieving pairs, converting them to data points, applying augmentations, and returning a ready-to-train Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# class NoteLinkingDataPoint(TypedDict):\n",
    "#     \"\"\"\n",
    "#     A dict object that is \n",
    "#     \"\"\"\n",
    "#     origin_note_name: str\n",
    "#     relied_note_name: str\n",
    "#     input_text: str\n",
    "#     # Keys derived from NoteLinkEnum (excluding NO_LINK)\n",
    "#     link_types: list[str] # The str are the names of `NoteLinkEnum`.\n",
    "#     # info_to_info_in_content: bool\n",
    "#     # info_to_info_in_see_also: bool\n",
    "#     # info_to_info_via_notat: bool\n",
    "#     # info_to_notat_via_embedding: bool\n",
    "#     # notat_to_info: bool\n",
    "#     # notat_to_info_via_notat: bool\n",
    "#     # notat_to_notat: bool\n",
    "\n",
    "#| export\n",
    "class NoteLinkingDataPoint(TypedDict):\n",
    "    \"\"\"\n",
    "    A dictionary structure representing a single training example for the model.\n",
    "    \"\"\"\n",
    "    origin_note_name: str # Name of the source note.\n",
    "    relied_note_name: str # Name of the target note.\n",
    "    input_text: str # Concatenated text of both notes (augmented).\n",
    "    link_types: list[str] # List of active link types (labels) for multi-label classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def dict_data_point_from_pair(\n",
    "        pair_data: NotePairData, # The note pair to convert.\n",
    "        format: Literal['bert', 't5'] # Format for the input text string.\n",
    "        ) -> NoteLinkingDataPoint: # The structured training example.\n",
    "    \"\"\"\n",
    "    Converts a raw `NotePairData` into a labeled `NoteLinkingDataPoint` for training.\n",
    "    \n",
    "    Extracts the link types (labels) and generates the formatted input text.\n",
    "    \"\"\"\n",
    "    origin_note_name = pair_data['origin_note'].note_name\n",
    "    relied_note_name = pair_data['relied_note'].note_name\n",
    "    input_text = string_from_note_pair(pair_data, format)    \n",
    "    \n",
    "    # Get active links as a set of Enums, convert to list of strings for JSON/Dataset compatibility\n",
    "    active_links = link_types_for_note_pair_data(pair_data)\n",
    "    link_types_str = [link.name for link in active_links]\n",
    "    \n",
    "    return NoteLinkingDataPoint(\n",
    "        origin_note_name=origin_note_name,\n",
    "        relied_note_name=relied_note_name,\n",
    "        input_text=input_text,\n",
    "        link_types=link_types_str\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Text: Mock Data for NoteA\n",
      "\n",
      "[SEP]\n",
      "\n",
      "Mock Data fo...\n",
      "Labels: ['INFO_TO_INFO_IN_CONTENT']\n"
     ]
    }
   ],
   "source": [
    "# Mock data (using the helper from previous sections)\n",
    "#| export\n",
    "from unittest.mock import MagicMock\n",
    "\n",
    "def mock_note_data(name, reverse_count=0, direct_links=None, tags=None, notation_str=\"\"):\n",
    "    \"\"\"\n",
    "    Creates a mock NoteData object.\n",
    "    \n",
    "    Args:\n",
    "        direct_links: Dictionary mapping NoteName -> Set of NoteLinkEnum\n",
    "                      Example: {\"TargetNote\": {NoteLinkEnum.INFO_TO_INFO_IN_CONTENT}}\n",
    "    \"\"\"\n",
    "    m = MagicMock()\n",
    "    m.note_name = name\n",
    "    \n",
    "    # Mock data_string to prevent string_from_note_pair failure\n",
    "    m.data_string.return_value = f\"Mock Data for {name}\"\n",
    "    \n",
    "    # Correctly mocking deepcopy to return itself (or a new mock) for augmentation tests\n",
    "    m.deepcopy.return_value = m \n",
    "\n",
    "    m.reverse_linked_notes = {f\"in_{i}\" for i in range(reverse_count)}\n",
    "    \n",
    "    # CRITICAL FIX: Ensure direct_links values are Sets/Iterables, not Integers\n",
    "    m.directly_linked_notes = direct_links if direct_links else {}\n",
    "    \n",
    "    m.tags = tags\n",
    "    m.parsed = MagicMock()\n",
    "    m.parsed.notation_str = notation_str if notation_str else name\n",
    "    return m\n",
    "\n",
    "#| example\n",
    "# Setup mock data with CORRECT Enum usage (with underscores) and structure (Set, not Int)\n",
    "origin_note = mock_note_data(\n",
    "    \"NoteA\", \n",
    "    direct_links={\"NoteB\": {NoteLinkEnum.INFO_TO_INFO_IN_CONTENT}} # Value must be a SET\n",
    ")\n",
    "relied_note = mock_note_data(\"NoteB\")\n",
    "pair = {\"origin_note\": origin_note, \"relied_note\": relied_note}\n",
    "\n",
    "# Convert single pair\n",
    "data_point = dict_data_point_from_pair(pair, format='bert')\n",
    "print(f\"Input Text: {data_point['input_text'][:40]}...\")\n",
    "print(f\"Labels: {data_point['link_types']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def dataset_from_note_data(\n",
    "        info_note_data: dict[str, InfoNoteData], # Pool of information notes.\n",
    "        notat_note_data: dict[str, NotatNoteData], # Pool of notation notes.\n",
    "        augment: bool, # Whether to generate augmented copies of data points.\n",
    "        format: Literal['bert', 't5'] # Model format for text encoding.\n",
    "        ) -> Dataset: # A HuggingFace Dataset ready for training.\n",
    "    \"\"\"\n",
    "    Full pipeline: Sieves note pairs, augments them, and compiles a HuggingFace Dataset.\n",
    "    \n",
    "    If `augment` is True, generates 3 additional versions (low, mid, high intensity) \n",
    "    for every sampled pair, effectively quadrupling the dataset size.\n",
    "    \"\"\"\n",
    "    # 1. Gather raw pairs via heuristic sieving\n",
    "    note_data_pairs: list[NotePairData] = sieve_note_data_pairs(\n",
    "        info_note_data, notat_note_data)\n",
    "    \n",
    "    dict_data: list[NoteLinkingDataPoint] = []\n",
    "    \n",
    "    for pair_data in note_data_pairs:\n",
    "        # Add original (un-augmented) data point\n",
    "        dict_data.append(dict_data_point_from_pair(\n",
    "            pair_data, format))\n",
    "        \n",
    "        if not augment:\n",
    "            continue\n",
    "            \n",
    "        # Add 3 augmented versions\n",
    "        for augmentation in ['low', 'mid', 'high']:\n",
    "            augmented_pair_data = augment_note_pair(\n",
    "                pair_data, augmentation)\n",
    "            dict_data.append(dict_data_point_from_pair(\n",
    "                augmented_pair_data, format))\n",
    "                \n",
    "    return Dataset.from_list(dict_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| test\n",
    "@patch('__main__.sieve_note_data_pairs')\n",
    "@patch('__main__.augment_note_pair')\n",
    "def test_dataset_generation_loop(mock_augment, mock_sieve):\n",
    "    # --- 1. Setup Mock Objects that return REAL STRINGS ---\n",
    "    \n",
    "    # Mock Origin/Relied notes for the initial Sieve output\n",
    "    sieve_origin = MagicMock()\n",
    "    sieve_origin.note_name = \"OriginNote\"   # <--- MUST be a string\n",
    "    sieve_origin.data_string.return_value = \"Origin Data\"\n",
    "    sieve_origin.directly_linked_notes = {}  # Empty dict needed for link_types_for_note_pair_data\n",
    "    \n",
    "    sieve_relied = MagicMock()\n",
    "    sieve_relied.note_name = \"ReliedNote\"   # <--- MUST be a string\n",
    "    sieve_relied.data_string.return_value = \"Relied Data\"\n",
    "    \n",
    "    # The sieve returns this pair\n",
    "    mock_sieve.return_value = [{\n",
    "        \"origin_note\": sieve_origin, \n",
    "        \"relied_note\": sieve_relied\n",
    "    }]\n",
    "\n",
    "    # --- 2. Setup Mock Objects for Augmentation Output ---\n",
    "    \n",
    "    aug_origin = MagicMock()\n",
    "    aug_origin.note_name = \"AugOriginNote\" # <--- MUST be a string\n",
    "    aug_origin.data_string.return_value = \"Aug Data\"\n",
    "    aug_origin.directly_linked_notes = {}\n",
    "    \n",
    "    aug_relied = MagicMock()\n",
    "    aug_relied.note_name = \"AugReliedNote\" # <--- MUST be a string\n",
    "    aug_relied.data_string.return_value = \"Aug Data\"\n",
    "    \n",
    "    # augment_note_pair returns this pair\n",
    "    mock_augment.return_value = {\n",
    "        \"origin_note\": aug_origin, \n",
    "        \"relied_note\": aug_relied\n",
    "    }\n",
    "    \n",
    "    # --- Run Tests ---\n",
    "    \n",
    "    # 1. No Augmentation\n",
    "    ds = dataset_from_note_data({}, {}, augment=False, format='bert')\n",
    "    assert len(ds) == 1\n",
    "    # Verify the dataset actually contains the string \"OriginNote\"\n",
    "    assert ds[0]['origin_note_name'] == \"OriginNote\"\n",
    "    \n",
    "    # 2. With Augmentation\n",
    "    ds_aug = dataset_from_note_data({}, {}, augment=True, format='bert')\n",
    "    assert len(ds_aug) == 4\n",
    "    # The first item is original\n",
    "    assert ds_aug[0]['origin_note_name'] == \"OriginNote\"\n",
    "    # The subsequent items are augmented (check name from mocked augmented output)\n",
    "    assert ds_aug[1]['origin_note_name'] == \"AugOriginNote\"\n",
    "\n",
    "test_dataset_generation_loop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get predictions from model pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MultiLabelPipeline(Pipeline):\n",
    "    \"\"\"\n",
    "    Implementing this `Pipeline` class is necessary because HuggingFAce's standard\n",
    "    `text-classification` pipeline uses softmax, which is suitable for single-label or\n",
    "    multi-class classification; a sigmoid activation function is more suitable for\n",
    "    multi-label classification.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, tokenizer, **kwargs):\n",
    "        super().__init__(model=model, tokenizer=tokenizer, **kwargs)\n",
    "\n",
    "    def _sanitize_parameters(self, **kwargs):\n",
    "        return {}, {}, {}\n",
    "\n",
    "    def preprocess(self, inputs, **kwargs):\n",
    "        return self.tokenizer(inputs, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    def _forward(self, model_inputs, **kwargs):\n",
    "        return self.model(**model_inputs)\n",
    "\n",
    "    def postprocess(self, model_outputs, **kwargs):\n",
    "        logits = model_outputs.logits\n",
    "        probabilities = torch.sigmoid(logits)  # Use sigmoid for multi-label\n",
    "        return probabilities.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def prediction_by_note_linking_model(\n",
    "        origin_data: NoteData, # The `NoteData` object representing the \"origin note\", i.e.  the note from which a link to the \"relied note\" is considered.\n",
    "        relied_data: NoteData, # The `NoteData` object representing the \"relied note\", i.e.  the note to which a link from the \"origin note\" is considered.\n",
    "        predictor: MultiLabelPipeline,\n",
    "        format: Literal['bert', 't5'] = 'bert', # Specifies how to format the input to `predictor`.\n",
    "        as_floats: bool = True, # If `True`, then return the predictions as floats indicating how likely it is that there should be a linking from the origin note to the relied note of each type.\n",
    "        threshold: float | dict[str, float] = 0.5, # Either a float value or a dictionary whose keys are the possible labels and whose values are floats. If a label is not one of the dictionary's key, then the default threshold value of 0.5 is used for that label. A float value exceeding this threshold corresponds to a prediction that a link of the given type should exist. This is only used if `as_floats` is `True`.\n",
    "        ) -> Union[dict[str, float], dict[str, bool]]: # A `dict` whose keys are the `labels` and whose values are either `float`s between 0.0 and 1.0 indicating how likely it is that there should be a linking from the origin note to the relied note of the type corresponding to the label. \n",
    "    r\"\"\"\n",
    "    Predict how likely/whether a note to should to another note for a specified reason.\n",
    "    \"\"\"\n",
    "    pair_data = NotePairData(origin_note=origin_data, relied_note=relied_data)\n",
    "    input_text = string_from_note_pair(pair_data, format)\n",
    "    preds: list[float] = predictor(input_text)[0]\n",
    "    id2label: dict[int, str] = predictor.model.config.id2label\n",
    "    output: Union[dict[str, float], dict[str, bool]] = {}\n",
    "    for id, label in id2label.items():\n",
    "        if as_floats:\n",
    "            output[label] = preds[id]\n",
    "        else:\n",
    "            if isinstance(threshold, float):\n",
    "                label_threshold = threshold\n",
    "            elif label in threshold:\n",
    "                label_threshold = threshold[label]\n",
    "            else:\n",
    "                label_threshold = 0.5\n",
    "            output[label] = preds[id] > label_threshold\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NO_LINK': False, 'INFO_TO_INFO_IN_CONTENT': False, 'INFO_TO_INFO_IN_SEE_ALSO': False, 'INFO_TO_INFO_VIA_NOTAT': False, 'INFO_TO_NOTAT_VIA_EMBEDDING': True, 'NOTAT_TO_INFO': False, 'NOTAT_TO_INFO_VIA_NOTAT': True, 'NOTAT_TO_NOTAT': False}\n",
      "{'NO_LINK': False, 'INFO_TO_INFO_IN_CONTENT': True, 'INFO_TO_INFO_IN_SEE_ALSO': False, 'INFO_TO_INFO_VIA_NOTAT': False, 'INFO_TO_NOTAT_VIA_EMBEDDING': False, 'NOTAT_TO_INFO': True, 'NOTAT_TO_INFO_VIA_NOTAT': True, 'NOTAT_TO_NOTAT': True}\n"
     ]
    }
   ],
   "source": [
    "from unittest.mock import patch as mock_patch\n",
    "with (mock_patch('__main__.string_from_note_pair') as mock_string_from_note_pair):\n",
    "    mock_origin_data = MagicMock()\n",
    "    mock_relied_data = MagicMock()\n",
    "    mock_predictor = MagicMock()\n",
    "    mock_predictor.model = MagicMock()\n",
    "    mock_predictor.model.config = MagicMock()\n",
    "    mock_predictor.model.config.id2label = {\n",
    "        0: 'NO_LINK',\n",
    "        1: 'INFO_TO_INFO_IN_CONTENT',\n",
    "        2: 'INFO_TO_INFO_IN_SEE_ALSO',\n",
    "        3: 'INFO_TO_INFO_VIA_NOTAT',\n",
    "        4: 'INFO_TO_NOTAT_VIA_EMBEDDING',\n",
    "        5: 'NOTAT_TO_INFO',\n",
    "        6: 'NOTAT_TO_INFO_VIA_NOTAT',\n",
    "        7: 'NOTAT_TO_NOTAT'}\n",
    "    \n",
    "    mock_predictor.return_value = [[0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6, 0.0]]\n",
    "    output = prediction_by_note_linking_model(\n",
    "        mock_origin_data, mock_relied_data, mock_predictor, as_floats=False, threshold=0.5)\n",
    "    print(output)\n",
    "    test_is(output['NO_LINK'], False)\n",
    "    test_is(output['INFO_TO_INFO_IN_CONTENT'], False)\n",
    "    test_is(output['INFO_TO_NOTAT_VIA_EMBEDDING'], True)\n",
    "    test_is(output['NOTAT_TO_INFO_VIA_NOTAT'], True)\n",
    "\n",
    "    mock_predictor.return_value = [[0.0, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]]\n",
    "    output = prediction_by_note_linking_model(\n",
    "        mock_origin_data, mock_relied_data, mock_predictor, as_floats=False, threshold={\n",
    "            'INFO_TO_INFO_VIA_NOTAT': 0.65,\n",
    "            'INFO_TO_INFO_IN_CONTENT': 0.10\n",
    "        })\n",
    "    print(output)\n",
    "    test_is(output['INFO_TO_INFO_VIA_NOTAT'], False)\n",
    "    test_is(output['INFO_TO_INFO_IN_CONTENT'], True)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def predict_note_linking(\n",
    "        origin_note: VaultNote, \n",
    "        relied_notes: VaultNote| list[VaultNote],\n",
    "        predictor: MultiLabelPipeline,\n",
    "        format: Literal['bert', 't5'] = 'bert', # Specifies how to format the input to `predictor`.\n",
    "        note_data: Optional[dict[str, NoteData]] = None, # For the purposes of predicting note linking, the note data only requires the positional data, so getting the note data via `note_data_from_index_note` should suffice (without having to use `find_reverse_links`, although `get_main_note_content_of_notat_note_data` should still be necessary).\n",
    "        omit_no_link_predictions: bool = True, # if `True` omit predictions of `NoteLinkEnum.NO_LINK`\n",
    "        threshold: float | dict[float]= 0.5, # See also `prediction_by_note_linking_model`. Either a float value or a dictionary whose keys are the possible labels and whose values are floats. If a label is not one of the dictionary's key, then the default threshold value of 0.5 is used for that label. A float value exceeding this threshold corresponds to a prediction that a link of the given type should exist. This is only used if `as_floats` is `True`.\n",
    "        ) -> dict[str, list[NoteLinkEnum]]: # The keys are the names of relied notes. The values are lists of `NoteLinkEnum` that specify the linking types from origin note to the relied note.\n",
    "    # TODO: add threshold parameter\n",
    "    if isinstance(relied_notes, VaultNote):\n",
    "        relied_notes: list[VaultNote] = [relied_notes]\n",
    "    if note_data and origin_note.name in note_data:\n",
    "        origin_note_data = note_data[origin_note.name]\n",
    "    else:\n",
    "        try:\n",
    "            origin_note_data = _note_data_from_vault_note_on_the_fly(\n",
    "                origin_note, reference='', note_data=note_data)\n",
    "        except Exception as e:\n",
    "            print(f\"An error ocurred while trying to get data for  `origin_note`: {origin_note}\")\n",
    "            print(e)\n",
    "            return\n",
    "    output_dict: dict[str, list[NoteLinkEnum]] = {}\n",
    "    for relied_note in relied_notes:\n",
    "        if relied_note.name == origin_note.name:\n",
    "            continue\n",
    "        if note_data and relied_note.name in note_data:\n",
    "            relied_note_data = note_data[relied_note.name]\n",
    "        else:\n",
    "            try:\n",
    "                relied_note_data = _note_data_from_vault_note_on_the_fly(\n",
    "                    relied_note, reference='', note_data=note_data)\n",
    "            except Exception as e:\n",
    "                print(f\"An error ocurred while trying to get data for  `relied_note`: {relied_note}\")\n",
    "                print(e)\n",
    "                continue\n",
    "        if relied_note_data == None:\n",
    "            print(relied_note)\n",
    "        preds: dict[str, bool] = prediction_by_note_linking_model(\n",
    "            origin_note_data, relied_note_data, predictor, format,\n",
    "            as_floats=False,\n",
    "            threshold=threshold)\n",
    "        output_dict[relied_note.name] = []\n",
    "        for enum_name, link_flag in preds.items():\n",
    "            if omit_no_link_predictions and enum_name == \"NO_LINK\":\n",
    "                continue\n",
    "            elif link_flag:\n",
    "                output_dict[relied_note.name].append(NoteLinkEnum[enum_name])\n",
    "    return output_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'relied_note_name': []}\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "with (mock_patch('__main__.prediction_by_note_linking_model') as mock_prediction_by_note_linking_model, \\\n",
    "          mock_patch('__main__._note_data_from_vault_note_on_the_fly') as mock_note_data_from_vault_note_on_the_fly):\n",
    "     mock_origin_note = MagicMock()\n",
    "     mock_relied_note = MagicMock()\n",
    "     mock_origin_note.name = 'origin_note_name'\n",
    "     mock_relied_note.name = 'relied_note_name'\n",
    "     relied_notes = [mock_relied_note]\n",
    "     mock_predictor = MagicMock()\n",
    "\n",
    "     mock_prediction_by_note_linking_model.return_value = {\n",
    "          'INFO_TO_INFO_IN_CONTENT': False,\n",
    "          'INFO_TO_INFO_IN_SEE_ALSO': False,\n",
    "          'INFO_TO_INFO_VIA_NOTAT': True,\n",
    "          'INFO_TO_NOTAT_VIA_EMBEDDING': False,\n",
    "          'NOTAT_TO_INFO': False,\n",
    "          'NOTAT_TO_INFO_VIA_NOTAT': False,\n",
    "          'NOTAT_TO_NOTAT': False,\n",
    "          'NO_LINK': False}\n",
    "     mock_note_data_from_vault_note_on_the_fly.side_effect = [MagicMock(), MagicMock()]\n",
    "     output = predict_note_linking(\n",
    "          mock_origin_note, relied_notes, mock_predictor, omit_no_link_predictions=True)\n",
    "     test_eq(\n",
    "          output,\n",
    "          {'relied_note_name': [NoteLinkEnum.INFO_TO_INFO_VIA_NOTAT]})\n",
    "\n",
    "\n",
    "     mock_prediction_by_note_linking_model.return_value = {\n",
    "          'INFO_TO_INFO_IN_CONTENT': False,\n",
    "          'INFO_TO_INFO_IN_SEE_ALSO': False,\n",
    "          'INFO_TO_INFO_VIA_NOTAT': False,\n",
    "          'INFO_TO_NOTAT_VIA_EMBEDDING': False,\n",
    "          'NOTAT_TO_INFO': False,\n",
    "          'NOTAT_TO_INFO_VIA_NOTAT': False,\n",
    "          'NOTAT_TO_NOTAT': False,\n",
    "          'NO_LINK': True}\n",
    "     mock_note_data_from_vault_note_on_the_fly.side_effect = [MagicMock(), MagicMock()]\n",
    "     output = predict_note_linking(\n",
    "          mock_origin_note, relied_notes, mock_predictor, omit_no_link_predictions=True)\n",
    "     test_eq(\n",
    "          output,\n",
    "          {'relied_note_name': []})\n",
    "     print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link cache note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the model will take a lot of time --- not only does each prediction take about a few seconds, but also the predictions need to be made on pairs of notes and hence the total time needed for predictions grows quadratically with the number of notes. As such, \"link cache notes\" will be made to record predictions.\n",
    "\n",
    "The link cache note will be saved in the root directory of its reference folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def link_cache_note(\n",
    "        vault: PathLike,\n",
    "        reference: str,\n",
    "        create_if_does_not_exist: bool = True,\n",
    "        ) -> VaultNote: # The `VaultNote` object representing the link cache note.\n",
    "    \"\"\"\n",
    "    Return a `VaultNote` object representing the link cache note in a reference of a vault.\n",
    "    \"\"\"\n",
    "    ind_note: VaultNote = index_note_for_reference(vault, reference, update_cache=True)\n",
    "    reference_folder: Path = ind_note.path(relative=True).parent\n",
    "    vn = VaultNote(vault, rel_path=reference_folder / f'_link_cache_{reference}.md')\n",
    "    if create_if_does_not_exist and not vn.exists():\n",
    "        vn.create()\n",
    "    return vn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The link cache note will be formatted as follows:\n",
    "\n",
    "```\n",
    "- [[origin_note_name_1]]\n",
    "    - [[relied_note_name_1]]: [<comma_separated_link_types_1>]\n",
    "    - [[relied_note_name_2]]: [<comma_separated_link_types_2>]\n",
    "    ...\n",
    "<blank space for separation>\n",
    "- [[origin_note_name_2]]\n",
    "    - ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def separate_blocks(\n",
    "        text: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Splits text into blocks separated by one or more blank lines.\n",
    "    Returns a list of blocks (strings) with whitespace stripped.\n",
    "    \"\"\"\n",
    "    blocks = []\n",
    "    current_block = []\n",
    "    \n",
    "    for line in text.splitlines():\n",
    "        if line.strip() == '':  # Blank line\n",
    "            if current_block:  # Only add if we have content\n",
    "                blocks.append('\\n'.join(current_block))\n",
    "                current_block = []\n",
    "        else:\n",
    "            current_block.append(line)\n",
    "    \n",
    "    # Add the last block if there's content remaining\n",
    "    if current_block:\n",
    "        blocks.append('\\n'.join(current_block))\n",
    "    \n",
    "    return blocks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['First line\\nSecond line', 'Third block starts here\\nWith multiple lines', 'Final block']\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"First line\n",
    "Second line\n",
    "\n",
    "Third block starts here\n",
    "With multiple lines\n",
    "\n",
    "Final block\"\"\"\n",
    "\n",
    "blocks = separate_blocks(text)\n",
    "print(blocks)\n",
    "# for i, block in enumerate(blocks, 1):\n",
    "#     print(f\"Block {i}:\\n{block}\\n{'-'*20}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def parse_link_cache_note(\n",
    "        link_cache_note: VaultNote,\n",
    "        ) -> dict[str, dict[str, list[NoteLinkEnum]]]: # The first key is the name of an \"origin note\". The second key is the name of a \"relied note\" with respect to the origin note. The value is a list of the link types from the origin note to the relied note.\n",
    "    \"\"\"\n",
    "    See also `write_link_cache_note`, which is essentially the opposite of this function.\n",
    "    \"\"\"\n",
    "    text = link_cache_note.text()\n",
    "    blocks = separate_blocks(text)\n",
    "    link_types: dict[str, dict[str, list[NoteLinkEnum]]] = {}\n",
    "    for block in blocks:\n",
    "        lines: list[str] = block.splitlines()\n",
    "        first_line_link: ObsidianLink = links_from_text(lines[0])[0]\n",
    "        origin_note_name = first_line_link.file_name\n",
    "        link_types[origin_note_name] = {}\n",
    "        for line in lines[1:]:\n",
    "            link: ObsidianLink = links_from_text(line)[0]\n",
    "            relied_note_name = link.file_name\n",
    "            ind = line.index(':')\n",
    "            note_type_list = ast.literal_eval(line[ind+2:])\n",
    "            link_types[origin_note_name][relied_note_name] = [\n",
    "                NoteLinkEnum[note_type_str] for note_type_str in note_type_list]\n",
    "    return link_types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mock_patch('__main__.VaultNote') as mock_vault_note:\n",
    "    mock_link_cache_note = mock_vault_note.return_value\n",
    "    mock_link_cache_note.text.return_value = '''\n",
    "- [[origin_note_1]]\n",
    "    - [[relied_note_1]]: ['INFO_TO_INFO_IN_CONTENT', 'INFO_TO_INFO_VIA_NOTAT']\n",
    "    - [[relied_note_2]]: ['INFO_TO_NOTAT_VIA_EMBEDDING']\n",
    "\n",
    "- [[origin_note_2]]\n",
    "    - [[relied_note_3]]: ['NOTAT_TO_NOTAT']\n",
    "    - [[relied_note_4]]: ['NOTAT_TO_INFO', 'NOTAT_TO_INFO_VIA_NOTAT']\n",
    "'''\n",
    "    parse_link_cache_note(mock_link_cache_note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def write_link_cache_note(\n",
    "        link_types: dict[str, dict[str, list[NoteLinkEnum]]],\n",
    "        cache_note: VaultNote,\n",
    "        ) -> None:\n",
    "    \"\"\"\n",
    "    Overwrite the contents of the note represented by `link_cache_note` using the data\n",
    "    from `link_types`.\n",
    "\n",
    "    `link_cache_notes` is assumed to exist.\n",
    "\n",
    "    See also `parse_link_cache_note`, which is essentially the opposite of this function.\n",
    "    \"\"\"\n",
    "    chunks: list[str] = []\n",
    "    for origin_note_name, relied_dict in link_types.items():\n",
    "        chunk_text = f\"- [[{origin_note_name}]]\\n\"\n",
    "        for relied_note_name, link_type_list in relied_dict.items():\n",
    "            chunk_text = f'{chunk_text}    - [[{relied_note_name}]]: {str([link_type.name for link_type in link_type_list])}\\n'\n",
    "        chunks.append(chunk_text)\n",
    "    cache_note.write('\\n\\n'.join(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- [[origin_note_1]]\n",
      "    - [[relied_note_1]]: ['INFO_TO_INFO_IN_CONTENT', 'INFO_TO_INFO_VIA_NOTAT']\n",
      "    - [[relied_note_2]]: ['INFO_TO_NOTAT_VIA_EMBEDDING']\n",
      "\n",
      "\n",
      "- [[origin_note_2]]\n",
      "    - [[relied_note_3]]: ['NOTAT_TO_NOTAT']\n",
      "    - [[relied_note_4]]: ['NOTAT_TO_INFO', 'NOTAT_TO_INFO_VIA_NOTAT']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with mock_patch('__main__.VaultNote') as mock_vault_note:\n",
    "    mock_link_cache_note = mock_vault_note.return_value\n",
    "    link_types = {\n",
    "        'origin_note_1': {\n",
    "            'relied_note_1': [\n",
    "                NoteLinkEnum.INFO_TO_INFO_IN_CONTENT, NoteLinkEnum.INFO_TO_INFO_VIA_NOTAT],\n",
    "            'relied_note_2': [\n",
    "                NoteLinkEnum.INFO_TO_NOTAT_VIA_EMBEDDING]},\n",
    "        'origin_note_2': {\n",
    "            'relied_note_3': [\n",
    "                NoteLinkEnum.NOTAT_TO_NOTAT],\n",
    "            'relied_note_4': [\n",
    "                NoteLinkEnum.NOTAT_TO_INFO, NoteLinkEnum.NOTAT_TO_INFO_VIA_NOTAT] }\n",
    "    }\n",
    "    write_link_cache_note(link_types, mock_link_cache_note)\n",
    "    args, _ = mock_link_cache_note.write.call_args\n",
    "    written_content = args[0]\n",
    "    print(written_content)\n",
    "    test_eq(\n",
    "        written_content,\n",
    "        '''- [[origin_note_1]]\n",
    "    - [[relied_note_1]]: ['INFO_TO_INFO_IN_CONTENT', 'INFO_TO_INFO_VIA_NOTAT']\n",
    "    - [[relied_note_2]]: ['INFO_TO_NOTAT_VIA_EMBEDDING']\n",
    "\n",
    "\n",
    "- [[origin_note_2]]\n",
    "    - [[relied_note_3]]: ['NOTAT_TO_NOTAT']\n",
    "    - [[relied_note_4]]: ['NOTAT_TO_INFO', 'NOTAT_TO_INFO_VIA_NOTAT']\n",
    "'''\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def consolidate_note_linking_predictions_into_cache(\n",
    "        origin_note: VaultNote | str,\n",
    "        predictions: dict[str, list[NoteLinkEnum]], # An output of `predict_note_linking``\n",
    "        cache: dict[str, dict[str, list[NoteLinkEnum]]], # See `parse_link_cache_note`. The first key is the name of an \"origin note\". The second key is the name of a \"relied note\" with respect to the origin note. The value is a list of the link types from the origin note to the relied note.\n",
    "        ):\n",
    "    \"\"\"\n",
    "    Consolidate the outputs of `predict_note_linking` into a link cache.\n",
    "    \"\"\"\n",
    "    if isinstance(origin_note, VaultNote):\n",
    "        origin_note = origin_note.name\n",
    "    if origin_note not in cache:\n",
    "        cache[origin_note] = {}\n",
    "    for relied_note_name, predicted_link_enums in predictions.items():\n",
    "        if origin_note == relied_note_name:\n",
    "            continue \n",
    "        predicted_link_enums = set(predicted_link_enums)\n",
    "        predicted_link_enums = predicted_link_enums - {NoteLinkEnum.NO_LINK}\n",
    "        if relied_note_name not in cache[origin_note]:\n",
    "            cache[origin_note][relied_note_name] = []\n",
    "        cached_link_enums = set(cache[origin_note][relied_note_name])\n",
    "        link_enums = cached_link_enums | predicted_link_enums\n",
    "        cache[origin_note][relied_note_name] = list(link_enums)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'origin_note_name': {'relied_note_name_1': [<NoteLinkEnum.INFO_TO_INFO_IN_SEE_ALSO: 2>, <NoteLinkEnum.INFO_TO_INFO_IN_CONTENT: 1>, <NoteLinkEnum.INFO_TO_INFO_VIA_NOTAT: 3>], 'relied_note_name_2': [], 'relied_note_name_3': [<NoteLinkEnum.INFO_TO_NOTAT_VIA_EMBEDDING: 4>]}}\n"
     ]
    }
   ],
   "source": [
    "predictions = {\n",
    "    'relied_note_name_1': [NoteLinkEnum.INFO_TO_INFO_IN_CONTENT, NoteLinkEnum.INFO_TO_INFO_VIA_NOTAT],\n",
    "    'relied_note_name_2': [],\n",
    "    'relied_note_name_3': [NoteLinkEnum.INFO_TO_NOTAT_VIA_EMBEDDING]}\n",
    "cache = {'origin_note_name': {'relied_note_name_1': [NoteLinkEnum.INFO_TO_INFO_IN_CONTENT, NoteLinkEnum.INFO_TO_INFO_IN_SEE_ALSO]}}\n",
    "\n",
    "consolidate_note_linking_predictions_into_cache('origin_note_name', predictions, cache)\n",
    "\n",
    "print(cache)\n",
    "test_eq(\n",
    "    set(cache['origin_note_name']['relied_note_name_1']), \n",
    "    set([NoteLinkEnum.INFO_TO_INFO_IN_CONTENT, NoteLinkEnum.INFO_TO_INFO_VIA_NOTAT, NoteLinkEnum.INFO_TO_INFO_IN_SEE_ALSO]))\n",
    "\n",
    "test_eq(\n",
    "    set(cache['origin_note_name']['relied_note_name_2']), \n",
    "    set([]))\n",
    "\n",
    "test_eq(\n",
    "    set(cache['origin_note_name']['relied_note_name_3']), \n",
    "    set([NoteLinkEnum.INFO_TO_NOTAT_VIA_EMBEDDING]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def consolidate_caches(\n",
    "        cache_1: dict[str, dict[str, list[NoteLinkEnum]]], # See `parse_link_cache_note`. The first key is the name of an \"origin note\". The second key is the name of a \"relied note\" with respect to the origin note. The value is a list of the link types from the origin note to the relied note.\n",
    "        cache_2: dict[str, dict[str, list[NoteLinkEnum]]],\n",
    "        ) -> dict[str, dict[str, list[NoteLinkEnum]]]:\n",
    "    new_cache: dict[str, dict[str, list[NoteLinkEnum]]] = copy.deepcopy(cache_1)\n",
    "    for origin_note_name, origin_note_dict in cache_2.items():\n",
    "        consolidate_note_linking_predictions_into_cache(\n",
    "            origin_note_name, origin_note_dict, new_cache)\n",
    "    return new_cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<NoteLinkEnum.INFO_TO_INFO_IN_SEE_ALSO: 2>, <NoteLinkEnum.INFO_TO_INFO_IN_CONTENT: 1>]\n"
     ]
    }
   ],
   "source": [
    "# Create two caches with some overlapping data\n",
    "cache_a = {\n",
    "    'Note_A': {'Note_B': [NoteLinkEnum.INFO_TO_INFO_IN_CONTENT]}\n",
    "}\n",
    "cache_b = {\n",
    "    'Note_A': {'Note_B': [NoteLinkEnum.INFO_TO_INFO_IN_SEE_ALSO]},\n",
    "    'Note_C': {'Note_D': [NoteLinkEnum.NOTAT_TO_INFO]}\n",
    "}\n",
    "\n",
    "# Consolidate them\n",
    "merged = consolidate_caches(cache_a, cache_b)\n",
    "\n",
    "# Verify the merge happened\n",
    "print(merged['Note_A']['Note_B']) \n",
    "# Output: [<NoteLinkEnum...CONTENT>, <NoteLinkEnum...SEE_ALSO>]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import *\n",
    "\n",
    "# --- Basic Edge Cases ---\n",
    "# Empty Inputs\n",
    "test_eq(consolidate_caches({}, {}), {})\n",
    "\n",
    "# Idempotency (Duplicates)\n",
    "dup_cache = {'A': {'B': [NoteLinkEnum.INFO_TO_INFO_IN_CONTENT]}}\n",
    "res = consolidate_caches(dup_cache, dup_cache)\n",
    "test_eq(len(res['A']['B']), 1)\n",
    "\n",
    "# --- Complex Scenario Tests ---\n",
    "# Base setup for the main test scenario\n",
    "cache_1 = {\n",
    "    'origin_note_1': {\n",
    "        'relied_note_1': [NoteLinkEnum.INFO_TO_INFO_IN_CONTENT],\n",
    "    },\n",
    "    'origin_note_2': { # Only exists in cache_1\n",
    "        'relied_note_1': [NoteLinkEnum.INFO_TO_INFO_IN_CONTENT],\n",
    "    },\n",
    "    'origin_note_3': { # Exists in both, but relied_note_2 is unique to cache_1\n",
    "        'relied_note_2': [NoteLinkEnum.NOTAT_TO_INFO],\n",
    "    }\n",
    "}\n",
    "cache_2 = {\n",
    "    'origin_note_1': { # Exists in both, relied_note_1 exists in both\n",
    "        'relied_note_1': [NoteLinkEnum.INFO_TO_INFO_IN_SEE_ALSO]\n",
    "    },\n",
    "    'origin_note_3': { # Exists in both, but relied_note_1 is unique to cache_2\n",
    "        'relied_note_1': [NoteLinkEnum.NOTAT_TO_INFO_VIA_NOTAT]\n",
    "    }\n",
    "}\n",
    "new_cache = consolidate_caches(cache_1, cache_2)\n",
    "\n",
    "# Case 1: Deep Merge (Union of Lists)\n",
    "test_eq(\n",
    "    set(new_cache['origin_note_1']['relied_note_1']), \n",
    "    {NoteLinkEnum.INFO_TO_INFO_IN_SEE_ALSO, NoteLinkEnum.INFO_TO_INFO_IN_CONTENT}\n",
    ")\n",
    "\n",
    "# Case 2: Preservation of Left-Only Data\n",
    "test_eq(\n",
    "    new_cache['origin_note_2']['relied_note_1'], [NoteLinkEnum.INFO_TO_INFO_IN_CONTENT]\n",
    ")\n",
    "\n",
    "# Case 3: Partial Merge (Left Unique Key in Shared Parent)\n",
    "test_eq(\n",
    "    new_cache['origin_note_3']['relied_note_2'], [NoteLinkEnum.NOTAT_TO_INFO]\n",
    ")\n",
    "\n",
    "# Case 4: Partial Merge (Right Unique Key in Shared Parent)\n",
    "test_eq(\n",
    "    new_cache['origin_note_3']['relied_note_1'], [NoteLinkEnum.NOTAT_TO_INFO_VIA_NOTAT]\n",
    ")\n",
    "\n",
    "# Case 6: Completely New Origin Key (Right-Only Top Level)\n",
    "cache_new_origin = {'Z': {'Y': [NoteLinkEnum.NOTAT_TO_INFO]}}\n",
    "res_new = consolidate_caches(cache_1, cache_new_origin)\n",
    "test_eq(res_new['Z']['Y'], [NoteLinkEnum.NOTAT_TO_INFO])\n",
    "test_eq(len(res_new), 4) # origin_1, origin_2, origin_3 + Z\n",
    "\n",
    "# Case 7: Empty Inputs (Identity)\n",
    "test_eq(consolidate_caches(cache_1, {}), cache_1)\n",
    "test_eq(consolidate_caches({}, cache_2), cache_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def remove_blank_or_no_link_data_from_cache(\n",
    "        cache: dict[str, dict[str, list[NoteLinkEnum]]], # See `parse_link_cache_note`. The first key is the name of an \"origin note\". The second key is the name of a \"relied note\" with respect to the origin note. The value is a list of the link types from the origin note to the relied note.\n",
    "        ) -> dict[str, dict[str, list[NoteLinkEnum]]]: # A new cache, with lists that are either blank or which only contain `NoteLinkEnum.NO_LINK` are removed and with blank dict values are also removed..\n",
    "    new_cache: dict[str, dict[str, list[NoteLinkEnum]]] = {} \n",
    "    for origin_note_name, origin_dict in cache.items():\n",
    "        cleaned_dict: dict[str, list[NoteLinkEnum]] = {}\n",
    "        for relied_note_name, listy in origin_dict.items():\n",
    "            if not listy or (len(set(listy)) == 1 and listy[0] == NoteLinkEnum.NO_LINK):\n",
    "                continue\n",
    "            cleaned_dict[relied_note_name] = listy\n",
    "        if cleaned_dict:\n",
    "            new_cache[origin_note_name] = cleaned_dict\n",
    "    return new_cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = {\n",
    "    'origin_note_1': {},\n",
    "    'origin_note_2': {\n",
    "        'relied_note_1': [],\n",
    "        'relied_note_2': [NoteLinkEnum.NO_LINK] \n",
    "    },\n",
    "    'origin_note_3': {\n",
    "        'relied_note_1': [NoteLinkEnum.INFO_TO_INFO_IN_CONTENT, NoteLinkEnum.INFO_TO_INFO_IN_SEE_ALSO]\n",
    "    }}\n",
    "output = remove_blank_or_no_link_data_from_cache(cache)\n",
    "test_eq(\n",
    "    output, \n",
    "    {'origin_note_3':\n",
    "     {'relied_note_1':\n",
    "      [NoteLinkEnum.INFO_TO_INFO_IN_CONTENT, NoteLinkEnum.INFO_TO_INFO_IN_SEE_ALSO]}}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def remove_nonexistent_note_names_from_cache(\n",
    "        cache: dict[str, dict[str, list[NoteLinkEnum]]], # See `parse_link_cache_note`. The first key is the name of an \"origin note\". The second key is the name of a \"relied note\" with respect to the origin note. The value is a list of the link types from the origin note to the relied note.\n",
    "        vault: PathLike\n",
    "        ) -> dict[str, dict[str, list[NoteLinkEnum]]]:\n",
    "    \"\"\"\n",
    "    Remove names of nonexistent notes in `cache`.\n",
    "    \"\"\"\n",
    "    cache_copy = copy.deepcopy(cache)\n",
    "    keys = cache_copy.keys()\n",
    "    for origin_note_name in list(keys):\n",
    "        origin_note = VaultNote(vault, name=origin_note_name)\n",
    "        if not origin_note.exists():\n",
    "            cache_copy.pop(origin_note_name)\n",
    "    for origin_note_name, origin_dict in cache_copy.items():\n",
    "        keys = origin_dict.keys()\n",
    "        for relied_note_name in list(keys):\n",
    "            relied_note = VaultNote(vault, name=relied_note_name)\n",
    "            if not relied_note.exists():\n",
    "                origin_dict.pop(relied_note_name)\n",
    "    return cache_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sieve note pairs to predict on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the number of pairs of notes grows quadratically in the number of notes, it takes too much time to make predictions one-by-one. It should be useful to prioritize certain pairs over others.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def sieve_potential_relied_notes(\n",
    "        vault: PathLike,\n",
    "        reference: str,\n",
    "        origin_note: VaultNote, # an info note\n",
    "        note_data: dict[str, NoteData],\n",
    "        # potential_relied_notes: list[VaultNote],\n",
    "        appendix_notes: list[VaultNote], # notes whose index notes are appendix notes\n",
    "        cache: dict[str, dict[str, list[NoteLinkEnum]]],\n",
    "        notation_similarity_threshold: float = 0.8, # The threshold that the similarity metric of a notion must exceed for the name of a notation note to be included in the output.\n",
    "        skip_already_made_predictions: bool = True,\n",
    "        ) -> set[str]: # Names of potential relied notes that may be good to predict note linking from `origin_note` for.`\n",
    "    if origin_note.name not in note_data:\n",
    "        print(f'`origin_note` was not in `note_data`. Perhaps a `origin_note` has been renamed at some point and it may be necessary to reload `note_data`. `origin_note`: {origin_note}.')\n",
    "        return set()\n",
    "    index_note: VaultNote = index_note_for_reference(vault, reference, update_cache=True)\n",
    "    info_notes: list[VaultNote] = notes_linked_in_notes_linked_in_note(index_note, as_dict=False)\n",
    "    appendix_note_names: set[str] = set([appendix_note.name for appendix_note in appendix_notes])\n",
    "\n",
    "    relied_note_names = set()\n",
    "\n",
    "    # Add an info not if it \n",
    "    # 1. is in the appendix or precedes `origin_note`, is a definition/notation note\n",
    "    # 2. is in the same section and precedes `origin_note` and is a context note.\n",
    "    # TODO: Automatically add a def/notat note if it precedes `origin_note` in a section by a little.\n",
    "    # Add a notation note if it \n",
    "    # 1. looks similar to a substr in a latex str in the origin_note.\n",
    "    for info_note in info_notes + appendix_notes:\n",
    "        if not info_note.exists():\n",
    "            continue\n",
    "        if info_note.name not in note_data:\n",
    "            # If this happens, it may be the case that `info_note` has been\n",
    "            # renamed, but this has not been reflected in `note_data`.\n",
    "            continue\n",
    "        # Ignore `info_note` if it was already predicted on or it precedes `origin_note` and is not an appendix note. \n",
    "        if (skip_already_made_predictions\n",
    "                and origin_note.name in cache\n",
    "                and info_note.name in cache[origin_note.name]):\n",
    "            continue\n",
    "        if (note_data_order_cmp(note_data[origin_note.name], note_data[info_note.name]) <= 0\n",
    "                and info_note.name not in appendix_note_names):\n",
    "            continue\n",
    "\n",
    "        mf = MarkdownFile.from_vault_note(info_note)\n",
    "        # ignore non-definition/notation notes.\n",
    "        if not (mf.has_tag('_auto/_meta/definition') or mf.has_tag('_auto/_meta/notation') or mf.has_tag('_meta/definition') or mf.has_tag('_meta/notation')):\n",
    "            continue\n",
    "        # admit context notes in the same section as `origin_note` that also precede `origin_note`.\n",
    "        elif (mf.has_tag(\"_auto/_meta/context\") or mf.has_tag('_meta/context')\n",
    "                and note_data_order_cmp(note_data[origin_note.name], note_data[info_note.name]) >= 0\n",
    "                and note_data[origin_note.name].section_num == note_data[info_note.name].section_num):\n",
    "            relied_note_names.add(info_note.name)\n",
    "            continue\n",
    "        relied_note_names.add(info_note.name)\n",
    "        # For each info note with notations, try to see if the notations resemble notations used in `origin_note`.\n",
    "        notat_notes: list[VaultNote] = notation_notes_linked_in_see_also_section(\n",
    "            info_note, vault, as_vault_notes=True)\n",
    "        if skip_already_made_predictions:\n",
    "            notat_notes = [\n",
    "                notat_note for notat_note in notat_notes\n",
    "                if not (origin_note.name in cache and notat_note.name in cache[origin_note.name])]\n",
    "        notat_note_candidates: list[VaultNote] = []\n",
    "        for notat_note in notat_notes:\n",
    "            individual_notat_note: list[VaultNote] = similar_notat_notes_in_note(\n",
    "                origin_note, notat_note, threshold=notation_similarity_threshold)\n",
    "            if not individual_notat_note:\n",
    "                continue\n",
    "            relied_note_names.add(info_note.name)\n",
    "            relied_note_names.add(notat_note.name)\n",
    "\n",
    "        # For each info note with definitions, try to see if the definitions resemble phrases used in `origin_note`. \n",
    "\n",
    "\n",
    "    \n",
    "    # # 2. find all context notes in the same section as origin_note\n",
    "    # origin_index_note = index_note_of_note(origin_note)\n",
    "    # section_notes: dict[str, VaultNote] = notes_linked_in_note(\n",
    "    #     origin_index_note, as_dict=True)\n",
    "    # relied_note_names.update(section_notes.keys())\n",
    "\n",
    "    return relied_note_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _predict_one_direction_and_consolidate_cache(\n",
    "        origin_note: VaultNote,\n",
    "        relied_note: VaultNote,\n",
    "        cache: dict[str, dict[str, list[NoteLinkEnum]]],\n",
    "        predictor: MultiLabelPipeline, \n",
    "        format: Literal['bert', 't5'],\n",
    "        note_data: dict[str, NoteData] | None,\n",
    "        skip_already_made_predictions: bool,\n",
    "        threshold: float | dict[str, float],\n",
    "        ) -> None:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if (skip_already_made_predictions\n",
    "            and origin_note.name in cache and relied_note.name in cache[origin_note.name]):\n",
    "        return\n",
    "    outputs: dict[str, list[NoteLinkEnum]] = predict_note_linking(\n",
    "        origin_note, relied_note, predictor, format, note_data, threshold=threshold)\n",
    "    consolidate_note_linking_predictions_into_cache(origin_note, outputs, cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def predict_on_relied_note_and_related_notat_notes(\n",
    "        origin_note: VaultNote,\n",
    "        relied_note: VaultNote,\n",
    "        cache: dict[str, dict[str, list[NoteLinkEnum]]], # The current cache of predictions, see `parse_link_cache_note` for example; this is used to skip predictions that have already been made. Moreover, the cache is updated based on the predictions made. \n",
    "        predictor: MultiLabelPipeline,\n",
    "        format: Literal['bert', 't5'] = 'bert',\n",
    "        note_data: Optional[dict[str, NoteData]] = None,\n",
    "        skip_already_made_predictions: bool = True,\n",
    "        predict_reverse_too: bool = False,\n",
    "        threshold: float | dict[str, float] = 0.5,\n",
    "        ) -> None:\n",
    "    \"\"\"\n",
    "    Update `cache` by making predictions from `origin_note` to `relied_note` (and vice versa).\n",
    "    Moreover, \n",
    "    \"\"\"\n",
    "    # predict `origin_note` to `relied_note``\n",
    "\n",
    "    _predict_one_direction_and_consolidate_cache(\n",
    "        origin_note, relied_note, cache, predictor,\n",
    "        format, note_data, skip_already_made_predictions, threshold)\n",
    "    if predict_reverse_too:\n",
    "        _predict_one_direction_and_consolidate_cache(\n",
    "            relied_note, origin_note, cache, predictor, format, note_data,\n",
    "            skip_already_made_predictions, threshold)\n",
    "\n",
    "    # For each relied note that is 1. an info note, 2. got predicted to be a relied note via info_to_info_via_notat, and 3. has a notation, predict whether the relevant notation notes ought to be linked.\n",
    "    if not origin_note.name in cache:\n",
    "        return\n",
    "    relied_note_names: list[str] = list(cache[origin_note.name])\n",
    "    for relied_note_name in relied_note_names:\n",
    "        relied_note_link_types = cache[origin_note.name][relied_note_name]\n",
    "        if not relied_note_link_types:\n",
    "            continue\n",
    "        if not NoteLinkEnum.INFO_TO_INFO_VIA_NOTAT in relied_note_link_types:\n",
    "            continue\n",
    "        relied_note = VaultNote(origin_note.vault, name=relied_note_name)\n",
    "        notat_notes: list[VaultNote] = notation_notes_linked_in_see_also_section(\n",
    "            relied_note, origin_note.vault, as_vault_notes=True)\n",
    "        for notat_note in notat_notes:\n",
    "            _predict_one_direction_and_consolidate_cache(\n",
    "                origin_note, notat_note, cache, predictor, format, note_data,\n",
    "                skip_already_made_predictions, threshold)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify notation notes that should be embedded as footnotes in information notes or linked in other notation notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def similar_notat_notes_in_note(\n",
    "        origin_note: VaultNote, # Either an info or a notat note\n",
    "        notation_notes: VaultNote | list[VaultNote], # The notation notes that are considered to be \n",
    "        threshold: float = 0.8, \n",
    "        ) -> list[VaultNote]: # The notation notes whose notations are determined to be similar to notations used in `origin_note`.\n",
    "    \"\"\"\n",
    "    Determine which notation notes introduce notations which resemble notations used\n",
    "    in `origin_note`.\n",
    "\n",
    "    This is a fuzzy function purely based on the str value of the notation and the text of `origin_note` and does not use ML predictions. \n",
    "    \"\"\"\n",
    "    if isinstance(notation_notes, VaultNote):\n",
    "        notation_notes = [notation_notes]\n",
    "\n",
    "    text = origin_note.text()\n",
    "    indices = latex_indices(text)\n",
    "    latex_texts_in_origin_note: list[str] = []\n",
    "    for start, end in indices:\n",
    "        latex_text = text[start:end]\n",
    "        latex_text = latex_text.strip('$ ')\n",
    "        latex_texts_in_origin_note.append(latex_text)\n",
    "    matching_notat_notes: list[VaultNote] = []\n",
    "    for notation_note in notation_notes:\n",
    "        if notation_note.name == origin_note.name:\n",
    "            continue\n",
    "        notation: str = notation_in_note(notation_note, include_dollar_signs=False)\n",
    "        for latex_text in latex_texts_in_origin_note:\n",
    "            if latex_str_is_likely_in_latex_str(notation, latex_text, threshold=threshold):\n",
    "                matching_notat_notes.append(notation_note)\n",
    "                break\n",
    "    return matching_notat_notes\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| TODO: test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def locate_footnote_embedded_notation_link(\n",
    "        origin_note: VaultNote, # An info note \n",
    "        notation_note: VaultNote, # The notation notes that are considered to be \n",
    "        locate_by: Literal['first', 'best'] = 'best', # If `'first'`, then the first latex string for which the `latex_str_in_latex_str_fuzz_metric` score exceed threshold is used as the location. If `'best'` or if no such latex string exists, then the latex string giving the greatest score is used as the location.\n",
    "        threshold: float = 0.8,\n",
    "        ) -> int | None: # The index in `origin_note.text()` at which the footnote to an embedded link to `notation_note` should be added. If the main note of `notation_note` is `origin_note`, then `None`.\n",
    "    \"\"\"\n",
    "    Determine where in `origin_note` a footnote to an embedded link to `notation_note` should be added.\n",
    "\n",
    "    Such a location would be at the end of the closing of a latex string in the text. \n",
    "\n",
    "    This is a fuzzy function purely based on the str value of the notation and the text of `origin_note` and does not use ML predictions. \n",
    "    \"\"\"\n",
    "    main_note = main_of_notation(notation_note, as_note=False)\n",
    "    if main_note and origin_note.name == main_note:\n",
    "        return None\n",
    "    notation: str = notation_in_note(notation_note, include_dollar_signs=False)\n",
    "    text = origin_note.text()\n",
    "    indices = latex_indices(text)\n",
    "    scores: dict[int, float] = {} # Keys are end indices and values are scores of how likely it seems that the latex str seems to use the notation.\n",
    "    for start, end in indices:\n",
    "        latex_text = text[start:end]\n",
    "        latex_text = latex_text.strip('$ ')\n",
    "        score: float = latex_str_in_latex_str_fuzz_metric(notation, latex_text)\n",
    "        if score > threshold and locate_by == 'first':\n",
    "            return end\n",
    "        else:\n",
    "            scores[end] = score\n",
    "    max_key = max(scores, key=scores.get)\n",
    "    return max_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222\n",
      "For each integer $m$ and each transitive $G \\leq S_m$, there are constants $C(G), Q(G)$, and $e(G)$ such that, for all $q>Q(G)$ coprime to $\\#G$ and all $X>0$, \n",
      "\n",
      "$$N_G(\\mathbb{F}_q(t),X) \\leq C(G) X^{a(G)} \\log(X)^{e(G)}$$\n"
     ]
    }
   ],
   "source": [
    "mock_origin_note = MagicMock()\n",
    "mock_notation_note = MagicMock()\n",
    "mock_origin_note.text.return_value = r\"\"\"For each integer $m$ and each transitive $G \\leq S_m$, there are constants $C(G), Q(G)$, and $e(G)$ such that, for all $q>Q(G)$ coprime to $\\#G$ and all $X>0$, \n",
    "\n",
    "$$N_G(\\mathbb{F}_q(t),X) \\leq C(G) X^{a(G)} \\log(X)^{e(G)}$$\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "mock_notation_note.text.return_value = r\"\"\"---\n",
    "detect_regex: \n",
    "latex_in_original: [\"a(G)\"]\n",
    "tags: [_meta/notation_note_named]\n",
    "---\n",
    "$a(G)$ [[ellenberg_tran_westerland_fnfcqsamcff_1. Introduction_ellenberg_tran_westerland_fnfcqsamcff|denotes]] $[\\min_{G \\setminus \\{1 \\}} ind(g)]^{-1}$ where $G$ is a transitive subgroup of $S_m$ and \n",
    "\n",
    "![[ellenberg_tran_westerland_fnfcqsamcff_1. Introduction_ellenberg_tran_westerland_fnfcqsamcff#^38959b]]\n",
    "\n",
    "for $g \\in S_m$.\n",
    "\n",
    "For instance, if $G = S_m$, the minimal index is $1$, realized by transpositions, and so $a(S_m) = 1$.\n",
    "- [$ind(g)$](ellenberg_tran_westerland_fnfcqsamcff_notation_ind_g_index_of_element_of_S_m.md)\"\"\"\n",
    "\n",
    "with (mock_patch('__main__.latex_str_in_latex_str_fuzz_metric')\n",
    "        as mock_latex_str_in_latex_str_fuzz_metric,\n",
    "        mock_patch('__main__.main_of_notation') as mock_main_of_notation,\n",
    "        mock_patch('__main__.notation_in_note') as mock_notation_in_note,\n",
    "        ):\n",
    "    mock_latex_str_in_latex_str_fuzz_metric.side_effect = [0, 0, 0, 0, 0, 0, 0, 1]\n",
    "    mock_main_note = MagicMock()\n",
    "    mock_main_of_notation.return_value = mock_main_note\n",
    "    mock_notation_in_note.return_value = '$a(G)$'\n",
    "    output = locate_footnote_embedded_notation_link(\n",
    "        mock_origin_note, mock_notation_note, locate_by='best', threshold=0.8)\n",
    "    print(output)\n",
    "    print(mock_origin_note.text.return_value[:output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _where_to_add_notation_links(\n",
    "        origin_note: VaultNote,\n",
    "        relied_notes: list[VaultNote],\n",
    "        locate_by: Literal['first', 'best'] = 'best',\n",
    "        threshold: float = 0.8,\n",
    "        ) -> dict[int, list[VaultNote]]:\n",
    "    \"\"\"\n",
    "    Helper function to `add_notation_note_embedded_footnotes_to_info_note`.\n",
    "    \"\"\"\n",
    "    where_to_add: dict[int, list[VaultNote]] = {} # Keys are end indices of latex str in `origin_note` and values are lists of VaultNote objects representing notation notes for which the embedded links should be added.\n",
    "    for relied_note in relied_notes:\n",
    "        location: int | None = locate_footnote_embedded_notation_link(\n",
    "            origin_note, relied_note, locate_by, threshold)\n",
    "        if location is None:\n",
    "            continue\n",
    "        if not location in where_to_add:\n",
    "            where_to_add[location] = []\n",
    "        where_to_add[location].append(relied_note)\n",
    "    return where_to_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _add_notation_note_embedded_footnotes(\n",
    "        text: str,\n",
    "        where_to_add: dict[int, list[VaultNote]],\n",
    "        ) -> str:\n",
    "    \"\"\"\n",
    "    Helper function to `add_notation_note_embedded_footnotes_to_info_note`.\n",
    "    \"\"\"\n",
    "    reverse_sorted_locations: list[int] = sorted(where_to_add.keys(), reverse=True)\n",
    "    # iterate in reverse to make sure that the modifications made along the way do not change\n",
    "    # the indices of the locations.\n",
    "    for location in reverse_sorted_locations:\n",
    "        notation_notes_to_link = where_to_add[location]\n",
    "        available_footnote_numbers: list[int] = identify_available_footnote_numbers(\n",
    "            text, count=len(notation_notes_to_link))\n",
    "        footnote_text = ''.join([f'[^{num}]' for num in available_footnote_numbers])\n",
    "        footnote_mentions = '\\n'.join(\n",
    "            [f'[^{num}]: ![[{notat_note.name}]]'\n",
    "             for num, notat_note in zip(available_footnote_numbers, notation_notes_to_link)])\n",
    "        new_line_index = text.find('\\n', location+1)\n",
    "        if new_line_index == -1:\n",
    "            new_line_index = len(text)\n",
    "        pieces = [text[0:location], text[location:new_line_index], text[new_line_index:]]\n",
    "\n",
    "        if location > 1 and text[location-2] == '$': # latex str ends with '$$'\n",
    "            # pieces.append(f'\\n\\n{footnote_text}\\n\\n{footnote_mentions}\\n\\n')\n",
    "            pieces.insert(2, f'\\n\\n{footnote_text}\\n\\n{footnote_mentions}\\n\\n')\n",
    "            text = ''.join(pieces)\n",
    "            # start a new line to add the footnotes and then start another\n",
    "            # to add the footnote mentions.\n",
    "        else: # latex str ends with '$'\n",
    "            pieces.insert(2, f'\\n\\n{footnote_mentions}\\n\\n')\n",
    "            pieces.insert(1, footnote_text)\n",
    "            text = ''.join(pieces)\n",
    "            # new_line_index = \n",
    "    return text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$$asdf$$\n",
      "\n",
      "[^1]\n",
      "\n",
      "[^1]: ![[notat_note_name]]\n",
      "\n",
      "\n",
      "asdf asdf $asdf$[^1] asdf asdf \n",
      "\n",
      "[^1]: ![[notat_note_name]]\n",
      "\n",
      "\n",
      "\n",
      "fjfjfjfj\n",
      "\n",
      "---\n",
      "cssclass: clean-embeds\n",
      "aliases: []\n",
      "tags: [_meta/literature_note, _reference/18785, _meta/concept, _meta/proof]\n",
      "---\n",
      "# Topic[^1]\n",
      "\n",
      "Theorem 2.1. The map $\\mathrm{q} \\mapsto \\mathrm{q} \\cap A$ defines a bijection from the set of prime ideals of $S^{-1} A$[^2] and the set of prime ideals of A that do not intersect $S .$ The inverse map is $\\mathfrak{p} \\mapsto \\mathfrak{p} S^{-1} A$.\n",
      "\n",
      "[^2]: ![[18785_notation_S_minus_1_A_localization_of_a_commutative_ring_with_respect_to_a_multiplicative_subset]]\n",
      "\n",
      "\n",
      "\n",
      "Proof. See [1, Cor.11.20] or [2, Prop. 3.11.iv].\n",
      "\n",
      "# See Also\n",
      "\n",
      "# Meta\n",
      "## References\n",
      "![[_reference_18785]]\n",
      "\n",
      "## Citations and Footnotes\n",
      "[^1]: Sutherland, Theorem 2.1, Page 11\n",
      "\n",
      "\n",
      "For each integer $m$ and each transitive $G \\leq S_m$, there are constants $C(G), Q(G)$, and $e(G)$ such that, for all $q>Q(G)$ coprime to $\\#G$ and all $X>0$, \n",
      "\n",
      "$$N_G(\\mathbb{F}_q(t),X) \\leq C(G) X^{a(G)} \\log(X)^{e(G)}$$\n",
      "\n",
      "\n",
      "[^1]\n",
      "\n",
      "[^1]: ![[notat_note_name]]\n",
      "\n",
      "\n",
      "blah blah\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "text = r\"\"\"$$asdf$$\"\"\"\n",
    "mock_notat_note = MagicMock()\n",
    "mock_notat_note.name = 'notat_note_name'\n",
    "where_to_add = {8: [mock_notat_note]}\n",
    "print(_add_notation_note_embedded_footnotes(text, where_to_add))\n",
    "\n",
    "text = r\"\"\"asdf asdf $asdf$ asdf asdf \n",
    "\n",
    "fjfjfjfj\n",
    "\"\"\"\n",
    "mock_notat_note = MagicMock()\n",
    "mock_notat_note.name = 'notat_note_name'\n",
    "where_to_add = {16: [mock_notat_note]}\n",
    "output = _add_notation_note_embedded_footnotes(text, where_to_add)\n",
    "print(output)\n",
    "\n",
    "\n",
    "text = r\"\"\"---\n",
    "cssclass: clean-embeds\n",
    "aliases: []\n",
    "tags: [_meta/literature_note, _reference/18785, _meta/concept, _meta/proof]\n",
    "---\n",
    "# Topic[^1]\n",
    "\n",
    "Theorem 2.1. The map $\\mathrm{q} \\mapsto \\mathrm{q} \\cap A$ defines a bijection from the set of prime ideals of $S^{-1} A$ and the set of prime ideals of A that do not intersect $S .$ The inverse map is $\\mathfrak{p} \\mapsto \\mathfrak{p} S^{-1} A$.\n",
    "\n",
    "Proof. See [1, Cor.11.20] or [2, Prop. 3.11.iv].\n",
    "\n",
    "# See Also\n",
    "\n",
    "# Meta\n",
    "## References\n",
    "![[_reference_18785]]\n",
    "\n",
    "## Citations and Footnotes\n",
    "[^1]: Sutherland, Theorem 2.1, Page 11\"\"\"\n",
    "\n",
    "mock_notat_note.name = '18785_notation_S_minus_1_A_localization_of_a_commutative_ring_with_respect_to_a_multiplicative_subset'\n",
    "where_to_add = {254: [mock_notat_note]}\n",
    "output = _add_notation_note_embedded_footnotes(text, where_to_add)\n",
    "print(output)\n",
    "\n",
    "\n",
    "text = r\"\"\"\n",
    "\n",
    "For each integer $m$ and each transitive $G \\leq S_m$, there are constants $C(G), Q(G)$, and $e(G)$ such that, for all $q>Q(G)$ coprime to $\\#G$ and all $X>0$, \n",
    "\n",
    "$$N_G(\\mathbb{F}_q(t),X) \\leq C(G) X^{a(G)} \\log(X)^{e(G)}$$\n",
    "\n",
    "blah blah\n",
    "\n",
    "\"\"\"\n",
    "mock_notat_note = MagicMock()\n",
    "mock_notat_note.name = 'notat_note_name'\n",
    "where_to_add = {224: [mock_notat_note]}\n",
    "output = _add_notation_note_embedded_footnotes(text, where_to_add)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def add_notation_note_embedded_footnotes_to_info_note(\n",
    "        origin_note: VaultNote, # An info note\n",
    "        relied_notes: Optional[VaultNote | list[VaultNote]] = None, # notation notes to add embedded footnotes for.\n",
    "        cache: Optional[dict[str, dict[str, list[NoteLinkEnum]]]] = None, # The cache from which to identify the notation notes to add embedded footnotes for.\n",
    "        locate_by: Literal['first', 'best'] = 'best',\n",
    "        threshold: float = 0.8,\n",
    "        ):\n",
    "    \"\"\"\n",
    "    Modify the contents of `origin_note` to add footnotes to embedded links to `relied_notes`\n",
    "\n",
    "    One of `relied_notes` or `cache` must be passed.\n",
    "    \"\"\"\n",
    "    if relied_notes is None and cache is None:\n",
    "        raise ValueError(\"Expected `relied_note` or `cache` to be specified, but both were `None`.\")\n",
    "    if relied_notes is None:\n",
    "        if origin_note.name not in cache:\n",
    "            print(f'`origin_note.name` is not in `cache`. `origin_note` is {origin_note}.')\n",
    "            return\n",
    "        cache = remove_nonexistent_note_names_from_cache(cache, origin_note.vault)\n",
    "        relied_notes: list[VaultNote] = []\n",
    "        for relied_note_name, link_enums in cache[origin_note.name].items():\n",
    "            relied_note = VaultNote(origin_note.vault, name=relied_note_name)\n",
    "            if not note_is_of_type(relied_note, PersonalNoteTypeEnum.NOTATION_NOTE):\n",
    "                continue\n",
    "            if NoteLinkEnum.INFO_TO_NOTAT_VIA_EMBEDDING in link_enums:\n",
    "                relied_notes.append(relied_note)\n",
    "    if isinstance(relied_notes, VaultNote):\n",
    "        relied_notes = [relied_notes]\n",
    "\n",
    "    # Try to only add embedded links to notation notes that do not already exist in `origin_note`\n",
    "    origin_note_text = origin_note.text()\n",
    "    embedded_links_in_text: list[ObsidianLink] = links_from_text(\n",
    "        origin_note_text, ObsidianLink(\n",
    "            is_embedded=True, file_name=-1, anchor=-1, custom_text=-1, link_type=LinkType.WIKILINK))\n",
    "    embedded_note_names_in_text: set[str] = set([link.file_name for link in embedded_links_in_text])\n",
    "    new_relied_notes: list[VaultNote] = [\n",
    "        relied_note for relied_note in relied_notes if relied_note.name not in embedded_note_names_in_text]\n",
    "\n",
    "    where_to_add: dict[int, list[VaultNote]] = _where_to_add_notation_links(\n",
    "        origin_note, new_relied_notes, locate_by, threshold)\n",
    "    new_text = _add_notation_note_embedded_footnotes(origin_note.text(), where_to_add)\n",
    "    origin_note.write(new_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# origin_note = VaultNote(vault, name='18785_Theorem 2.1')\n",
    "# notat_note = VaultNote(vault, name='18785_notation_S_minus_1_A_localization_of_a_commutative_ring_with_respect_to_a_multiplicative_subset')\n",
    "# add_notation_note_embedded_footnotes_to_info_note(\n",
    "#     origin_note, notat_note,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(origin_note.text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notation Summarization using `NoteData` classes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions above surrounding the `NoteData` classes should actually be close to providing the means for gathering data for other ML tasks, such as the summarization task (thus far provided by `25_machine_learning.notation.summarization.ipynb`) and the definition naming task (thus far provided by `35_machine_learning.definition_and_notation_naming.ipynb`), and even improve upon them by providing contextual data (given by the notes linked by a given note)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SummarizationDataPoint(TypedDict):\n",
    "    input: str\n",
    "    output: str\n",
    "    notat_note_name: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def summarization_data(\n",
    "        notat_note_data_point: NotatNoteData,\n",
    "        info_note_data: dict[str, InfoNoteData], # For getting data from the linked notes.\n",
    "        notat_note_data: dict[str, NotatNoteData], # For getting data from the linked notes.\n",
    "        format: Literal['bert', 't5'],\n",
    "        augmentation: Optional[Literal['high', 'mid', 'low']] = None,\n",
    "        ) -> SummarizationDataPoint:\n",
    "    \"\"\"\n",
    "    Compile the summarization data from a `NotatNoteData` . \n",
    "\n",
    "    `notat_note_data_point` must have a nonblank value for its `note_content` attribute.\n",
    "\n",
    "    The summarization data consists of the notation note's main note content and (optionally)\n",
    "    the position data (which is the position data of the main note) along with (also optionally)\n",
    "    the content and/or position data of info notes that either the notation note or its\n",
    "    main note depend on (via the `NoteLinkEnum.NOTAT_TO_INFO_VIA_NOTAT` or \n",
    "    `NoteLinkEnum.INFO_TO_INFO_IN_CONTENT` enum items).\n",
    "\n",
    "    The augmentations are not applied to `notat_note_data_point` but are rather applied to\n",
    "    (copies of) the info notes that the notation note or its main note depends on. Use\n",
    "    `augment_notat_note_data_for_summarization` to augment that `NotatNoteData` object.\n",
    "\n",
    "    **Raises**\n",
    "    - `ValueError`\n",
    "        - if `notat_note_data_point.note_content` is not a nonblank `str`.\n",
    "    \"\"\"\n",
    "    content = notat_note_data_point.note_content\n",
    "    if content is None or content.strip() == '':\n",
    "        raise ValueError(f\"Expected `notat_note_data_point.content` to be a non blank string but was {notat_note_data_point.content}. The relevant notation note name is {notat_note_data_point.note_name}.\")\n",
    "    # Temporarily blank out the `note_content` attribute for the `.data_string` method\n",
    "    # To exclude whatever is in the `note_content` attribute.\n",
    "    notat_note_data_point.note_content = None\n",
    "    notat_note_string = notat_note_data_point.data_string(format)\n",
    "    notat_note_data_point.note_content = content\n",
    "\n",
    "    info_note_names_to_consider: set[str] = set()\n",
    "    for relied_note_name, link_types in notat_note_data_point.directly_linked_notes.items():\n",
    "        if NoteLinkEnum.NOTAT_TO_INFO_VIA_NOTAT in link_types:\n",
    "            info_note_names_to_consider.add(relied_note_name)\n",
    "    if (notat_note_data_point.main_note\n",
    "            and notat_note_data_point.main_note in info_note_data):\n",
    "        main_note_data_point = info_note_data[notat_note_data_point.main_note]\n",
    "        for relied_note_name, link_types in main_note_data_point.directly_linked_notes.items():\n",
    "            if NoteLinkEnum.INFO_TO_INFO_IN_CONTENT in link_types:\n",
    "                info_note_names_to_consider.add(relied_note_name)\n",
    "\n",
    "    parts: list[str] = [notat_note_string]\n",
    "    for relied_note_name in list(info_note_names_to_consider):\n",
    "        if not relied_note_name in info_note_data:\n",
    "            continue\n",
    "        relied_note_data_point = info_note_data[relied_note_name]\n",
    "        relied_note_data_point = relied_note_data_point.deepcopy()\n",
    "        if augmentation:\n",
    "            erase_position_metadata = _erase_position_metadata(augmentation)\n",
    "            relied_note_data_point.randomly_modify(augmentation, erase_position_metadata)\n",
    "        parts.append(relied_note_data_point.data_string(format))\n",
    "\n",
    "    sep_str = '\\n\\n[SEP]\\n\\n' if format == 'bert' else '\\n\\n</s>\\n\\n'\n",
    "    input = sep_str.join(parts)\n",
    "    output = notat_note_data_point.note_content\n",
    "    return SummarizationDataPoint(\n",
    "        input=input, output=output,\n",
    "        notat_note_name=notat_note_data_point.note_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def augment_notat_note_data_for_summarization(\n",
    "        notat_note_data_point: NotatNoteData,\n",
    "        augmentation: Literal['high', 'mid' ,'low'],\n",
    "        info_note_data: dict[str, InfoNoteData], # For getting data from the linked notes.\n",
    "        modify_links: bool = True, # If `True`, randomly modify the linking data from that of `notat_note_data_point`\n",
    "        ) -> NotatNoteData:\n",
    "    \"\"\"\n",
    "    Return a modified copy of `notat_note_data` augmented for providing\n",
    "    notation summarization data.\n",
    "\n",
    "    The `note_content` attribute of the outputted copy should not be modified\n",
    "    as it serves as the output of the training data.\n",
    "    \"\"\"\n",
    "    notat_note_data_copy = notat_note_data_point.deepcopy()\n",
    "    erase_position_metadata = _erase_position_metadata(augmentation)\n",
    "    content = notat_note_data_copy.note_content\n",
    "    notat_note_data_copy.randomly_modify(augmentation, erase_position_metadata)\n",
    "    notat_note_data_copy.note_content = content\n",
    "\n",
    "    if modify_links:\n",
    "        if augmentation == 'high':\n",
    "            num_rand_info_note_data_to_add = 3\n",
    "            key_deletion_prob = 0.10\n",
    "        elif augmentation == 'mid':\n",
    "            num_rand_info_note_data_to_add = 2\n",
    "            key_deletion_prob = 0.05\n",
    "        elif augmentation == 'low':\n",
    "            num_rand_info_note_data_to_add = 1\n",
    "            key_deletion_prob = 0.02\n",
    "        else:\n",
    "            num_rand_info_note_data_to_add = 0\n",
    "            key_deletion_prob = 0\n",
    "        # Delete \"links\" at random.\n",
    "        keys = list(notat_note_data_copy.directly_linked_notes)\n",
    "        for key in keys:\n",
    "            if random.random() < key_deletion_prob:\n",
    "                notat_note_data_copy.directly_linked_notes.pop(key)\n",
    "        # Add \"links\" to info notes at random.\n",
    "        random_info_note_names_to_add = random.choices(\n",
    "            list(info_note_data), k=min(len(info_note_data), num_rand_info_note_data_to_add))\n",
    "        for random_info_note_name in random_info_note_names_to_add:\n",
    "            _update_dict(\n",
    "                notat_note_data_copy.directly_linked_notes, \n",
    "                random_info_note_name,\n",
    "                NoteLinkEnum.NOTAT_TO_INFO_VIA_NOTAT)\n",
    "    return notat_note_data_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def notat_note_data_admissible_for_summarization_data(\n",
    "        notat_note_data_point: NotatNoteData\n",
    "        ) -> bool:  # `True` if the notation note data does not have the `_auto/notation_summary` tag, and the content of the notation note is essentially note blank.\n",
    "    if notat_note_data_point.tags and '_auto/notation_summary' in notat_note_data_point.tags:\n",
    "        return False\n",
    "    if notat_note_data_point.note_content is None:\n",
    "        return False \n",
    "    return bool(notat_note_data_point.note_content.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for name, data_point in notat_note_data.items():\n",
    "#     if notat_note_data_admissible_for_summarization_data(data_point):\n",
    "#         print(name)\n",
    "#         break\n",
    "\n",
    "# summ_data = summarization_data(data_point, info_note_data, notat_note_data, 'bert')\n",
    "# print(summ_data['input'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notat_note_data_admissible_for_summarization_data(notat_note_data['achter_pries_imht_notation_T_bar_gamma_smooth_trielliptic_curves_with_inertia_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _add_augmented_data_points(\n",
    "        info_note_data: dict[str, InfoNoteData],\n",
    "        notat_note_data: dict[str, NotatNoteData],\n",
    "        format: Literal['bert', 't5'],\n",
    "        data_point: NotatNoteData,\n",
    "        dict_data: list[SummarizationDataPoint],\n",
    "        ):\n",
    "    \"\"\"\n",
    "    Helper function to `summarization_dataset_from_note_data`.\n",
    "    \"\"\"\n",
    "    data_point_without_links = data_point.deepcopy()\n",
    "    data_point_without_links.directly_linked_notes = {}\n",
    "    for augmentation in ['low', 'mid', 'high']:\n",
    "        for modify_links, base_data_point in zip([True, False], [data_point, data_point_without_links]):\n",
    "            aug_data_point: NotatNoteData = augment_notat_note_data_for_summarization(\n",
    "                base_data_point, augmentation, info_note_data, modify_links)\n",
    "            dict_data.append(summarization_data(\n",
    "                aug_data_point, info_note_data, notat_note_data, format, augmentation))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def summarization_dataset_from_note_data(\n",
    "        info_note_data: dict[str, InfoNoteData],\n",
    "        notat_note_data: dict[str, NotatNoteData],\n",
    "        augment: bool,\n",
    "        format: Literal['bert', 't5'],\n",
    "        ) -> Dataset:\n",
    "    admissible_notat_note_names: list[str] = []\n",
    "    for notat_note_name, data_point in notat_note_data.items():\n",
    "        if notat_note_data_admissible_for_summarization_data(data_point):\n",
    "            admissible_notat_note_names.append(notat_note_name)\n",
    "    dict_data: list[SummarizationDataPoint] = []\n",
    "    for admissible_notat_note_name in admissible_notat_note_names:\n",
    "        data_point: NotatNoteData = notat_note_data[admissible_notat_note_name]\n",
    "        dict_data.append(summarization_data(\n",
    "            data_point, info_note_data, notat_note_data, format, augmentation=None))\n",
    "        if not augment:\n",
    "            continue\n",
    "        _add_augmented_data_points(info_note_data, notat_note_data, format, data_point, dict_data)\n",
    "    return Dataset.from_list(dict_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
