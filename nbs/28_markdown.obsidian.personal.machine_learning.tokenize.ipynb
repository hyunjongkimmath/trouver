{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp markdown.obsidian.personal.machine_learning.tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# markdown.obsidian.personal.machine_learning.tokenize\n",
    "> Functions for gathering and processing tokenization data and for using ML models trained with such data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previous, `trouver` just had functionalities for using ML models to identify newly introduced notations in text and for gathering data to train such models. Moreover, such models were merely classification models, and using these models to identify newly introduced notations had a lot of computational redundancies.\n",
    "\n",
    "This module aims to provide the same functionalities for both definitions and notations by training and using token classification models instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os \n",
    "from os import PathLike\n",
    "from pathlib import Path\n",
    "from typing import Union\n",
    "\n",
    "import bs4\n",
    "import pandas as pd\n",
    "from transformers import BatchEncoding, PreTrainedTokenizer, PreTrainedTokenizerFast\n",
    "\n",
    "from trouver.helper import current_time_formatted_to_minutes, definition_asterisk_indices, double_asterisk_indices, notation_asterisk_indices, replace_string_by_indices, remove_html_tags_in_text\n",
    "from trouver.markdown.markdown.file import MarkdownFile\n",
    "from trouver.markdown.obsidian.personal.note_processing import process_standard_information_note\n",
    "from trouver.markdown.obsidian.vault import VaultNote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unittest import mock\n",
    "\n",
    "from datasets import ClassLabel, Dataset, Features, Sequence, Value\n",
    "from transformers import AutoTokenizer\n",
    "from fastcore.test import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather ML data from information notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def convert_double_asterisks_to_html_tags(\n",
    "        text: str\n",
    "        ) -> str:\n",
    "    \"\"\"\n",
    "    Replace the double asterisks, which signify definitions and notations,\n",
    "    in `text` with HTML tags.\n",
    "    \"\"\"\n",
    "    double_asts = double_asterisk_indices(text)\n",
    "    replacement_html_tags = [\n",
    "        _html_tag_from_double_ast(text[start:end])\n",
    "        for start, end in double_asts]\n",
    "    return replace_string_by_indices(\n",
    "        text, double_asts, replacement_html_tags)\n",
    "\n",
    "\n",
    "def _html_tag_from_double_ast(\n",
    "        double_ast_string: str # Starts and ends with double asts\n",
    "        ) -> str:\n",
    "    \"\"\"\n",
    "    Get the HTML tag representing definition or notation data from\n",
    "    a string surrounded by double asterisks.\n",
    "\n",
    "    This is used in the `_convert_double_asterisks_to_html_tags` function.\n",
    "    \"\"\"\n",
    "    no_asts = double_ast_string[2:-2]\n",
    "    if notation_asterisk_indices(double_ast_string):\n",
    "        return f'<span notation=\"\">{no_asts}</span>'\n",
    "    else:\n",
    "        return f'<b definition=\"\">{no_asts}</b>'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<b definition=\"\">hi</b>. Here is a notation <span notation=\"\">$asdf$</span>\n"
     ]
    }
   ],
   "source": [
    "print(convert_double_asterisks_to_html_tags(\"**hi**. Here is a notation **$asdf$**\"))\n",
    "test_eq(convert_double_asterisks_to_html_tags(\"**hi**. Here is a notation **$asdf$**\"), '<b definition=\"\">hi</b>. Here is a notation <span notation=\"\">$asdf$</span>')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def raw_text_with_html_tags_from_markdownfile(\n",
    "        mf: MarkdownFile,\n",
    "        vault: PathLike\n",
    "        ) -> str:\n",
    "    \"\"\"\n",
    "    Process the `MarkdownFile`, replacing the double asterisk surrounded\n",
    "    text indicating definitions and notations to be HTML tags instead.\n",
    "    \"\"\"\n",
    "    mf = process_standard_information_note(\n",
    "        mf, vault, remove_double_asterisks=False,\n",
    "        remove_html_tags=False)\n",
    "    return convert_double_asterisks_to_html_tags(str(mf))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Some kind of potato[^2]\\n\\n[^2]: Some footnote\\n\\nSome link\\n'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "\n",
    "# TODO: \n",
    "# I want to make sure that footnotes are getting properly removed.\n",
    "mf = MarkdownFile.from_string(\n",
    "    r\"\"\"---\n",
    "aliases: []\n",
    "tags: []\n",
    "---\n",
    "# Something  \n",
    "\n",
    "Some kind of potato[^2]\n",
    "\n",
    "[^2]: Some footnote\n",
    "\n",
    "[[link_to_note|Some link]]\n",
    "\n",
    "\n",
    "# See Also\n",
    "# Meta\n",
    "## References and Citations\n",
    "\"\"\") \n",
    "raw_text_with_html_tags_from_markdownfile(mf, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "mf = MarkdownFile.from_string(\n",
    "    r\"\"\"---\n",
    "aliases: []\n",
    "tags: []\n",
    "---\n",
    "# Galois group of a separable and normal finite field extension\n",
    "\n",
    "Let $L/K$ be a separable and normal finite field extension. Its <b definition=\"Galois group of a separable and normal finite field extension\">Galois group</b> <span notation=\"\">$\\operatorname{Gal}(L/K)$</span> is...\n",
    "\n",
    "# Galois group of a separable and normal profinite field extension\n",
    "\n",
    "In fact, the notion of a Galois group can be defined for profinite field extensions. Given a separable and normal profinite field extension $L/K$, say that\n",
    "$L = \\varinjlim_i L_i$ where $L_i/K$ are finite extensions. Its **Galois group** **$\\operatorname{Gal}(L/K)$**\n",
    "\n",
    "# See Also\n",
    "# Meta\n",
    "## References and Citations\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following example, let `mf` be the following `MarkdownFile`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "aliases: []\n",
      "tags: []\n",
      "---\n",
      "# Galois group of a separable and normal finite field extension\n",
      "\n",
      "Let $L/K$ be a separable and normal finite field extension. Its <b definition=\"Galois group of a separable and normal finite field extension\">Galois group</b> <span notation=\"\">$\\operatorname{Gal}(L/K)$</span> is...\n",
      "\n",
      "# Galois group of a separable and normal profinite field extension\n",
      "\n",
      "In fact, the notion of a Galois group can be defined for profinite field extensions. Given a separable and normal profinite field extension $L/K$, say that\n",
      "$L = \\varinjlim_i L_i$ where $L_i/K$ are finite extensions. Its **Galois group** **$\\operatorname{Gal}(L/K)$**\n",
      "\n",
      "# See Also\n",
      "# Meta\n",
      "## References and Citations\n"
     ]
    }
   ],
   "source": [
    "print(str(mf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `raw_text_with_html_tags_from_markdownfile` function processes the `MarkdownFile` much in the same way as the `process_standard_information_note` function, except it 1. preserves HTML tags, and 2. replaces text surrounded by double asterisks `**` with HTML tags signifiying whether the text displays a definition or a notation.\n",
    "\n",
    "In the below example, note that the `vault` parameter is set to `None`; this is fine for this example becaues the `process_standard_information_note` function only needs a `vault` argument when embedded links need to be replaced with text (via the `MarkdownFile.replace_embedded_links_with_text` function), but `mf` has no embedded links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let $L/K$ be a separable and normal finite field extension. Its <b definition=\"Galois group of a separable and normal finite field extension\">Galois group</b> <span notation=\"\">$\\operatorname{Gal}(L/K)$</span> is...\n",
      "\n",
      "In fact, the notion of a Galois group can be defined for profinite field extensions. Given a separable and normal profinite field extension $L/K$, say that\n",
      "$L = \\varinjlim_i L_i$ where $L_i/K$ are finite extensions. Its <b definition=\"\">Galois group</b> <span notation=\"\">$\\operatorname{Gal}(L/K)$</span>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(raw_text_with_html_tags_from_markdownfile(mf, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "assert '**' not in raw_text_with_html_tags_from_markdownfile(mf, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# TODO: implement a measure to not get the definition identification data, e.g. by \n",
    "# detecting a `_auto/definition_identification` tag.\n",
    "def html_data_from_note(\n",
    "        note: VaultNote,\n",
    "        vault: PathLike\n",
    "        ) -> Union[dict, None]: # The keys to the dict are \"Note name\", \"Raw text\", \"Tag data\". However, `None` is returned if `note` does not exist or the note is marked with auto-generated, unverified data.\n",
    "    # TODO: implement obtaining multiple datapoints from a single note\n",
    "    # Via typos for example.\n",
    "    \"\"\"Obtain html data for token classification from the information note.\n",
    "\n",
    "    Currently, the token types mainly revolve around definitions and\n",
    "    notations.\n",
    "\n",
    "    If `note` has the tag `_auto/def_and_notat_identified`, then the data\n",
    "    in the note is assumed to be auto-generated and not verified and\n",
    "    `None` is returned.\n",
    "\n",
    "    **Returns**\n",
    "    - Union[dict, None]\n",
    "        - The keys-value pairs are \n",
    "            - `\"Note name\"` - The name of the note\n",
    "            - `\"Raw text\"` - The raw text to include in the data.\n",
    "            - `\"Tag data\"` - The list with HTML tags carrying definition/notation\n",
    "              data and their locations in the Raw text. See the second output to\n",
    "              the function `remove_html_tags_in_text`.\n",
    "                - Each element of the list is a tuple consisting of a ``bs4.element.Tag``\n",
    "                  and two ints.\n",
    "    \"\"\"\n",
    "    if not note.exists():\n",
    "        return None\n",
    "    mf = MarkdownFile.from_vault_note(note)\n",
    "    if mf.has_tag('_auto/def_and_notat_identified'):\n",
    "        return None\n",
    "    raw_text_with_tags = raw_text_with_html_tags_from_markdownfile(mf, vault)\n",
    "    raw_text, tags_and_locations = remove_html_tags_in_text(raw_text_with_tags)\n",
    "    return {\n",
    "        \"Note name\": note.name,\n",
    "        \"Raw text\": raw_text,\n",
    "        \"Tag data\": tags_and_locations}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following example, we mock a `VaultNote` whose content is that of `mf` in the example for the `raw_text_with_html_tags_from_markdownfile` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Note name': \"Note's name\", 'Raw text': 'Let $L/K$ be a separable and normal finite field extension. Its Galois group $\\\\operatorname{Gal}(L/K)$ is...\\n\\nIn fact, the notion of a Galois group can be defined for profinite field extensions. Given a separable and normal profinite field extension $L/K$, say that\\n$L = \\\\varinjlim_i L_i$ where $L_i/K$ are finite extensions. Its Galois group $\\\\operatorname{Gal}(L/K)$\\n', 'Tag data': [(<b definition=\"Galois group of a separable and normal finite field extension\">Galois group</b>, 64, 76), (<span notation=\"\">$\\operatorname{Gal}(L/K)$</span>, 77, 102), (<b definition=\"\">Galois group</b>, 330, 342), (<span notation=\"\">$\\operatorname{Gal}(L/K)$</span>, 343, 368)]}\n"
     ]
    }
   ],
   "source": [
    "with (mock.patch('__main__.VaultNote') as mock_VaultNote,\n",
    "      mock.patch('__main__.MarkdownFile.from_vault_note') as mock_from_vault_note):\n",
    "    mock_VaultNote.exists.return_value = True\n",
    "    mock_VaultNote.name = \"Note's name\"\n",
    "    mock_from_vault_note.return_value = mf\n",
    "\n",
    "    html_data = html_data_from_note(mock_VaultNote, None)\n",
    "    print(html_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def tokenize_html_data(\n",
    "        html_locus: dict, # An output of `html_data_from_note`\n",
    "        tokenizer: Union[PreTrainedTokenizer, PreTrainedTokenizerFast],\n",
    "        max_length: int, # Max length for each sequence of tokens\n",
    "        ner_tag_from_html_tag: callable, # takes in a bs4.element.Tag and outputs the ner_tag (as a string or `None`)\n",
    "        label2id: dict[str, int], # The keys ner_tag's of the form f\"I-{output}\" or f\"B-{output}\" where `output` is an output of `ner_tag_from_html_tag`.\n",
    "        default_label: str = \"O\", # The default label for the NER tagging.\n",
    "        ) -> tuple[list[list[str]], list[list[int]]]: # The first list consists of the tokens and the second list consists of the named entity recognition tags.\n",
    "    \"\"\"Actually tokenize the html data outputted by `html_data_from_note`.\n",
    "\n",
    "    To account for the possibility that the raw text is long,\n",
    "    this function uses the `tokenizer.batch_encode_plus` function\n",
    "    to tokenize the text into sequences. \n",
    "    \"\"\"\n",
    "    tokenized = tokenizer.batch_encode_plus(\n",
    "        [html_locus[\"Raw text\"]], max_length=max_length, return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True, truncation=True)\n",
    "\n",
    "    default_id = label2id[default_label]        \n",
    "    ner_ids = [[default_id for _ in seq_input_ids]\n",
    "               for seq_input_ids in tokenized['input_ids']]\n",
    "    for tag, start, end in html_locus[\"Tag data\"]:\n",
    "        ner_tag = ner_tag_from_html_tag(tag)\n",
    "        if ner_tag is None:\n",
    "            continue  # `ner_tag` is not of relevant data.\n",
    "        tuppy = _start_end_seqs_indices_for_html_tag(tokenized, start, end - 1)\n",
    "        (start_seq, start_index_in_seq), (end_seq, end_index_in_seq) = tuppy\n",
    "        _set_ner_ids_for_tag(\n",
    "            ner_ids, start_seq, start_index_in_seq, end_seq, end_index_in_seq,\n",
    "            label2id, ner_tag)\n",
    "    return tokenized[\"input_ids\"], ner_ids\n",
    "\n",
    "\n",
    "def _start_end_seqs_indices_for_html_tag(\n",
    "        tokenized: BatchEncoding,\n",
    "        tag_start_ind: int,\n",
    "        tag_end_ind: int\n",
    "        ) -> tuple[tuple[int, int], tuple[int, int]]: # The first tuple is `(a, b)` where `tokenized['input_ids'][a][b]` is the token corresponding to the start of the HTML tag's (raw) text. The second tuple is `(c, d)` where `tokenized['input_ids'][c][d]` is the token corresponding to the end of the HTML tag's (raw) text.\n",
    "    start_seq = _search_seq_ind_for_char(tokenized['offset_mapping'], tag_start_ind)\n",
    "    # start_index_in_seq = tokenized.char_to_token(batch_or_char_index=start_seq, char_index=tag_start_ind)\n",
    "    start_index_in_seq = _search_within_seq_for_char(tokenized['offset_mapping'][start_seq], tag_start_ind)\n",
    "    end_seq = _search_seq_ind_for_char(tokenized['offset_mapping'], tag_end_ind)\n",
    "    # end_index_in_seq = tokenized.char_to_token(batch_or_char_index=end_seq, char_index=tag_end_ind)\n",
    "    end_index_in_seq = _search_within_seq_for_char(tokenized['offset_mapping'][end_seq], tag_end_ind)\n",
    "    return (start_seq, start_index_in_seq), (end_seq, end_index_in_seq)\n",
    "\n",
    "\n",
    "def _min_max_char_ind_for_seq(\n",
    "        offset_for_seq: list[tuple[int,int]] # An item in tokenized['offset_mapping']\n",
    "        ):\n",
    "    min_char_ind, max_char_ind = 0, 0\n",
    "    for inds in offset_for_seq:\n",
    "        if inds != (0,0):\n",
    "            min_char_ind = inds[0]\n",
    "            break\n",
    "    for inds in reversed(offset_for_seq):\n",
    "        if inds != (0,0):\n",
    "            max_char_ind = inds[1]\n",
    "            break\n",
    "    return min_char_ind, max_char_ind\n",
    "\n",
    "def _char_is_in_seq(\n",
    "        offset_for_seq: list[int], # An item in tokenized['offset_mapping']\n",
    "        char: int # The index of a character in the original raw text\n",
    "        ) -> bool:\n",
    "    min_char_ind, max_char_ind = _min_max_char_ind_for_seq(offset_for_seq)\n",
    "    return min_char_ind <= char and char < max_char_ind\n",
    "\n",
    "def _search_seq_ind_for_char(\n",
    "        offsets: list[tuple[int, int]], # tokenized['offset_mapping']\n",
    "        char: int # The index of a character in the original raw text\n",
    "        ) -> int:\n",
    "    \"\"\"\n",
    "    Binary search the index of the sequence containing the token at the \n",
    "    location of the index `char` within the original (raw) text.\n",
    "\n",
    "    Based on pseudocode from https://pseudoeditor.com/guides/binary-search\n",
    "    \"\"\"\n",
    "    left = 0\n",
    "    right = len(offsets) - 1\n",
    "    while left <= right:\n",
    "        mid = (left + right) // 2\n",
    "        min_char_ind, max_char_ind = _min_max_char_ind_for_seq(offsets[mid])\n",
    "        if min_char_ind <= char and char < max_char_ind:\n",
    "            return mid\n",
    "        elif max_char_ind <= char:\n",
    "            left = mid + 1\n",
    "        else:\n",
    "            right = mid - 1\n",
    "    return -1  # This should not be returned under normal use.\n",
    "\n",
    "\n",
    "def _search_within_seq_for_char(\n",
    "        seq_offset: list[tuple[int, int]],\n",
    "        char: int\n",
    "    ) -> int:\n",
    "    \"\"\"\n",
    "    Binary search for the index within the sequence corresponding\n",
    "    to the token at the location of the index `char` within the\n",
    "    original (raw) text.\n",
    "\n",
    "    Based on pseudocode from https://pseudoeditor.com/guides/binary-search\n",
    "    \"\"\"\n",
    "    left = 0\n",
    "    right = len(seq_offset) - 1\n",
    "    while left <= right:\n",
    "        mid = (left + right) // 2\n",
    "        min_char_ind, max_char_ind = seq_offset[mid] \n",
    "        if min_char_ind <= char and char < max_char_ind:\n",
    "            return mid\n",
    "        elif max_char_ind <= char:\n",
    "            left = mid + 1\n",
    "        else:\n",
    "            right = mid - 1\n",
    "    return -1  # This should not be returned under normal use.\n",
    "\n",
    "\n",
    "def _set_ner_ids_for_tag(\n",
    "        ner_ids: list[list[int]],\n",
    "        start_seq: int, \n",
    "        start_index_in_seq: int,\n",
    "        end_seq: int,\n",
    "        end_index_in_seq: int,\n",
    "        label2id: dict[str, int],\n",
    "        ner_tag: str\n",
    "        ) -> None:\n",
    "    \"\"\"\n",
    "    After the locations of the tokens corresponding to a HTML tag have been found, \n",
    "    mark within `ner_ids` the appropriate NER tags at the locations corresponding\n",
    "    to the tokens' locations.\n",
    "    \"\"\"\n",
    "    ner_ids[start_seq][start_index_in_seq] = label2id[f\"B-{ner_tag}\"]\n",
    "    i_ner_id = label2id[f\"I-{ner_tag}\"]\n",
    "    seq, ind = start_seq, start_index_in_seq + 1\n",
    "    while seq < end_seq or ind <= end_index_in_seq:\n",
    "        if len(ner_ids[seq]) <= ind:\n",
    "            seq += 1\n",
    "            ind = 0\n",
    "        else:\n",
    "            ner_ids[seq][ind] = i_ner_id \n",
    "            ind += 1\n",
    "    \n",
    "\n",
    "\n",
    "def def_or_notat_from_html_tag(\n",
    "        tag: bs4.element.Tag\n",
    "        ) -> Union[str, None]:\n",
    "    \"\"\"\n",
    "    Can be passed as the `ner_tag_from_html_tag` argument in `tokenize_html_data`\n",
    "    for the purposes of compiling a dataset for definition and notation\n",
    "    identification.\n",
    "\n",
    "    The strings f\"I-{output}\" and f\"B-{output}\" are valid ner_tags. To use for \n",
    "    \"\"\"\n",
    "    if \"definition\" in tag.attrs:\n",
    "        return \"definition\"\n",
    "    elif \"notation\" in tag.attrs:\n",
    "        return \"notation\"\n",
    "    return None  # If the HTML tag carries neither definition nor notation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_eq(_min_max_char_ind_for_seq([(0,0), (1,3), (3,4), (4,7), (7,15), (0,0)]), (1,15))\n",
    "\n",
    "offsets = [[(0,0), (0,3), (4,5), (5,6), (6,7), (7,8), (8,9),],\n",
    "           [(10,12), (13,14), (15,18), (18,24)],\n",
    "           [(25,28), (29,35), (36,42), ]]\n",
    "test_eq(_search_seq_ind_for_char(offsets, 0), 0)\n",
    "test_eq(_search_seq_ind_for_char(offsets, 1), 0)\n",
    "test_eq(_search_seq_ind_for_char(offsets, 5), 0)\n",
    "test_eq(_search_seq_ind_for_char(offsets, 8), 0)\n",
    "# I don't think that character index 9 is something that I need to worry about.\n",
    "test_eq(_search_seq_ind_for_char(offsets, 10), 1)\n",
    "test_eq(_search_seq_ind_for_char(offsets, 23), 1)\n",
    "test_eq(_search_seq_ind_for_char(offsets, 25), 2)\n",
    "test_eq(_search_seq_ind_for_char(offsets, 41), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We continue with an example using the HTML data from the example for the `html_data_from_note` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(<b definition=\"Galois group of a separable and normal finite field extension\">Galois group</b>,\n",
       "  64,\n",
       "  76),\n",
       " (<span notation=\"\">$\\operatorname{Gal}(L/K)$</span>, 77, 102),\n",
       " (<b definition=\"\">Galois group</b>, 330, 342),\n",
       " (<span notation=\"\">$\\operatorname{Gal}(L/K)$</span>, 343, 368)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html_data['Raw text']\n",
    "html_data[\"Tag data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {\n",
    "    \"O\": 0,\n",
    "    \"B-definition\": 1,\n",
    "    \"I-definition\": 2,\n",
    "    \"B-notation\": 3,\n",
    "    \"I-notation\": 4\n",
    "}\n",
    "token_ids, ner_tag_ids = tokenize_html_data(html_data, tokenizer, 510, def_or_notat_from_html_tag, label2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, `max_length` is set to 510 (tokens). The string (\"Raw text\") is not very long, so only one sequence should be present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(len(token_ids), 1)\n",
    "test_eq(len(ner_tag_ids), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us see what has been tagged:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'O',\n",
       " 1: 'B-definition',\n",
       " 2: 'I-definition',\n",
       " 3: 'B-notation',\n",
       " 4: 'I-notation'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label = {value: key for key, value in label2id.items()}\n",
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gal\t\tB-definition\n",
      "##ois\t\tI-definition\n",
      "group\t\tI-definition\n",
      "$\t\tB-notation\n",
      "\\\t\tI-notation\n",
      "operator\t\tI-notation\n",
      "##name\t\tI-notation\n",
      "{\t\tI-notation\n",
      "gal\t\tI-notation\n",
      "}\t\tI-notation\n",
      "(\t\tI-notation\n",
      "l\t\tI-notation\n",
      "/\t\tI-notation\n",
      "k\t\tI-notation\n",
      ")\t\tI-notation\n",
      "$\t\tI-notation\n",
      "gal\t\tB-definition\n",
      "##ois\t\tI-definition\n",
      "group\t\tI-definition\n",
      "$\t\tB-notation\n",
      "\\\t\tI-notation\n",
      "operator\t\tI-notation\n",
      "##name\t\tI-notation\n",
      "{\t\tI-notation\n",
      "gal\t\tI-notation\n",
      "}\t\tI-notation\n",
      "(\t\tI-notation\n",
      "l\t\tI-notation\n",
      "/\t\tI-notation\n",
      "k\t\tI-notation\n",
      ")\t\tI-notation\n",
      "$\t\tI-notation\n"
     ]
    }
   ],
   "source": [
    "for token_id, ner_tag in zip(token_ids[0], ner_tag_ids[0]):\n",
    "    if ner_tag != 0:\n",
    "        print(f\"{tokenizer.decode(token_id)}\\t\\t{id2label[ner_tag]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us set `max_length` to be shorter to observe an example of a tokenization of a single text across multiple sequences (Of course, in practice, the max token length would be set to be longer, say around 512 or 1024.):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids, ner_tag_ids = tokenize_html_data(html_data, tokenizer, 20, def_or_notat_from_html_tag, label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "print(len(token_ids))\n",
    "print(len(ner_tag_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2],\n",
       " [2, 2, 2, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 3, 4, 4],\n",
       " [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0]]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_tag_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is sample code to then gather data for definition/notation identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "notes = [] # Replace with actual notes\n",
    "vault = '' # Replace with actual vault\n",
    "\n",
    "html_data = [html_data_from_note(note, vault) for note in notes]\n",
    "max_length = 1022\n",
    "\n",
    "tokenized_html_data = [tokenize_html_data(html_locus, tokenizer, max_length, def_or_notat_from_html_tag, label2id) for html_locus in html_data]\n",
    "token_id_data = [token_ids for token_ids, _ in tokenized_html_data]\n",
    "ner_tag_data = [ner_tag_ids for _, ner_tag_ids in tokenized_html_data]\n",
    "token_seqs = [token_seq for token_seq in token_ids for token_ids in token_id_data]\n",
    "ner_tag_seqs = [ner_tag_seq for ner_tag_seq in ner_tag_ids for ner_tag_ids in ner_tag_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "max_length = 1022\n",
    "label2id = {\n",
    "    \"O\": 0,\n",
    "    \"B-definition\": 1,\n",
    "    \"I-definition\": 2,\n",
    "    \"B-notation\": 3,\n",
    "    \"I-notation\": 4\n",
    "} \n",
    "id2label = {value: key for key, value in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "note_names, token_seqs, ner_tag_seqs = [], [], []\n",
    "for html_locus, (token_ids, ner_tag_ids) in zip(html_data, tokenized_html_data):\n",
    "    note_names.extend([html_locus[\"Note name\"]] * len(token_ids))\n",
    "    token_seqs.extend(token_ids)\n",
    "    ner_tag_seqs.extend(ner_tag_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "ner_tags = ClassLabel(names=list(label2id))\n",
    "\n",
    "ds = Dataset.from_dict(\n",
    "        {\"note name\": note_names,\n",
    "        \"tokens\": token_ids,\n",
    "        \"ner_tags\": ner_tag_ids},\n",
    "        features=Features(\n",
    "            {\"tokens\": Sequence(Value(dtype='string')), \"ner_tags\": Sequence(ner_tags)}\n",
    "        ))\n",
    "\n",
    "ds.save_to_disk(\".\")\n",
    "\n",
    "# ds.load_from_disk(\".\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See https://huggingface.co/docs/transformers/tasks/token_classification for training a token classification model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
