{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp markdown.obsidian.personal.machine_learning.tokenize.def_and_notat_token_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# markdown.obsidian.personal.machine_learning.tokenize.def_and_notat_token_classification\n",
    "> Functions for gathering and processing tokenization data and for using ML models trained with such data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previous, `trouver` just had functionalities for using ML models to identify newly introduced notations in text and for gathering data to train such models. Moreover, such models were merely classification models, and using these models to identify newly introduced notations had a lot of computational redundancies.\n",
    "\n",
    "This module aims to provide the same functionalities for both definitions and notations by training and using token classification models instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a new module dedicated to definition and notation identification and move approparite functions over there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from itertools import pairwise\n",
    "import os \n",
    "from os import PathLike\n",
    "from pathlib import Path\n",
    "from typing import Optional, Union\n",
    "import warnings\n",
    "\n",
    "import bs4\n",
    "from transformers import BatchEncoding, pipelines, PreTrainedTokenizer, PreTrainedTokenizerFast\n",
    "\n",
    "from trouver.helper.definition_and_notation import double_asterisk_indices, notation_asterisk_indices\n",
    "from trouver.helper.html import add_HTML_tag_data_to_raw_text, add_space_to_lt_symbols_without_space, remove_html_tags_in_text\n",
    "from trouver.helper.regex import latex_indices, replace_string_by_indices\n",
    "from trouver.markdown.markdown.file import MarkdownFile, MarkdownLineEnum\n",
    "from trouver.markdown.obsidian.personal.note_processing import process_standard_information_note\n",
    "from trouver.markdown.obsidian.vault import VaultNote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unittest import mock\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "from datasets import ClassLabel, Dataset, Features, Sequence, Value\n",
    "from transformers import AutoTokenizer\n",
    "from fastcore.test import *\n",
    "\n",
    "from trouver.helper.tests import _test_directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather ML data from information notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def convert_double_asterisks_to_html_tags(\n",
    "        text: str\n",
    "        ) -> str:\n",
    "    \"\"\"\n",
    "    Replace the double asterisks, which signify definitions and notations,\n",
    "    in `text` with HTML tags.\n",
    "    \"\"\"\n",
    "    double_asts = double_asterisk_indices(text)\n",
    "    replacement_html_tags = [\n",
    "        _html_tag_from_double_ast(text[start:end])\n",
    "        for start, end in double_asts]\n",
    "    return replace_string_by_indices(\n",
    "        text, double_asts, replacement_html_tags)\n",
    "\n",
    "\n",
    "def _html_tag_from_double_ast(\n",
    "        double_ast_string: str # Starts and ends with double asts\n",
    "        ) -> str:\n",
    "    \"\"\"\n",
    "    Get the HTML tag representing definition or notation data from\n",
    "    a string surrounded by double asterisks.\n",
    "\n",
    "    This is used in the `_convert_double_asterisks_to_html_tags` function.\n",
    "    \"\"\"\n",
    "    no_asts = double_ast_string[2:-2]\n",
    "    if notation_asterisk_indices(double_ast_string):\n",
    "        return f'<span notation=\"\">{no_asts}</span>'\n",
    "    else:\n",
    "        return f'<b definition=\"\">{no_asts}</b>'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<b definition=\"\">hi</b>. Here is a notation <span notation=\"\">$asdf$</span>\n"
     ]
    }
   ],
   "source": [
    "print(convert_double_asterisks_to_html_tags(\"**hi**. Here is a notation **$asdf$**\"))\n",
    "test_eq(convert_double_asterisks_to_html_tags(\"**hi**. Here is a notation **$asdf$**\"), '<b definition=\"\">hi</b>. Here is a notation <span notation=\"\">$asdf$</span>')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def raw_text_with_html_tags_from_markdownfile(\n",
    "        mf: MarkdownFile,\n",
    "        vault: PathLike\n",
    "        ) -> str:\n",
    "    \"\"\"\n",
    "    Process the `MarkdownFile`, replacing the double asterisk surrounded\n",
    "    text indicating definitions and notations to be HTML tags instead.\n",
    "    \"\"\"\n",
    "    mf = process_standard_information_note(\n",
    "        mf, vault, remove_double_asterisks=False,\n",
    "        remove_html_tags=False)\n",
    "    return convert_double_asterisks_to_html_tags(str(mf))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Some kind of potato[^2]\\n\\n[^2]: Some footnote\\n\\nSome link\\n'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "\n",
    "# TODO: \n",
    "# I want to make sure that footnotes are getting properly removed.\n",
    "mf = MarkdownFile.from_string(\n",
    "    r\"\"\"---\n",
    "aliases: []\n",
    "tags: []\n",
    "---\n",
    "# Something  \n",
    "\n",
    "Some kind of potato[^2]\n",
    "\n",
    "[^2]: Some footnote\n",
    "\n",
    "[[link_to_note|Some link]]\n",
    "\n",
    "\n",
    "# See Also\n",
    "# Meta\n",
    "## References and Citations\n",
    "\"\"\") \n",
    "raw_text_with_html_tags_from_markdownfile(mf, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "mf = MarkdownFile.from_string(\n",
    "    r\"\"\"---\n",
    "aliases: []\n",
    "tags: []\n",
    "---\n",
    "# Galois group of a separable and normal finite field extension\n",
    "\n",
    "Let $L/K$ be a separable and normal finite field extension. Its <b definition=\"Galois group of a separable and normal finite field extension\">Galois group</b> <span notation=\"\">$\\operatorname{Gal}(L/K)$</span> is...\n",
    "\n",
    "# Galois group of a separable and normal profinite field extension\n",
    "\n",
    "In fact, the notion of a Galois group can be defined for profinite field extensions. Given a separable and normal profinite field extension $L/K$, say that\n",
    "$L = \\varinjlim_i L_i$ where $L_i/K$ are finite extensions. Its **Galois group** **$\\operatorname{Gal}(L/K)$**\n",
    "\n",
    "# See Also\n",
    "# Meta\n",
    "## References and Citations\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following example, let `mf` be the following `MarkdownFile`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "aliases: []\n",
      "tags: []\n",
      "---\n",
      "# Galois group of a separable and normal finite field extension\n",
      "\n",
      "Let $L/K$ be a separable and normal finite field extension. Its <b definition=\"Galois group of a separable and normal finite field extension\">Galois group</b> <span notation=\"\">$\\operatorname{Gal}(L/K)$</span> is...\n",
      "\n",
      "# Galois group of a separable and normal profinite field extension\n",
      "\n",
      "In fact, the notion of a Galois group can be defined for profinite field extensions. Given a separable and normal profinite field extension $L/K$, say that\n",
      "$L = \\varinjlim_i L_i$ where $L_i/K$ are finite extensions. Its **Galois group** **$\\operatorname{Gal}(L/K)$**\n",
      "\n",
      "# See Also\n",
      "# Meta\n",
      "## References and Citations\n"
     ]
    }
   ],
   "source": [
    "print(str(mf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `raw_text_with_html_tags_from_markdownfile` function processes the `MarkdownFile` much in the same way as the `process_standard_information_note` function, except it 1. preserves HTML tags, and 2. replaces text surrounded by double asterisks `**` with HTML tags signifiying whether the text displays a definition or a notation.\n",
    "\n",
    "In the below example, note that the `vault` parameter is set to `None`; this is fine for this example becaues the `process_standard_information_note` function only needs a `vault` argument when embedded links need to be replaced with text (via the `MarkdownFile.replace_embedded_links_with_text` function), but `mf` has no embedded links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let $L/K$ be a separable and normal finite field extension. Its <b definition=\"Galois group of a separable and normal finite field extension\">Galois group</b> <span notation=\"\">$\\operatorname{Gal}(L/K)$</span> is...\n",
      "\n",
      "In fact, the notion of a Galois group can be defined for profinite field extensions. Given a separable and normal profinite field extension $L/K$, say that\n",
      "$L = \\varinjlim_i L_i$ where $L_i/K$ are finite extensions. Its <b definition=\"\">Galois group</b> <span notation=\"\">$\\operatorname{Gal}(L/K)$</span>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(raw_text_with_html_tags_from_markdownfile(mf, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "assert '**' not in raw_text_with_html_tags_from_markdownfile(mf, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def html_data_from_note(\n",
    "        note_or_mf: Union[VaultNote, MarkdownFile], # Either a `VaultNote`` object to a note or a `MarkdownFile` object from which to extra html data.\n",
    "        vault: Optional[PathLike] = None, # If vault to use when processing the `MarkdownFile` objects (if `note_of_mf` is a `VaultNote`, then this `MarkdownFile` object is created from the text of the note), cf. the `process_standard_information_note` function.\n",
    "        note_name: Optional[str] = None, # If `note_or_mf` is a `MarkdownFile`, `note_name` should be the name of the note from which the `MarkdownFile` comes from if applicable. If `note_or_mf` is a `VaultNote` object, then `note_name` is ignored and `note_or_mf.name` is used instead.\n",
    "        ) -> Union[dict, None]: # The keys to the dict are \"Note name\", \"Raw text\", \"Tag data\". However, `None` is returned if `note` does not exist or the note is marked with auto-generated, unverified data.\n",
    "    # TODO: implement obtaining multiple datapoints from a single note\n",
    "    # Via typos for example.\n",
    "    # TODO: implement various data augmentation techniques\n",
    "    \"\"\"Obtain html data for token classification from the information note.\n",
    "\n",
    "    Currently, the token types mainly revolve around definitions and\n",
    "    notations.\n",
    "\n",
    "    If `note` has the tag `_auto/def_and_notat_identified`, then the data\n",
    "    in the note is assumed to be auto-generated and not verified and\n",
    "    `None` is returned.\n",
    "\n",
    "    **Returns**\n",
    "    - Union[dict, None]\n",
    "        - The keys-value pairs are \n",
    "            - `\"Note name\"` - The name of the note\n",
    "            - `\"Raw text\"` - The raw text to include in the data.\n",
    "            - `\"Tag data\"` - The list with HTML tags carrying definition/notation\n",
    "              data and their locations in the Raw text. See the second output to\n",
    "              the function `remove_html_tags_in_text`.\n",
    "                - Each element of the list is a tuple consisting of a ``bs4.element.Tag``\n",
    "                  and two ints.\n",
    "    \"\"\"\n",
    "    if isinstance(note_or_mf, VaultNote) and not note_or_mf.exists():\n",
    "        return None\n",
    "    if isinstance(note_or_mf, VaultNote):\n",
    "        mf = MarkdownFile.from_vault_note(note_or_mf)\n",
    "        note_name = note_or_mf.name\n",
    "    else: # isinstance(note_or_mf, MarkdownFile):\n",
    "        mf = note_or_mf.copy(deep=False)\n",
    "    if mf.has_tag('_auto/def_and_notat_identified'):\n",
    "        return None\n",
    "    raw_text_with_tags = raw_text_with_html_tags_from_markdownfile(mf, vault)\n",
    "    raw_text, tags_and_locations = remove_html_tags_in_text(raw_text_with_tags)\n",
    "    return {\n",
    "        \"Note name\": note_name,\n",
    "        \"Raw text\": raw_text,\n",
    "        \"Tag data\": tags_and_locations}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following example, we mock a `VaultNote` whose content is that of `mf` in the example for the `raw_text_with_html_tags_from_markdownfile` function. Note that there is some text surrounded by double within `mf` surrounded by double asterisks `**` and some text surrounded by HTML tags to indicate definitions and notations introduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is the text from mf:\n",
      "\n",
      "---\n",
      "aliases: []\n",
      "tags: []\n",
      "---\n",
      "# Galois group of a separable and normal finite field extension\n",
      "\n",
      "Let $L/K$ be a separable and normal finite field extension. Its <b definition=\"Galois group of a separable and normal finite field extension\">Galois group</b> <span notation=\"\">$\\operatorname{Gal}(L/K)$</span> is...\n",
      "\n",
      "# Galois group of a separable and normal profinite field extension\n",
      "\n",
      "In fact, the notion of a Galois group can be defined for profinite field extensions. Given a separable and normal profinite field extension $L/K$, say that\n",
      "$L = \\varinjlim_i L_i$ where $L_i/K$ are finite extensions. Its **Galois group** **$\\operatorname{Gal}(L/K)$**\n",
      "\n",
      "# See Also\n",
      "# Meta\n",
      "## References and Citations\n",
      "{'Note name': \"Note's name\", 'Raw text': 'Let $L/K$ be a separable and normal finite field extension. Its Galois group $\\\\operatorname{Gal}(L/K)$ is...\\n\\nIn fact, the notion of a Galois group can be defined for profinite field extensions. Given a separable and normal profinite field extension $L/K$, say that\\n$L = \\\\varinjlim_i L_i$ where $L_i/K$ are finite extensions. Its Galois group $\\\\operatorname{Gal}(L/K)$\\n', 'Tag data': [(<b definition=\"Galois group of a separable and normal finite field extension\">Galois group</b>, 64, 76), (<span notation=\"\">$\\operatorname{Gal}(L/K)$</span>, 77, 102), (<b definition=\"\">Galois group</b>, 330, 342), (<span notation=\"\">$\\operatorname{Gal}(L/K)$</span>, 343, 368)]}\n",
      "[(<b definition=\"Galois group of a separable and normal finite field extension\">Galois group</b>, 64, 76), (<span notation=\"\">$\\operatorname{Gal}(L/K)$</span>, 77, 102), (<b definition=\"\">Galois group</b>, 330, 342), (<span notation=\"\">$\\operatorname{Gal}(L/K)$</span>, 343, 368)]\n"
     ]
    }
   ],
   "source": [
    "mf = MarkdownFile.from_string(\n",
    "    r\"\"\"---\n",
    "aliases: []\n",
    "tags: []\n",
    "---\n",
    "# Galois group of a separable and normal finite field extension\n",
    "\n",
    "Let $L/K$ be a separable and normal finite field extension. Its <b definition=\"Galois group of a separable and normal finite field extension\">Galois group</b> <span notation=\"\">$\\operatorname{Gal}(L/K)$</span> is...\n",
    "\n",
    "# Galois group of a separable and normal profinite field extension\n",
    "\n",
    "In fact, the notion of a Galois group can be defined for profinite field extensions. Given a separable and normal profinite field extension $L/K$, say that\n",
    "$L = \\varinjlim_i L_i$ where $L_i/K$ are finite extensions. Its **Galois group** **$\\operatorname{Gal}(L/K)$**\n",
    "\n",
    "# See Also\n",
    "# Meta\n",
    "## References and Citations\n",
    "\"\"\")\n",
    "\n",
    "with (mock.patch('__main__.VaultNote') as mock_VaultNote,\n",
    "      mock.patch('__main__.MarkdownFile.from_vault_note') as mock_from_vault_note,\n",
    "      mock.patch('__main__.isinstance') as mock_isinstance):\n",
    "    mock_VaultNote.exists.return_value = True\n",
    "    mock_VaultNote.name = \"Note's name\"\n",
    "    mock_from_vault_note.return_value = mf\n",
    "    mock_isinstance.return_value = True\n",
    "\n",
    "    print(f\"The following is the text from mf:\\n\\n{str(mf)}\")\n",
    "\n",
    "    html_data = html_data_from_note(mock_VaultNote, None)\n",
    "    print(html_data)\n",
    "\n",
    "    test_eq(html_data['Note name'], \"Note's name\")\n",
    "    assert '**' not in html_data['Raw text']\n",
    "    assert '<' not in html_data['Raw text']  # Test the lack of HTML tags in the raw text\n",
    "\n",
    "    print(html_data['Tag data'])\n",
    "    test_eq(len(html_data['Tag data']), 4)\n",
    "    assert isinstance(html_data['Tag data'][0][0], bs4.element.Tag)\n",
    "    assert html_data['Tag data'][0][0].has_attr('definition')\n",
    "    assert not html_data['Tag data'][0][0].has_attr('notation')\n",
    "    assert html_data['Tag data'][1][0].has_attr('notation')\n",
    "    assert not html_data['Tag data'][1][0].has_attr('definition')\n",
    "    assert html_data['Tag data'][2][0].has_attr('definition')\n",
    "    assert not html_data['Tag data'][2][0].has_attr('notation')\n",
    "    assert html_data['Tag data'][3][0].has_attr('notation')\n",
    "    assert not html_data['Tag data'][3][0].has_attr('definition')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also just pass a `MarkdonwFile` object instead of a `VaultNote` object. In this case, we can specify the `note_name` parameter to indicate which note the `MarkdownFile` object came from, if applicable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Note name': \"Note's name\", 'Raw text': 'Let $L/K$ be a separable and normal finite field extension. Its Galois group $\\\\operatorname{Gal}(L/K)$ is...\\n\\nIn fact, the notion of a Galois group can be defined for profinite field extensions. Given a separable and normal profinite field extension $L/K$, say that\\n$L = \\\\varinjlim_i L_i$ where $L_i/K$ are finite extensions. Its Galois group $\\\\operatorname{Gal}(L/K)$\\n', 'Tag data': [(<b definition=\"Galois group of a separable and normal finite field extension\">Galois group</b>, 64, 76), (<span notation=\"\">$\\operatorname{Gal}(L/K)$</span>, 77, 102), (<b definition=\"\">Galois group</b>, 330, 342), (<span notation=\"\">$\\operatorname{Gal}(L/K)$</span>, 343, 368)]}\n",
      "[(<b definition=\"Galois group of a separable and normal finite field extension\">Galois group</b>, 64, 76), (<span notation=\"\">$\\operatorname{Gal}(L/K)$</span>, 77, 102), (<b definition=\"\">Galois group</b>, 330, 342), (<span notation=\"\">$\\operatorname{Gal}(L/K)$</span>, 343, 368)]\n"
     ]
    }
   ],
   "source": [
    "html_data = html_data_from_note(mf, vault=None, note_name=\"Note's name\")\n",
    "print(html_data)\n",
    "\n",
    "test_eq(html_data['Note name'], \"Note's name\")\n",
    "assert '**' not in html_data['Raw text']\n",
    "assert '<' not in html_data['Raw text']  # Test the lack of HTML tags in the raw text\n",
    "\n",
    "print(html_data['Tag data'])\n",
    "test_eq(len(html_data['Tag data']), 4)\n",
    "assert isinstance(html_data['Tag data'][0][0], bs4.element.Tag)\n",
    "assert html_data['Tag data'][0][0].has_attr('definition')\n",
    "assert not html_data['Tag data'][0][0].has_attr('notation')\n",
    "assert html_data['Tag data'][1][0].has_attr('notation')\n",
    "assert not html_data['Tag data'][1][0].has_attr('definition')\n",
    "assert html_data['Tag data'][2][0].has_attr('definition')\n",
    "assert not html_data['Tag data'][2][0].has_attr('notation')\n",
    "assert html_data['Tag data'][3][0].has_attr('notation')\n",
    "assert not html_data['Tag data'][3][0].has_attr('definition')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we do not specify `note_name`, then `None` is used for the `'Note name'` key in the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Note name': None, 'Raw text': 'Let $L/K$ be a separable and normal finite field extension. Its Galois group $\\\\operatorname{Gal}(L/K)$ is...\\n\\nIn fact, the notion of a Galois group can be defined for profinite field extensions. Given a separable and normal profinite field extension $L/K$, say that\\n$L = \\\\varinjlim_i L_i$ where $L_i/K$ are finite extensions. Its Galois group $\\\\operatorname{Gal}(L/K)$\\n', 'Tag data': [(<b definition=\"Galois group of a separable and normal finite field extension\">Galois group</b>, 64, 76), (<span notation=\"\">$\\operatorname{Gal}(L/K)$</span>, 77, 102), (<b definition=\"\">Galois group</b>, 330, 342), (<span notation=\"\">$\\operatorname{Gal}(L/K)$</span>, 343, 368)]}\n"
     ]
    }
   ],
   "source": [
    "html_data = html_data_from_note(mf, vault=None, note_name=None)\n",
    "print(html_data)\n",
    "\n",
    "assert html_data['Note name'] is None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the following example, the note has an HTML tag already with extra data (attributes other than `'definition'` or `'notation'`). We assert that the extra data is preserved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is the text of the mocked note: \n",
      "\n",
      " Let $X$ be a topological space and let $U \\subseteq X$ be an subspace. The <b definition=\"Closure of a subspace of a topological space\" typo=\"dosure of $U$\">closure of $U$</b> is defined as...\n",
      "\n",
      "\n",
      "{'Note name': \"Note's name\", 'Raw text': 'Let $X$ be a topological space and let $U \\\\subseteq X$ be an subspace. The closure of $U$ is defined as...', 'Tag data': [(<b definition=\"Closure of a subspace of a topological space\" typo=\"dosure of $U$\">closure of $U$</b>, 75, 89)]}\n"
     ]
    }
   ],
   "source": [
    "with (mock.patch('__main__.VaultNote') as mock_VaultNote,\n",
    "      mock.patch('__main__.MarkdownFile.from_vault_note') as mock_from_vault_note,\n",
    "      mock.patch('__main__.isinstance') as mock_isinstance):\n",
    "    mock_VaultNote.exists.return_value = True\n",
    "    mock_VaultNote.name = \"Note's name\"\n",
    "    mock_isinstance.return_value = True\n",
    "\n",
    "    text = 'Let $X$ be a topological space and let $U \\subseteq X$ be an subspace. The <b definition=\"Closure of a subspace of a topological space\" typo=\"dosure of $U$\">closure of $U$</b> is defined as...'\n",
    "    mf = MarkdownFile.from_string(text)\n",
    "    mock_from_vault_note.return_value = mf\n",
    "    print(f\"The following is the text of the mocked note: \\n\\n {text}\\n\\n\")\n",
    "\n",
    "    html_data = html_data_from_note(mock_VaultNote, None)\n",
    "    print(html_data)\n",
    "    assert html_data['Tag data'][0][0].has_attr('typo')\n",
    "    test_eq(html_data['Tag data'][0][0].attrs['typo'], 'dosure of $U$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following example, the (mocked) note has the `#_auto/def_and_notats_identified` tag to indicate that its definition and notation markings were auto-generated by a model (trained with data processed by the `tokenize_html_data` function) using the `auto_mark_def_and_notats` function. In this case, the `html_data_from_note` function returns `None` to prevent gathering data that is unverified and auto-generated by a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is the text of the mocked note: \n",
      "\n",
      "---\n",
      "tags: [_auto/def_and_notat_identified]\n",
      "---\n",
      "Let $X$ be a topological space and let $U \\subseteq X$ be an subspace. The <b definition=\"Closure of a subspace of a topological space\" typo=\"dosure of $U$\">closure of $U$</b> is defined as...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# with (mock.patch('__main__.VaultNote') as mock_VaultNote,\n",
    "#       mock.patch('__main__.MarkdownFile.from_vault_note') as mock_from_vault_note):\n",
    "#     mock_VaultNote.exists.return_value = True\n",
    "#     mock_VaultNote.name = \"Note's name\"\n",
    "text = r'''---\n",
    "tags: [_auto/def_and_notat_identified]\n",
    "---\n",
    "Let $X$ be a topological space and let $U \\subseteq X$ be an subspace. The <b definition=\"Closure of a subspace of a topological space\" typo=\"dosure of $U$\">closure of $U$</b> is defined as...'''\n",
    "\n",
    "mf = MarkdownFile.from_string(text)\n",
    "mock_from_vault_note.return_value = mf\n",
    "print(f\"The following is the text of the mocked note: \\n\\n{text}\\n\\n\")\n",
    "\n",
    "html_data = html_data_from_note(note_or_mf=mf)\n",
    "assert(html_data is None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def tokenize_html_data(\n",
    "        html_locus: dict, # An output of `html_data_from_note`\n",
    "        tokenizer: Union[PreTrainedTokenizer, PreTrainedTokenizerFast],\n",
    "        max_length: int, # Max length for each sequence of tokens\n",
    "        ner_tag_from_html_tag: callable, # takes in a bs4.element.Tag and outputs the ner_tag (as a string or `None`)\n",
    "        label2id: dict[str, int], # The keys ner_tag's of the form f\"I-{output}\" or f\"B-{output}\" where `output` is an output of `ner_tag_from_html_tag`.\n",
    "        default_label: str = \"O\", # The default label for the NER tagging.\n",
    "        ) -> tuple[list[list[str]], list[list[int]]]: # The first list consists of the tokens and the second list consists of the named entity recognition tags.\n",
    "    \"\"\"Actually tokenize the html data outputted by `html_data_from_note`.\n",
    "\n",
    "    To account for the possibility that the raw text is long,\n",
    "    this function uses the `tokenizer.batch_encode_plus` function\n",
    "    to tokenize the text into sequences. \n",
    "    \"\"\"\n",
    "    tokenized = tokenizer.batch_encode_plus(\n",
    "        [html_locus[\"Raw text\"]], max_length=max_length, return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True, truncation=True)\n",
    "\n",
    "    default_id = label2id[default_label]        \n",
    "    ner_ids = [[default_id for _ in seq_input_ids]\n",
    "               for seq_input_ids in tokenized['input_ids']]\n",
    "    for tag, start, end in html_locus[\"Tag data\"]:\n",
    "        ner_tag = ner_tag_from_html_tag(tag)\n",
    "        if ner_tag is None:\n",
    "            continue  # `ner_tag` is not of relevant data.\n",
    "        tuppy = _start_end_seqs_indices_for_html_tag(tokenized, start, end - 1)\n",
    "        (start_seq, start_index_in_seq), (end_seq, end_index_in_seq) = tuppy\n",
    "        _set_ner_ids_for_tag(\n",
    "            ner_ids, start_seq, start_index_in_seq, end_seq, end_index_in_seq,\n",
    "            label2id, ner_tag)\n",
    "    # return tokenized[\"input_ids\"], ner_ids\n",
    "    tokens = [tokenizer.convert_ids_to_tokens(tokens_for_seq)\n",
    "              for tokens_for_seq in tokenized[\"input_ids\"]]\n",
    "    return tokens, ner_ids\n",
    "\n",
    "\n",
    "def _start_end_seqs_indices_for_html_tag(\n",
    "        tokenized: BatchEncoding,\n",
    "        tag_start_ind: int,\n",
    "        tag_end_ind: int\n",
    "        ) -> tuple[tuple[int, int], tuple[int, int]]: # The first tuple is `(a, b)` where `tokenized['input_ids'][a][b]` is the token corresponding to the start of the HTML tag's (raw) text. The second tuple is `(c, d)` where `tokenized['input_ids'][c][d]` is the token corresponding to the end of the HTML tag's (raw) text.\n",
    "    start_seq = _search_seq_ind_for_char(tokenized['offset_mapping'], tag_start_ind)\n",
    "    # start_index_in_seq = tokenized.char_to_token(batch_or_char_index=start_seq, char_index=tag_start_ind)\n",
    "    start_index_in_seq = _search_within_seq_for_char(tokenized['offset_mapping'][start_seq], tag_start_ind)\n",
    "    end_seq = _search_seq_ind_for_char(tokenized['offset_mapping'], tag_end_ind)\n",
    "    # end_index_in_seq = tokenized.char_to_token(batch_or_char_index=end_seq, char_index=tag_end_ind)\n",
    "    end_index_in_seq = _search_within_seq_for_char(tokenized['offset_mapping'][end_seq], tag_end_ind)\n",
    "    return (start_seq, start_index_in_seq), (end_seq, end_index_in_seq)\n",
    "\n",
    "\n",
    "def _min_max_char_ind_for_seq(\n",
    "        offset_for_seq: list[tuple[int,int]] # An item in tokenized['offset_mapping']\n",
    "        ):\n",
    "    min_char_ind, max_char_ind = 0, 0\n",
    "    for inds in offset_for_seq:\n",
    "        if inds != (0,0):\n",
    "            min_char_ind = inds[0]\n",
    "            break\n",
    "    for inds in reversed(offset_for_seq):\n",
    "        if inds != (0,0):\n",
    "            max_char_ind = inds[1]\n",
    "            break\n",
    "    return min_char_ind, max_char_ind\n",
    "\n",
    "def _char_is_in_seq(\n",
    "        offset_for_seq: list[int], # An item in tokenized['offset_mapping']\n",
    "        char: int # The index of a character in the original raw text\n",
    "        ) -> bool:\n",
    "    min_char_ind, max_char_ind = _min_max_char_ind_for_seq(offset_for_seq)\n",
    "    return min_char_ind <= char and char < max_char_ind\n",
    "\n",
    "def _search_seq_ind_for_char(\n",
    "        offsets: list[tuple[int, int]], # tokenized['offset_mapping']\n",
    "        char: int # The index of a character in the original raw text\n",
    "        ) -> int:\n",
    "    \"\"\"\n",
    "    Binary search the index of the sequence containing the token at the \n",
    "    location of the index `char` within the original (raw) text.\n",
    "\n",
    "    Based on pseudocode from https://pseudoeditor.com/guides/binary-search\n",
    "    \"\"\"\n",
    "    left = 0\n",
    "    right = len(offsets) - 1\n",
    "    while left <= right:\n",
    "        mid = (left + right) // 2\n",
    "        min_char_ind, max_char_ind = _min_max_char_ind_for_seq(offsets[mid])\n",
    "        if min_char_ind <= char and char < max_char_ind:\n",
    "            return mid\n",
    "        elif max_char_ind <= char:\n",
    "            left = mid + 1\n",
    "        else:\n",
    "            right = mid - 1\n",
    "    return -1  # This should not be returned under normal use.\n",
    "\n",
    "\n",
    "def _search_within_seq_for_char(\n",
    "        seq_offset: list[tuple[int, int]],\n",
    "        char: int\n",
    "    ) -> int:\n",
    "    \"\"\"\n",
    "    Binary search for the index within the sequence corresponding\n",
    "    to the token at the location of the index `char` within the\n",
    "    original (raw) text.\n",
    "\n",
    "    Based on pseudocode from https://pseudoeditor.com/guides/binary-search\n",
    "    \"\"\"\n",
    "    left = 0\n",
    "    right = len(seq_offset) - 1\n",
    "    while left <= right:\n",
    "        mid = (left + right) // 2\n",
    "        min_char_ind, max_char_ind = seq_offset[mid] \n",
    "        if min_char_ind <= char and char < max_char_ind:\n",
    "            return mid\n",
    "        elif max_char_ind <= char:\n",
    "            left = mid + 1\n",
    "        else:\n",
    "            right = mid - 1\n",
    "    return -1  # This should not be returned under normal use.\n",
    "\n",
    "\n",
    "def _set_ner_ids_for_tag(\n",
    "        ner_ids: list[list[int]],\n",
    "        start_seq: int, \n",
    "        start_index_in_seq: int,\n",
    "        end_seq: int,\n",
    "        end_index_in_seq: int,\n",
    "        label2id: dict[str, int],\n",
    "        ner_tag: str\n",
    "        ) -> None:\n",
    "    \"\"\"\n",
    "    After the locations of the tokens corresponding to a HTML tag have been found, \n",
    "    mark within `ner_ids` the appropriate NER tags at the locations corresponding\n",
    "    to the tokens' locations.\n",
    "    \"\"\"\n",
    "    ner_ids[start_seq][start_index_in_seq] = label2id[f\"B-{ner_tag}\"]\n",
    "    i_ner_id = label2id[f\"I-{ner_tag}\"]\n",
    "    seq, ind = start_seq, start_index_in_seq + 1\n",
    "    while seq < end_seq or ind <= end_index_in_seq:\n",
    "        if len(ner_ids[seq]) <= ind:\n",
    "            seq += 1\n",
    "            ind = 0\n",
    "        else:\n",
    "            ner_ids[seq][ind] = i_ner_id \n",
    "            ind += 1\n",
    "    \n",
    "\n",
    "\n",
    "def def_or_notat_from_html_tag(\n",
    "        tag: bs4.element.Tag\n",
    "        ) -> Union[str, None]:\n",
    "    \"\"\"\n",
    "    Can be passed as the `ner_tag_from_html_tag` argument in `tokenize_html_data`\n",
    "    for the purposes of compiling a dataset for definition and notation\n",
    "    identification.\n",
    "\n",
    "    The strings f\"I-{output}\" and f\"B-{output}\" are valid ner_tags. To use for \n",
    "    \"\"\"\n",
    "    if \"definition\" in tag.attrs:\n",
    "        return \"definition\"\n",
    "    elif \"notation\" in tag.attrs:\n",
    "        return \"notation\"\n",
    "    return None  # If the HTML tag carries neither definition nor notation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_eq(_min_max_char_ind_for_seq([(0,0), (1,3), (3,4), (4,7), (7,15), (0,0)]), (1,15))\n",
    "\n",
    "offsets = [[(0,0), (0,3), (4,5), (5,6), (6,7), (7,8), (8,9),],\n",
    "           [(10,12), (13,14), (15,18), (18,24)],\n",
    "           [(25,28), (29,35), (36,42), ]]\n",
    "test_eq(_search_seq_ind_for_char(offsets, 0), 0)\n",
    "test_eq(_search_seq_ind_for_char(offsets, 1), 0)\n",
    "test_eq(_search_seq_ind_for_char(offsets, 5), 0)\n",
    "test_eq(_search_seq_ind_for_char(offsets, 8), 0)\n",
    "# I don't think that character index 9 is something that I need to worry about.\n",
    "test_eq(_search_seq_ind_for_char(offsets, 10), 1)\n",
    "test_eq(_search_seq_ind_for_char(offsets, 23), 1)\n",
    "test_eq(_search_seq_ind_for_char(offsets, 25), 2)\n",
    "test_eq(_search_seq_ind_for_char(offsets, 41), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We continue with an example using the HTML data from the example for the `html_data_from_note` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Note name': None, 'Raw text': 'Let $L/K$ be a separable and normal finite field extension. Its Galois group $\\\\operatorname{Gal}(L/K)$ is...\\n\\nIn fact, the notion of a Galois group can be defined for profinite field extensions. Given a separable and normal profinite field extension $L/K$, say that\\n$L = \\\\varinjlim_i L_i$ where $L_i/K$ are finite extensions. Its Galois group $\\\\operatorname{Gal}(L/K)$\\n', 'Tag data': [(<b definition=\"Galois group of a separable and normal finite field extension\">Galois group</b>, 64, 76), (<span notation=\"\">$\\operatorname{Gal}(L/K)$</span>, 77, 102), (<b definition=\"\">Galois group</b>, 330, 342), (<span notation=\"\">$\\operatorname{Gal}(L/K)$</span>, 343, 368)]}\n"
     ]
    }
   ],
   "source": [
    "mf = MarkdownFile.from_string(\n",
    "    r\"\"\"---\n",
    "aliases: []\n",
    "tags: []\n",
    "---\n",
    "# Galois group of a separable and normal finite field extension\n",
    "\n",
    "Let $L/K$ be a separable and normal finite field extension. Its <b definition=\"Galois group of a separable and normal finite field extension\">Galois group</b> <span notation=\"\">$\\operatorname{Gal}(L/K)$</span> is...\n",
    "\n",
    "# Galois group of a separable and normal profinite field extension\n",
    "\n",
    "In fact, the notion of a Galois group can be defined for profinite field extensions. Given a separable and normal profinite field extension $L/K$, say that\n",
    "$L = \\varinjlim_i L_i$ where $L_i/K$ are finite extensions. Its **Galois group** **$\\operatorname{Gal}(L/K)$**\n",
    "\n",
    "# See Also\n",
    "# Meta\n",
    "## References and Citations\n",
    "\"\"\")\n",
    "\n",
    "html_data = html_data_from_note(mf, vault=None, note_name=None)\n",
    "print(html_data)\n",
    "\n",
    "# assert html_data['Note name'] is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(<b definition=\"Galois group of a separable and normal finite field extension\">Galois group</b>,\n",
       "  64,\n",
       "  76),\n",
       " (<span notation=\"\">$\\operatorname{Gal}(L/K)$</span>, 77, 102),\n",
       " (<b definition=\"\">Galois group</b>, 330, 342),\n",
       " (<span notation=\"\">$\\operatorname{Gal}(L/K)$</span>, 343, 368)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html_data['Raw text']\n",
    "html_data[\"Tag data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hyunj\\Documents\\Development\\Python\\trouver_py310_venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {\n",
    "    \"O\": 0,\n",
    "    \"B-definition\": 1,\n",
    "    \"I-definition\": 2,\n",
    "    \"B-notation\": 3,\n",
    "    \"I-notation\": 4\n",
    "}\n",
    "tokens, ner_tag_ids = tokenize_html_data(html_data, tokenizer, 510, def_or_notat_from_html_tag, label2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, `max_length` is set to 510 (tokens). The string (\"Raw text\") is not very long, so only one sequence should be present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(len(tokens), 1)\n",
    "test_eq(len(ner_tag_ids), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us see what has been tagged:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'O',\n",
       " 1: 'B-definition',\n",
       " 2: 'I-definition',\n",
       " 3: 'B-notation',\n",
       " 4: 'I-notation'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label = {value: key for key, value in label2id.items()}\n",
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gal\t\tB-definition\n",
      "##ois\t\tI-definition\n",
      "group\t\tI-definition\n",
      "$\t\tB-notation\n",
      "\\\t\tI-notation\n",
      "operator\t\tI-notation\n",
      "##name\t\tI-notation\n",
      "{\t\tI-notation\n",
      "gal\t\tI-notation\n",
      "}\t\tI-notation\n",
      "(\t\tI-notation\n",
      "l\t\tI-notation\n",
      "/\t\tI-notation\n",
      "k\t\tI-notation\n",
      ")\t\tI-notation\n",
      "$\t\tI-notation\n",
      "gal\t\tB-definition\n",
      "##ois\t\tI-definition\n",
      "group\t\tI-definition\n",
      "$\t\tB-notation\n",
      "\\\t\tI-notation\n",
      "operator\t\tI-notation\n",
      "##name\t\tI-notation\n",
      "{\t\tI-notation\n",
      "gal\t\tI-notation\n",
      "}\t\tI-notation\n",
      "(\t\tI-notation\n",
      "l\t\tI-notation\n",
      "/\t\tI-notation\n",
      "k\t\tI-notation\n",
      ")\t\tI-notation\n",
      "$\t\tI-notation\n"
     ]
    }
   ],
   "source": [
    "for token, ner_tag in zip(tokens[0], ner_tag_ids[0]):\n",
    "    if ner_tag != 0:\n",
    "        print(f\"{token}\\t\\t{id2label[ner_tag]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us set `max_length` to be shorter to observe an example of a tokenization of a single text across multiple sequences (Of course, in practice, the max token length would be set to be longer, say around 512 or 1024.):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids, ner_tag_ids = tokenize_html_data(html_data, tokenizer, 20, def_or_notat_from_html_tag, label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "print(len(token_ids))\n",
    "print(len(ner_tag_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2],\n",
       " [2, 2, 2, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 3, 4, 4],\n",
       " [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0]]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_tag_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is sample code to then gather data for definition/notation identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "\n",
    "# TODO: test\n",
    "\n",
    "notes = [] # Replace with actual notes\n",
    "vault = '' # Replace with actual vault\n",
    "\n",
    "# vault = 'C:' # Replace with actual vault\n",
    "# notes = [] # Replace with actual notes\n",
    "\n",
    "html_data = [html_data_from_note(note, vault) for note in notes]\n",
    "max_length = 1022\n",
    "\n",
    "tokenized_html_data = [tokenize_html_data(html_locus, tokenizer, max_length, def_or_notat_from_html_tag, label2id) for html_locus in html_data]\n",
    "token_id_data = [token_ids for token_ids, _ in tokenized_html_data]\n",
    "ner_tag_data = [ner_tag_ids for _, ner_tag_ids in tokenized_html_data]\n",
    "token_seqs = [token_seq for token_seq in token_ids for token_ids in token_id_data]\n",
    "ner_tag_seqs = [ner_tag_seq for ner_tag_seq in ner_tag_ids for ner_tag_ids in ner_tag_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "max_length = 1022\n",
    "label2id = {\n",
    "    \"O\": 0,\n",
    "    \"B-definition\": 1,\n",
    "    \"I-definition\": 2,\n",
    "    \"B-notation\": 3,\n",
    "    \"I-notation\": 4\n",
    "} \n",
    "id2label = {value: key for key, value in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "note_names, token_seqs, ner_tag_seqs = [], [], []\n",
    "for html_locus, (token_ids, ner_tag_ids) in zip(html_data, tokenized_html_data):\n",
    "    note_names.extend([html_locus[\"Note name\"]] * len(token_ids))\n",
    "    token_seqs.extend(token_ids)\n",
    "    ner_tag_seqs.extend(ner_tag_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "# ner_tags = ClassLabel(names=list(label2id))\n",
    "\n",
    "# ds = Dataset.from_dict(\n",
    "#         {\"note_name\": note_names,\n",
    "#         \"tokens\": token_ids,\n",
    "#         \"ner_tags\": ner_tag_ids},\n",
    "#         features=Features(\n",
    "#             {\n",
    "#              \"note_name\": Value(dtype='string'),\n",
    "#              \"tokens\": Sequence(Value(dtype='string')),\n",
    "#              \"ner_tags\": Sequence(ner_tags)}\n",
    "#         ))\n",
    "\n",
    "# ds.save_to_disk(\".\")\n",
    "\n",
    "# ds.load_from_disk(\".\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See https://huggingface.co/docs/transformers/tasks/token_classification for training a token classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<b definition=\"\" style=\"border-width:1px;border-style:solid;padding:3px\">hi</b>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = bs4.BeautifulSoup('', 'html.parser')\n",
    "tag = soup.new_tag('b', style=\"border-width:1px;border-style:solid;padding:3px\", definition=\"\")\n",
    "tag.string = 'hi'\n",
    "tag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _make_tag(\n",
    "        text: str,\n",
    "        entity_type: str # 'definition' or 'notation'\n",
    "        ) -> bs4.element.Tag:\n",
    "    \"\"\"\n",
    "    Helper function to `_html_tag_data_from_part` and `_consolidate_token_preds`.\n",
    "    \"\"\"\n",
    "    soup = bs4.BeautifulSoup('', 'html.parser')\n",
    "    if entity_type == 'definition':\n",
    "        tag = soup.new_tag(\n",
    "            'b',\n",
    "            style=\"border-width:1px;border-style:solid;padding:3px\",\n",
    "            definition=\"\")\n",
    "    else:\n",
    "        tag = soup.new_tag(\n",
    "            'span',\n",
    "            style=\"border-width:1px;border-style:solid;padding:3px\",\n",
    "            notation=\"\")\n",
    "    tag.string = text\n",
    "    return tag\n",
    "\n",
    "\n",
    "def _html_tag_data_from_part(\n",
    "        main_text: str,\n",
    "        part: list[dict[str]]) -> tuple[bs4.element.Tag, int, int]:\n",
    "    \"\"\"\n",
    "    Helper function to `_html_tags_from_token_preds`\n",
    "    \"\"\"\n",
    "    start_token = part[0]\n",
    "    end_token = part[-1]\n",
    "    start_char = start_token['start']\n",
    "    end_char = end_token['end']\n",
    "    # the `'entity'` is either 'I-definition', 'B-definition', 'I-notation',\n",
    "    # or 'B-notation'\n",
    "    entity_type = start_token['entity'][2:]\n",
    "    html_text = main_text[start_char:end_char]\n",
    "    return (_make_tag(html_text, entity_type), start_char, end_char)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "main_text = \"Let $I \\subset A$ be an ideal. Define its radical by $\\sqrt{I}$\"\n",
    "\n",
    "sample_output_1 = _html_tag_data_from_part(\n",
    "    main_text, [{\n",
    "        'entity': 'B-definition',\n",
    "        'score': 0.37319255,\n",
    "        'index': 25,  # This is moot for the purposes of this test.\n",
    "        'word': 'radical',\n",
    "        'start': 42,\n",
    "        'end': 49\n",
    "    }])\n",
    "test_eq(str(sample_output_1[0]), '<b definition=\"\" style=\"border-width:1px;border-style:solid;padding:3px\">radical</b>')\n",
    "\n",
    "sample_output_2 = _html_tag_data_from_part(\n",
    "    main_text, [{\n",
    "        'entity': 'B-notation',\n",
    "        'score': 0.67021805,\n",
    "        'index': 27,  # This is moot for the purposes of this test.\n",
    "        'word': '$',\n",
    "        'start': 53,\n",
    "        'end': 54},\n",
    "    {\n",
    "        'entity': 'I-notation',\n",
    "        'score': 0.9748327,\n",
    "        'index': 28,  # This is moot for the purposes of this test.\n",
    "        'word': '\\\\',\n",
    "        'start': 54,\n",
    "        'end': 55},\n",
    "    {\n",
    "        'entity': 'I-notation',\n",
    "        'score': 0.9754836,\n",
    "        'index': 29,  # This is moot for the purposes of this test.\n",
    "        'word': 'sq',\n",
    "        'start': 55,\n",
    "        'end': 57},\n",
    "    {\n",
    "        'entity': 'I-notation',\n",
    "        'score': 0.9750675,\n",
    "        'index': 30,  # This is moot for the purposes of this test.\n",
    "        'word': '##rt',\n",
    "        'start': 57,\n",
    "        'end': 59},\n",
    "    {\n",
    "        'entity': 'I-notation',\n",
    "        'score': 0.97785944,\n",
    "        'index': 31,  # This is moot for the purposes of this test.\n",
    "        'word': '{',\n",
    "        'start': 59,\n",
    "        'end': 60},\n",
    "    {\n",
    "        'entity': 'I-notation',\n",
    "        'score': 0.97785944,\n",
    "        'index': 32,  # This is moot for the purposes of this test.\n",
    "        'word': 'i',\n",
    "        'start': 60,\n",
    "        'end': 61},\n",
    "    {\n",
    "        'entity': 'I-notation',\n",
    "        'score': 0.97785944,\n",
    "        'index': 33,  # This is moot for the purposes of this test.\n",
    "        'word': '}',\n",
    "        'start': 61,\n",
    "        'end': 62},\n",
    "    {\n",
    "        'entity': 'I-notation',\n",
    "        'score': 0.97785944,\n",
    "        'index': 34,  # This is moot for the purposes of this test.\n",
    "        'word': '$',\n",
    "        'start': 62,\n",
    "        'end': 63},\n",
    "    ])\n",
    "test_eq(str(sample_output_2[0]), '<span notation=\"\" style=\"border-width:1px;border-style:solid;padding:3px\">$\\sqrt{I}$</span>')\n",
    "# main_text.find('radical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _current_token_continues_the_previous_token(\n",
    "        current_token: dict, previous_token: dict, note: Optional[VaultNote]\n",
    "        ) -> bool:\n",
    "    \"\"\"\n",
    "    Helper function to `_divide_token_preds_into_parts`.\n",
    "    \"\"\"\n",
    "    if current_token['entity'].startswith('I-'):\n",
    "        if current_token['entity'][2:] == previous_token['entity'][2:]:\n",
    "            return True\n",
    "        elif note:\n",
    "            warnings.warn(rf\"\"\"\n",
    "                In the note {note.name} at {note.path()},\n",
    "                The token '{previous_token['word']}' is marked as '{previous_token['entity']}'\n",
    "                and the subsequent token '{current_token['word']}' is marked as '{current_token['entity']}',\n",
    "                which is unusual because the two consecutive tokens seem to be of different\n",
    "                entities, and yet the latter token does not start with a 'B-'.\n",
    "\n",
    "                The latter token will be treated like the beginning of a new entity.\"\"\"\n",
    "                    )\n",
    "            return False\n",
    "    else:\n",
    "        return False\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "previous_token_1 = {\n",
    "        'entity': 'I-notation',\n",
    "        'score': 0.97785944,\n",
    "        'index': 33,  # This is moot for the purposes of this test.\n",
    "        'word': '}',\n",
    "        'start': 61,\n",
    "        'end': 62}\n",
    "current_token_1 = {\n",
    "        'entity': 'I-notation',\n",
    "        'score': 0.97785944,\n",
    "        'index': 34,  # This is moot for the purposes of this test.\n",
    "        'word': '$',\n",
    "        'start': 62,\n",
    "        'end': 63}\n",
    "assert _current_token_continues_the_previous_token(current_token_1, previous_token_1, note=VaultNote('', rel_path='hi'))\n",
    "\n",
    "# Something like below should hopefully not happen, but it should still give a warning message\n",
    "previous_token_2 = {\n",
    "        'entity': 'I-definition',\n",
    "        'score': 0.97785944,\n",
    "        'index': 33,  # This is moot for the purposes of this test.\n",
    "        'word': '}',\n",
    "        'start': 61,\n",
    "        'end': 62}\n",
    "current_token_2 = {\n",
    "        'entity': 'I-notation',\n",
    "        'score': 0.97785944,\n",
    "        'index': 34,  # This is moot for the purposes of this test.\n",
    "        'word': '$',\n",
    "        'start': 62,\n",
    "        'end': 63}\n",
    "\n",
    "with warnings.catch_warnings(record=True) as w:\n",
    "    sample_output = _current_token_continues_the_previous_token(current_token_2, previous_token_2, note=VaultNote('', rel_path='hi'))\n",
    "    assert w\n",
    "    assert not sample_output\n",
    "\n",
    "previous_token_3 = {\n",
    "        'entity': 'I-definition',\n",
    "        'score': 0.97785944,\n",
    "        'index': 33,  # This is moot for the purposes of this test.\n",
    "        'word': '##tion',\n",
    "        'start': 58,\n",
    "        'end': 62}\n",
    "current_token_3 = {\n",
    "        'entity': 'B-notation',\n",
    "        'score': 0.97785944,\n",
    "        'index': 34,  # This is moot for the purposes of this test.\n",
    "        'word': '$',\n",
    "        'start': 62,\n",
    "        'end': 63}\n",
    "\n",
    "assert not _current_token_continues_the_previous_token(current_token_3, previous_token_3, note=VaultNote('', rel_path='hi'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _divide_token_preds_into_parts(\n",
    "        token_preds: list[dict[str]],\n",
    "        note: VaultNote,\n",
    "        excessive_space_threshold: int\n",
    "        ) -> list[list[dict[str]]]:\n",
    "    \"\"\"\n",
    "    Divide `token_preds` into parts so that each part\n",
    "    represents a single definition/notation marking.\n",
    "\n",
    "    Helper function to `_html_tags_from_token_preds`.\n",
    "    \"\"\"\n",
    "    token_preds_parts = []\n",
    "    for current_token in token_preds:\n",
    "        if not token_preds_parts:\n",
    "            token_preds_parts.append([current_token])\n",
    "            continue\n",
    "        prev_token = token_preds_parts[-1][-1]\n",
    "        if _current_token_continues_the_previous_token(\n",
    "                current_token, prev_token, note):\n",
    "            prev_token_end = prev_token['end']\n",
    "            cur_token_start = current_token['start']\n",
    "            if prev_token_end + excessive_space_threshold >= cur_token_start and note:\n",
    "                Warning(rf\"\"\"\n",
    "                    In the note {note.name} at {note.path()},\n",
    "                    There seems to be excessive space between the token\n",
    "                    {prev_token['word']} and {current_token['word']}, which\n",
    "                    seem to be part of the same entity\"\"\"\n",
    "                        )\n",
    "            token_preds_parts[-1].append(current_token)\n",
    "        else:\n",
    "            token_preds_parts.append([current_token])\n",
    "    return token_preds_parts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "main_text = \"Let $I \\subset A$ be an ideal. Define its radical by $\\sqrt{I}$\"\n",
    "\n",
    "preds = [\n",
    "    {\n",
    "        'entity': 'B-definition',\n",
    "        'score': 0.37319255,\n",
    "        'index': 25,  # This is moot for the purposes of this test.\n",
    "        'word': 'radical',\n",
    "        'start': 42,\n",
    "        'end': 49\n",
    "    },\n",
    "    {\n",
    "        'entity': 'B-notation',\n",
    "        'score': 0.67021805,\n",
    "        'index': 27,  # This is moot for the purposes of this test.\n",
    "        'word': '$',\n",
    "        'start': 53,\n",
    "        'end': 54},\n",
    "    {\n",
    "        'entity': 'I-notation',\n",
    "        'score': 0.9748327,\n",
    "        'index': 28,  # This is moot for the purposes of this test.\n",
    "        'word': '\\\\',\n",
    "        'start': 54,\n",
    "        'end': 55},\n",
    "    {\n",
    "        'entity': 'I-notation',\n",
    "        'score': 0.9754836,\n",
    "        'index': 29,  # This is moot for the purposes of this test.\n",
    "        'word': 'sq',\n",
    "        'start': 55,\n",
    "        'end': 57},\n",
    "    {\n",
    "        'entity': 'I-notation',\n",
    "        'score': 0.9750675,\n",
    "        'index': 30,  # This is moot for the purposes of this test.\n",
    "        'word': '##rt',\n",
    "        'start': 57,\n",
    "        'end': 59},\n",
    "    {\n",
    "        'entity': 'I-notation',\n",
    "        'score': 0.97785944,\n",
    "        'index': 31,  # This is moot for the purposes of this test.\n",
    "        'word': '{',\n",
    "        'start': 59,\n",
    "        'end': 60},\n",
    "    {\n",
    "        'entity': 'I-notation',\n",
    "        'score': 0.97785944,\n",
    "        'index': 32,  # This is moot for the purposes of this test.\n",
    "        'word': 'i',\n",
    "        'start': 60,\n",
    "        'end': 61},\n",
    "    {\n",
    "        'entity': 'I-notation',\n",
    "        'score': 0.97785944,\n",
    "        'index': 33,  # This is moot for the purposes of this test.\n",
    "        'word': '}',\n",
    "        'start': 61,\n",
    "        'end': 62},\n",
    "    {\n",
    "        'entity': 'I-notation',\n",
    "        'score': 0.97785944,\n",
    "        'index': 34,  # This is moot for the purposes of this test.\n",
    "        'word': '$',\n",
    "        'start': 62,\n",
    "        'end': 63},\n",
    "    ]\n",
    "\n",
    "output = _divide_token_preds_into_parts(\n",
    "    preds, VaultNote('', rel_path='hi'), excessive_space_threshold=2\n",
    ")\n",
    "\n",
    "# Test that the list finds two parts, one for the definition, and the other for the notation.\n",
    "test_eq(len(output), 2)\n",
    "test_eq(len(output[0]), 1)\n",
    "test_eq(len(output[1]), len(preds) - 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _ranges_overlap(\n",
    "        current_1: tuple[bs4.element.Tag, int, int],\n",
    "        current_2: tuple[bs4.element.Tag, int, int]\n",
    "        ) -> bool:\n",
    "    \"\"\"\n",
    "    Based on https://stackoverflow.com/a/64745177\n",
    "\n",
    "    Helper function to `_collate_html_tags`, `_consolidate_token_preds`.\n",
    "    \"\"\"\n",
    "    return max(current_1[1], current_2[1]) < min(current_1[2], current_2[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# In actuality, there should be bs4.element.Tag objects in place of ''.\n",
    "assert _ranges_overlap(('', 3, 8), ('', 6, 12))\n",
    "assert _ranges_overlap(('', 3, 8), ('', 3, 4))\n",
    "assert not _ranges_overlap(('', 3, 8), ('', 8, 9))\n",
    "assert _ranges_overlap(('', 6, 12), ('', 3, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _consolidate_token_preds(\n",
    "        main_text: str,\n",
    "        tag_data: list[tuple[bs4.element.Tag, int, int]]\n",
    "        ) -> list[tuple[bs4.element.Tag, int, int]]:\n",
    "    \"\"\"\n",
    "    Since the model's predictions can yield some odd results\n",
    "    (e.g. notations not being marked for an entire LaTeX string\n",
    "    $<span notation=\"\">$S_k := ...</span>$$), this function tries\n",
    "    to consolidate some oddities.\n",
    "    \n",
    "    \"\"\"\n",
    "    latex_inds = latex_indices(main_text)\n",
    "    extended_tag_data = _extend_tag_data_ranges(main_text, latex_inds, tag_data)\n",
    "    tag_data_notats_chopped = _cutoff_notation_tag_data(main_text, extended_tag_data)\n",
    "    # Go through the extended tag data to throw out overlapping ones.\n",
    "    ultimate_tag_data = []\n",
    "    for tag_point in tag_data_notats_chopped:\n",
    "        if _no_overlap_with_previous_tag_data(ultimate_tag_data, tag_point):\n",
    "            ultimate_tag_data.append(tag_point)\n",
    "    return ultimate_tag_data\n",
    "\n",
    "\n",
    "def _extend_tag_data_ranges(\n",
    "        main_text: str,\n",
    "        latex_inds: list[tuple[int, int]],\n",
    "        tag_data: list[tuple[bs4.element.Tag, int, int]],\n",
    "        ) -> list[tuple[bs4.element.Tag, int, int]]:\n",
    "    \"\"\"Helper function to `_consolidate_token_preds`.\n",
    "\n",
    "    Extend tag data so that the tag data does not start or\n",
    "    end within any latex math mode string. \n",
    "    \"\"\"\n",
    "    extended_tag_data = []\n",
    "    for tag_tuple in tag_data:\n",
    "    # for tex_range in latex_inds:\n",
    "        combined_range = (tag_tuple[1], tag_tuple[2])\n",
    "        for tex_range in latex_inds:\n",
    "        # for tag_tuple in tag_data:\n",
    "            if not _ranges_overlap((0, tex_range[0], tex_range[1]), tag_tuple):\n",
    "                continue\n",
    "            combined_range = (min(combined_range[0], tex_range[0]), max(combined_range[1], tex_range[1]))\n",
    "        new_text = main_text[combined_range[0]:combined_range[1]]\n",
    "        tag_type = 'definition' if 'definition' in tag_tuple[0].attrs else 'notation'\n",
    "        extended_tag = _make_tag(new_text, tag_type)\n",
    "        extended_tag_data.append((extended_tag, combined_range[0], combined_range[1]))\n",
    "    return extended_tag_data\n",
    "\n",
    "\n",
    "def _cutoff_notation_tag_data(\n",
    "        main_text: str,\n",
    "        tag_data: list[tuple[bs4.element.Tag, int, int]],\n",
    "        ) -> list[tuple[bs4.element.Tag, int, int]]:\n",
    "    \"\"\"\n",
    "    Helper function to `_consolidate_token_preds`.\n",
    "\n",
    "    Guarantees that a notation tag is a pure math mode latex string\n",
    "    by cutting only the pure math mode string\n",
    "    that occurs within it. Assumes that `_extend_tag_data_ranges`\n",
    "    works as intended.\n",
    "    \"\"\"\n",
    "    cutout_notation_tag_data = []\n",
    "    for tag, start, end in tag_data:\n",
    "        if not 'notation' in tag.attrs:\n",
    "            cutout_notation_tag_data.append((tag, start, end))\n",
    "            continue\n",
    "        tag_text = main_text[start:end]\n",
    "        tex_inds_in_tagged = latex_indices(tag_text)\n",
    "        for sub_start, sub_end in tex_inds_in_tagged:\n",
    "            tex_str = main_text[start+sub_start:start+sub_end]\n",
    "            cutout_tag = _make_tag(tex_str, 'notation')\n",
    "            cutout_notation_tag_data.append(\n",
    "                (cutout_tag, start+sub_start, start+sub_end))\n",
    "    return cutout_notation_tag_data\n",
    "\n",
    "\n",
    "\n",
    "def _no_overlap_with_previous_tag_data(\n",
    "        ultimate_tag_data: list[tuple[bs4.element.Tag, int, int]],\n",
    "        current_tag_data: tuple[bs4.element.Tag, int, int]  # Current tag data\n",
    "        ) -> bool:\n",
    "    \"\"\"\n",
    "    Return `True`, if the `current_tag_data` does not overlap with\n",
    "    any tag data that will be ultimately added. \n",
    "\n",
    "    Helper function for `_consolidate_token_preds`.\n",
    "    \"\"\"\n",
    "    # current_range = (current_tag_data[1], current_tag_data[2])\n",
    "    for prev in reversed(ultimate_tag_data):\n",
    "        # prev_range = (prev[1], prev[2])\n",
    "        if _ranges_overlap(prev, current_tag_data):\n",
    "            return False\n",
    "    return True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(<b definition=\"\" style=\"border-width:1px;border-style:solid;padding:3px\">Riemann zeta function $\\zeta(s)$</b>,\n",
       "  4,\n",
       "  36)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "main_text = 'Hi. This is some text. Here is a notation $$M_k :=... $$ and here is some more $$G_k :=...$$'\n",
    "\n",
    "soup = bs4.BeautifulSoup('', 'html.parser')\n",
    "# In actuality, the tag data will have more information, but the following\n",
    "# is good enough\n",
    "# for the purposes of this test\n",
    "tag_1 = soup.new_tag('span', notation='')\n",
    "tag_2 = soup.new_tag('span', notation='')\n",
    "# The following tries to test an erroneous short/incomplete marking of the very first \n",
    "# dollar sign `$` as well as the subtext 'G_K ' of the second math mode text.\n",
    "\n",
    "tag_data = [(tag_1, 42, 43), (tag_2, 81, 85)]\n",
    "output = _consolidate_token_preds(main_text, tag_data)\n",
    "test_eq(main_text[output[0][1]: output[0][2]], '$$M_k :=... $$')\n",
    "test_eq(main_text[output[1][1]: output[1][2]], '$$G_k :=...$$')\n",
    "\n",
    "\n",
    "# In the following example, the notation is a priori\n",
    "# found to be 'zeta(s)$ as $$\\zeta'. \n",
    "# So first, the tagged data is extended to encompass\n",
    "# '$\\zeta(s)$ as $$\\zeta(s) = ...$$' and then pure latex\n",
    "# math mode str are extracted\n",
    "main_text = 'Define $\\zeta(s)$ as $$\\zeta(s) = ...$$  '\n",
    "\n",
    "soup = bs4.BeautifulSoup('', 'html.parser')\n",
    "tag_1 = soup.new_tag('span', notation='')\n",
    "tag_data = [(tag_1,9,27)]\n",
    "output = _consolidate_token_preds(main_text, tag_data)\n",
    "# main_text.find('zeta')\n",
    "test_eq(main_text[output[0][1]:output[0][2]], r'$\\zeta(s)$')\n",
    "test_eq(main_text[output[1][1]:output[1][2]], r'$$\\zeta(s) = ...$$')\n",
    "\n",
    "\n",
    "\n",
    "# In the following example, we have erroneous\n",
    "# notation and definition markings which start/end within the same \n",
    "# math mode string.\n",
    "# In this case, the preceding marking takes precedence and \n",
    "# the latter overlapping marking is discarded;\n",
    "# this is an unfortunate feature that must be implemented to get\n",
    "# around shortcomings of the model.\n",
    "main_text = 'The Riemann zeta function $\\zeta(s)$ is defined as...'\n",
    "soup = bs4.BeautifulSoup('', 'html.parser')\n",
    "tag_1 = soup.new_tag('b', definition='')\n",
    "tag_2 = soup.new_tag('span', notation='')\n",
    "tag_data = [(tag_1,4,27), (tag_2,26,30)]\n",
    "output = _consolidate_token_preds(main_text, tag_data)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _html_tags_from_token_preds(\n",
    "        main_text: str,\n",
    "        token_preds: list[dict[str]],\n",
    "        note: VaultNote,\n",
    "        excessive_space_threshold: int\n",
    "        ) -> list[tuple[bs4.element.Tag, int, int]]:  # Tag element, start, end, where main_text[start:end] needs to be replaced by the tag element.\n",
    "    \"\"\"\n",
    "    Return HTML tags for definition and notation classification.\n",
    "\n",
    "    Helper function to `auto_mark_def_and_notats`.\n",
    "    \"\"\"\n",
    "    parts = _divide_token_preds_into_parts(\n",
    "        token_preds, note, excessive_space_threshold)\n",
    "    return [_html_tag_data_from_part(main_text, part) for part in parts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _collate_html_tags(\n",
    "        tag_data_1: list[tuple[bs4.element.Tag, int, int]],\n",
    "        tag_data_2: list[tuple[bs4.element.Tag, int, int]],\n",
    "    ) -> list[tuple[bs4.element.Tag], int, int]:\n",
    "    \"\"\"\n",
    "    Collates the lists of HTML tags and the indices within a certain text\n",
    "    (which is not-needed for this function and hence not included)\n",
    "    that the HTML tags need to replace.\n",
    "\n",
    "    If there are entries in `tag_data_1` and `tag_data_2` with overlapping\n",
    "    ranges, then the entry from `tag_data_1` is prioritized and the entry\n",
    "    from `tag_data_2` is discarded.\n",
    "\n",
    "    Helper function to `auto_mark_def_and_notats`\n",
    "    \"\"\"\n",
    "    collated_list = []\n",
    "    i, j = 0, 0\n",
    "    while i < len(tag_data_1) and j < len(tag_data_2):\n",
    "        current_1 = tag_data_1[i]\n",
    "        current_2 = tag_data_2[j]\n",
    "        if _ranges_overlap(current_1, current_2): # Ignore current_2\n",
    "            j += 1\n",
    "            continue\n",
    "        if current_1[1] > current_2[1]:\n",
    "            collated_list.append(current_2)\n",
    "            j += 1\n",
    "        else:\n",
    "            collated_list.append(current_1)\n",
    "            i += 1\n",
    "    while i < len(tag_data_1):\n",
    "        collated_list.append(tag_data_1[i])\n",
    "        i += 1\n",
    "    while j < len(tag_data_2):\n",
    "        collated_list.append(tag_data_2[j])\n",
    "        j += 1\n",
    "    return collated_list\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "\n",
    "tag_data_1 = [\n",
    "    ('', 0, 1),\n",
    "    ('', 9, 12),\n",
    "    ('', 20, 21)\n",
    "]\n",
    "\n",
    "tag_data_2 = [\n",
    "    ('', 2, 4),\n",
    "    ('', 6, 7),\n",
    "    ('', 8, 10), # This should be discarded\n",
    "    ('', 10, 13), # This should be discarded\n",
    "    ('', 17, 20),\n",
    "    ('', 21, 24)\n",
    "]\n",
    "output = _collate_html_tags(tag_data_1, tag_data_2)\n",
    "test_eq(output, [('', 0, 1), ('', 2, 4), ('', 6, 7), ('', 9, 12), ('', 17, 20), ('', 20, 21), ('', 21, 24)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _add_nice_boxing_attrs_to_def_and_notat_tags(\n",
    "        html_tag_data: list[tuple[bs4.element.Tag, int, int]]\n",
    "        ) -> list[tuple[bs4.element.Tag, int, int]]:\n",
    "    \"\"\"\n",
    "    Add HTML tag attributes to draw boxes around notation data\n",
    "\n",
    "    Helper function to `auto_mark_def_and_notats`.\n",
    "    \"\"\"\n",
    "    listy = []\n",
    "    for tag, start, end in html_tag_data:\n",
    "        if ('notation' in tag.attrs or 'definition' in tag.attrs) and 'style' not in tag.attrs:\n",
    "            tag.attrs['style'] = \"border-width:1px;border-style:solid;padding:3px\"\n",
    "        listy.append((tag, start, end)) \n",
    "    return listy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "soup = bs4.BeautifulSoup('', \"html.parser\")\n",
    "tag = soup.new_tag(\"span\", notation=\"\")\n",
    "tag.string = 'hi'\n",
    "tag_data = [\n",
    "    (tag, 0, 2),\n",
    "]\n",
    "output = _add_nice_boxing_attrs_to_def_and_notat_tags(tag_data)\n",
    "assert \"style\" in output[0][0].attrs\n",
    "\n",
    "tag = soup.new_tag(\"span\", definition=\"\")\n",
    "tag.string = 'hi'\n",
    "tag_data = [\n",
    "    (tag, 0, 2),\n",
    "]\n",
    "output = _add_nice_boxing_attrs_to_def_and_notat_tags(tag_data)\n",
    "assert \"style\" in output[0][0].attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def def_and_notat_preds_by_model(\n",
    "        text: str,  \n",
    "        pipeline # The pipeline object created using the token classification model and its tokenizer\n",
    "        ) -> list[tuple[bs4.element.Tag, int, int]]: # Each tuple consists of an HTML tag carrying the data of the prediction and ints marking where in `text` the definition or notation is at.\n",
    "    \"\"\"\n",
    "    Predict where definitions and notations occur in `text`\n",
    "\n",
    "    This function uses some of the same helper functions as\n",
    "    `auto_mark_def_and_notats`, but does not raise warning messages as\n",
    "    in `auto_mark_def_and_notats`.\n",
    "    \"\"\"\n",
    "    tag_data = _html_tags_from_token_preds(text, pipeline(text), None, 2)\n",
    "    return tag_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def auto_mark_def_and_notats(\n",
    "        note: VaultNote,  # The standard information note in which to find the definitions and notations.\n",
    "        pipeline: pipelines.token_classification.TokenClassificationPipeline, # The token classification pipeline that is used to predict whether tokens are part of definitions or notations introduced in the text.\n",
    "        # remove_existing_def_and_notat_markings: bool = False,  # If `True`, remove definition and notation markings (both via surrounding by double asterisks `**` as per the legacy method and via HTML tags)\n",
    "        excessive_space_threshold: int = 2,\n",
    "        add_boxing_attr_to_existing_def_and_notat_markings: bool = True # If `True`, then nice attributes are added to the existing notation HTML tags, if not already present.\n",
    "    ) -> None:\n",
    "    \"\"\"\n",
    "    Predict and mark where definitions and notation occur in a note using\n",
    "    a token classification ML model.\n",
    "\n",
    "    Assumes that the note is a standard information note that does not\n",
    "    have a lot of \"user modifications\", such as footnotes, links,\n",
    "    and HTML tags. If\n",
    "    there are many modifications, then these might be deleted.\n",
    "\n",
    "    Assumes that the paragraphs in the text of the note are \"not too long\".\n",
    "    Currently, this means that the paragraphs in the number of tokens\n",
    "    in the text of the note should (roughly) not exceed \n",
    "    `pipeline.tokenizer.model_max_length`.\n",
    "\n",
    "    Existing markings for definition and notation data (i.e. by\n",
    "    surrounding with double asterisks or by HTML tags) are preserved\n",
    "    (and turned into HTML tags), unless the markings overlap with \n",
    "    predictions, in which case the original is preserved (and still\n",
    "    turned into an HTML tag if possible)\n",
    "\n",
    "    Since the model can make \"invalid\" predictions (mostly those which\n",
    "    start or end within a LaTeX math mode str), the actual markings\n",
    "    are not necessarily direct translates from the model's predictions.\n",
    "    See the helper function `_consolidate_token_preds` for more details\n",
    "    on how this is implemented.\n",
    "    \n",
    "    **Raises**\n",
    "    Warning messages (`UserWarning`) are printed in the following situations:\n",
    "\n",
    "    - There are two consecutive tokens within the `pipeline`'s predictions\n",
    "      of different entity types (e.g. one is predicted to belong within a\n",
    "      definition and the other within a notation), but the latter token's\n",
    "      predicted `'entity'` more specifically begins with `'I-'` (i.e. is\n",
    "      `'I-definition'` or `'I-notation'`) as opposed to `'B-'`.\n",
    "        - `note`'s name, and path are included in the warning message in\n",
    "          this case.\n",
    "    - There are two consecutive tokens within the `pipeline`'s predictions\n",
    "      which the pipeline predicts to belong to the same entity, and yet\n",
    "      there is excessive space (specified by `excessive_space_threshold`)\n",
    "      between the end of the first token and the start of the second.\n",
    "\n",
    "    \"\"\"\n",
    "    mf = MarkdownFile.from_vault_note(note)\n",
    "    _process_mf(mf)\n",
    "    first_non_metadata_line, see_also_line = _get_main_text_lines(mf)\n",
    "     \n",
    "    main_text = mf.text_of_lines(first_non_metadata_line, see_also_line)\n",
    "    main_text = _format_main_text_and_add_html_tag_data(\n",
    "        note, pipeline, add_boxing_attr_to_existing_def_and_notat_markings,\n",
    "        excessive_space_threshold, main_text)\n",
    "    _write_text_with_html_tag_preds_to_note(\n",
    "        note, mf, main_text, first_non_metadata_line, see_also_line)\n",
    "\n",
    "\n",
    "def _process_mf(\n",
    "        mf: MarkdownFile) -> None:\n",
    "    \"\"\"Helper function to `auto_mark_def_and_notats`\"\"\"\n",
    "    mf.merge_display_math_mode()\n",
    "    mf.merge_display_math_mode_into_preceding_text()\n",
    "\n",
    "\n",
    "def _get_main_text_lines(\n",
    "        mf: MarkdownFile) -> tuple[int, int]:\n",
    "    \"\"\"Helper function to `auto_mark_def_and_notats`\"\"\"\n",
    "    tuppy = mf.metadata_lines()\n",
    "    if tuppy is not None:\n",
    "        first_non_metadata_line = tuppy[1] + 1\n",
    "    else:\n",
    "        first_non_metadata_line = 0 \n",
    "    see_also_line = mf.get_line_number_of_heading('See Also')\n",
    "    return first_non_metadata_line, see_also_line\n",
    "\n",
    "\n",
    "def _format_main_text_and_add_html_tag_data(\n",
    "        note: VaultNote,\n",
    "        pipeline: pipelines.token_classification.TokenClassificationPipeline, # The token classification pipeline that is used to predict whether tokens are part of definitions or notations introduced in the text.\n",
    "        add_boxing_attr_to_existing_def_and_notat_markings: bool,\n",
    "        excessive_space_threshold: int,\n",
    "        main_text: str,  # The main text to format and to add HTML tag data to\n",
    "        ) -> str:\n",
    "    \"\"\"Helper function to `auto_mark_def_and_notats`\"\"\"\n",
    "    main_text = add_space_to_lt_symbols_without_space(main_text)\n",
    "    main_text = convert_double_asterisks_to_html_tags(main_text)\n",
    "    main_text, existing_html_tag_data = remove_html_tags_in_text(main_text)\n",
    "    if add_boxing_attr_to_existing_def_and_notat_markings:\n",
    "        existing_html_tag_data = _add_nice_boxing_attrs_to_def_and_notat_tags(\n",
    "            existing_html_tag_data)\n",
    "\n",
    "    html_tags_to_add = _get_token_preds_by_dividing_main_text(\n",
    "        main_text, pipeline, note, excessive_space_threshold)\n",
    "\n",
    "    html_tags_to_add_back = _collate_html_tags(\n",
    "        existing_html_tag_data, html_tags_to_add)\n",
    "    return add_HTML_tag_data_to_raw_text(main_text, html_tags_to_add_back)\n",
    "\n",
    "\n",
    "def _get_token_preds_by_dividing_main_text(\n",
    "        main_text: str,\n",
    "        pipeline: pipelines.token_classification.TokenClassificationPipeline, # The token classification pipeline that is used to predict whether tokens are part of definitions or notations introduced in the text. Here, the tokenizer of this pipeline is used to estimate how many tokens a piece of subtext will have.\n",
    "        note: VaultNote,\n",
    "        excessive_space_threshold: int,    \n",
    "        ) -> list[tuple[bs4.element.Tag, int, int]]:  # Tag element, start, end, where main_text[start:end] needs to be replaced by the tag element.\n",
    "    \"\"\"\n",
    "    Divide the `main_text` into not-too-long pieces to return HTML tag predictions\n",
    "\n",
    "    Helper function for `_format_main_text_and_add_html_tag_data`.\n",
    "    \"\"\"\n",
    "    pieces_start_and_end = _divide_main_text(main_text, pipeline)\n",
    "    cumulative_html_tags_in_main = []\n",
    "    for start_of_piece, end_of_piece in pieces_start_and_end:\n",
    "        # text = main_text[start_of_piece:end_of_piece]\n",
    "        text = main_text[start_of_piece:]\n",
    "        html_tags_in_piece = _html_tags_from_token_preds(\n",
    "            text, pipeline(text), note, excessive_space_threshold)\n",
    "        html_tags_in_piece = _consolidate_token_preds(\n",
    "            text, html_tags_in_piece)\n",
    "        # start and end indices need to be re-adjusted with respect to their places in `main_text`\n",
    "        html_tags_for_piece_in_main_text = [\n",
    "            (tag, start_of_piece + start, start_of_piece + end)\n",
    "            for tag, start, end in html_tags_in_piece]\n",
    "        cumulative_html_tags_in_main = _collate_html_tags(\n",
    "            cumulative_html_tags_in_main, html_tags_for_piece_in_main_text)\n",
    "    return cumulative_html_tags_in_main\n",
    "\n",
    "\n",
    "def _divide_main_text(\n",
    "        main_text: str,\n",
    "        pipeline: pipelines.token_classification.TokenClassificationPipeline, # The token classification pipeline that is used to predict whether tokens are part of definitions or notations introduced in the text. Here, the tokenizer of this pipeline is used to estimate how many tokens a piece of subtext will have.\n",
    "        # ) -> list[tuple[str, int, int]]:  # The str is a chunk of text, the first int is the index in `main_text` that the chunk starts at, and the second int is the approximate token length of the text. Appending all the chunks of text as they are should result back in the original text.\n",
    "        ) -> list[tuple[int, int]]:  # Each tuple is a start and end range for pieces of `main_text` to be considered for predictions\n",
    "    \"\"\"Divides `main_text` so that predictions can be made on smaller chunks of text.\n",
    "    \n",
    "    Assumes that dividing `main_text` along newline characters `\\n` will result in\n",
    "    pieces that are \"not too long\".\n",
    "\n",
    "    Helper function to `_format_main_text_and_add_html_tag_data`.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    main_text.split('\\n')\n",
    "    tokenizer = pipeline.tokenizer\n",
    "    newline_indices = [i for i, char in enumerate(main_text) if char == '\\n']\n",
    "    newline_indices.insert(0, 0)\n",
    "    chunks = []  # list[tuple[str, int, int]]  # The str is a chunk of text, the first int is the index in `main_text` that the chunk starts at, and the second int is the approximate token length of the text. Appending all the chunks of text as they are should result back in the original text.\n",
    "    for start, end in pairwise(newline_indices):\n",
    "        chunk = main_text[start:end]\n",
    "        chunks.append((chunk, start, len(tokenizer(chunk)['input_ids'])))\n",
    "    last_chunk = main_text[newline_indices[-1]:]\n",
    "    chunks.append((last_chunk, newline_indices[-1], len(tokenizer(chunk))))\n",
    "    return _find_places_to_divide_from_chunks(chunks, pipeline)\n",
    "\n",
    "\n",
    "def _find_places_to_divide_from_chunks(\n",
    "        chunks: list[tuple[str, int, int]], # The str is a chunk of text, the first int is the index in `main_text` that the chunk starts at, and the second int is the approximate token length of the text. Appending all the chunks of text as they are should result back in the original text.\n",
    "        pipeline: pipelines.token_classification.TokenClassificationPipeline, # The token classification pipeline that is used to predict whether tokens are part of definitions or notations introduced in the text. Here, the tokenizer of this pipeline is used to estimate how many tokens a piece of subtext will have.\n",
    "        ) -> list[tuple[int, int]]: # Each tuple is a start and end range for pieces of `main_text` to be considered for predictions\n",
    "    \"\"\"Identify appropriate indices in `main_text` where (overlapping)\n",
    "    pieces in `main_text` should start/end for predictions with `pipeline`.\n",
    "    \n",
    "    Helper function to `_divide_main_text`.\n",
    "\n",
    "    We describe how this function is implemented: starting at the first chunk\n",
    "    (chunks are non-overlapping), start to consider consecutive chunks to\n",
    "    make up a piece. So maybe we have chunks\n",
    "\n",
    "        A B C D E F ....\n",
    "\n",
    "    We build a piece chunk-by-chunk, considering the total token length of the\n",
    "    built sub-piece along the way. The first chunk within a sub-piece \n",
    "    that makes the sub-piece of token-length greater than half the max\n",
    "    token length with respect to `pipeline.tokenizer` will become the start of the\n",
    "    next piece, unless the very first chunk in the piece is already longer than half the max\n",
    "    token length with respect to the tokenizer (this is to ensure that the\n",
    "    piece-building process does not keep starting at the same chunk).\n",
    "    Moreover, a piece will stop building as soon as its token-length exceeds\n",
    "    the max length of the tokenizer.\n",
    "\n",
    "    For instance, maybe the max length for the tokenizer is 512, and the chunks\n",
    "    are of the following length:\n",
    "\n",
    "        A   B    C  D   E    F     ...\n",
    "        76  130  70 13  150  140   ...\n",
    "\n",
    "    We first build the piece starting at A:\n",
    "\n",
    "        A\n",
    "        76\n",
    "\n",
    "    We continue building the piece by \"appending\" B:\n",
    "\n",
    "        A   B\n",
    "        76  130\n",
    "\n",
    "    Once we append C as well, the piece's length is now 276 and hence over half of 512,\n",
    "    so the next piece will start at C: \n",
    "\n",
    "        A   B    C\n",
    "        76  130  70\n",
    "\n",
    "    Subsequently, we continue building the piece. Only once F is appended does the \n",
    "    length of the entire piece exceed 512 (the length is 579):\n",
    "\n",
    "        A   B    C  D   E    F\n",
    "        76  130  70 13  150  140\n",
    "    \n",
    "    And then we begin building the next piece from C.\n",
    "\n",
    "    Also, consider an example where the first chunk's length exceeds half the max length\n",
    "    of the tokenizer:\n",
    "\n",
    "        A   B    C   ...\n",
    "        300 200  100 ...\n",
    "\n",
    "    Here, the first piece will consist of the chunks A, B, and C because\n",
    "    the length of the piece exceeds the max length of 512 only after appending C.\n",
    "    To guarantee that the next piece does not start with the chunk A again, B is \n",
    "    used as the first chunk in the next piece:\n",
    "\n",
    "        B    C   ...\n",
    "        200  100 ...\n",
    "\n",
    "    If any chunk's token length exceeds the tokenizer's max_model_length, then\n",
    "    the pipeline/model can only predict on the starting tokens in the chunk. \n",
    "    As such, the chunks must not be \"too long\" for best results on the model's predictions.\n",
    "    \"\"\"\n",
    "    tokenizer = pipeline.tokenizer\n",
    "    start_chunk_index, next_piece_start_chunk_index = 0, 0\n",
    "    current_piece_token_len = 0\n",
    "    pieces_start_and_end = []\n",
    "    i = 0\n",
    "    while i < len(chunks):\n",
    "        chunk = chunks[i]\n",
    "        current_piece_token_len = current_piece_token_len + chunk[2]\n",
    "        if (current_piece_token_len > tokenizer.model_max_length / 2\n",
    "                and start_chunk_index == next_piece_start_chunk_index):\n",
    "            # Mark where the next piece should start\n",
    "            next_piece_start_chunk_index = i if start_chunk_index != i else i+1\n",
    "        if (current_piece_token_len > tokenizer.model_max_length):\n",
    "            # Add a new item in the list and then Start a new piece\n",
    "            # start_chunk, end_chunk = chunks[start_chunk_index], chunk\n",
    "            # start_char_index = start_chunk[1]\n",
    "            # end_char_index = end_chunk[1] + len(end_chunk[0])\n",
    "            # pieces_start_and_end.append([start_char_index, end_char_index])\n",
    "            _append_to_pieces_start_and_end(\n",
    "                pieces_start_and_end, chunks[start_chunk_index], chunk)\n",
    "            i, start_chunk_index = (\n",
    "                next_piece_start_chunk_index, next_piece_start_chunk_index)\n",
    "            current_piece_token_len = 0\n",
    "            continue\n",
    "        i += 1\n",
    "    # Add the last chunk at the end\n",
    "    _append_to_pieces_start_and_end(\n",
    "        pieces_start_and_end, chunks[start_chunk_index], chunks[-1])\n",
    "    return pieces_start_and_end\n",
    "\n",
    "\n",
    "def _append_to_pieces_start_and_end(\n",
    "        pieces_start_and_end: list[tuple[int, int]],\n",
    "        start_chunk: tuple[str, int, int],\n",
    "        end_chunk: tuple[str, int, int]\n",
    "        ) -> None:\n",
    "    \"\"\"Helper function to `_find_places_to_divide_from_chunks`\"\"\"\n",
    "    start_char_index = start_chunk[1]\n",
    "    end_char_index = end_chunk[1] + len(end_chunk[0])\n",
    "    pieces_start_and_end.append([start_char_index, end_char_index])\n",
    "\n",
    "\n",
    "\n",
    "def _write_text_with_html_tag_preds_to_note(\n",
    "        note: VaultNote,\n",
    "        mf: MarkdownFile,\n",
    "        main_text: str,\n",
    "        first_non_metadata_line: int,\n",
    "        see_also_line: int\n",
    "        ) -> None:\n",
    "    \"\"\"Helper function to `auto_mark_def_and_notats`\"\"\"\n",
    "    mf.remove_lines(first_non_metadata_line, see_also_line)\n",
    "    mf.insert_line(first_non_metadata_line,\n",
    "                   {'type': MarkdownLineEnum.DEFAULT, 'line': main_text})\n",
    "    mf.add_tags('_auto/def_and_notat_identified')\n",
    "    mf.write(note)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following examples, we mock pipeline objects instead of using actual ones.\n",
    "\n",
    "In the below example, we run the `auto_mark_def_and_notats` function on a note that has double asterisks `**` surrounding parts of the text that introduced definitions or notations. In these cases, appropriate HTML tags replace the double asterisks instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text before:\n",
      "\n",
      "\n",
      "---\n",
      "cssclass: clean-embeds\n",
      "aliases: []\n",
      "tags: [_meta/literature_note, _meta/definition, _meta/notation]\n",
      "---\n",
      "# Ring of integers modulo $n$[^1]\n",
      "\n",
      "Let $n \\geq 1$ be an integer. The **ring of integers modulo $n$**, denoted by **$\\mathbb{Z}/n\\mathbb{Z}$**, is, informally, the ring whose elements are represented by the integers with the understanding that $0$ and $n$ are equal.\n",
      "\n",
      "More precisely, $\\mathbb{Z}/n\\mathbb{Z}$ has the elements $0,1,\\ldots,n-1$.\n",
      "\n",
      "...\n",
      "\n",
      "\n",
      "# See Also\n",
      "- [[reference_with_tag_labels_Exercise 1|reference_with_tag_labels_Z_nZ_is_a_ring]]\n",
      "# Meta\n",
      "## References\n",
      "\n",
      "## Citations and Footnotes\n",
      "[^1]: Kim, Definition 2\n",
      "\n",
      "\n",
      "\n",
      "Text after:\n",
      "\n",
      "---\n",
      "cssclass: clean-embeds\n",
      "aliases: []\n",
      "tags: [_meta/literature_note, _auto/def_and_notat_identified, _meta/notation, _meta/definition]\n",
      "---\n",
      "# Ring of integers modulo $n$[^1]\n",
      "\n",
      "Let $n \\geq 1$ be an integer. The <b definition=\"\" style=\"border-width:1px;border-style:solid;padding:3px\">ring of integers modulo $n$</b>, denoted by <span notation=\"\" style=\"border-width:1px;border-style:solid;padding:3px\">$\\mathbb{Z}/n\\mathbb{Z}$</span>, is, informally, the ring whose elements are represented by the integers with the understanding that $0$ and $n$ are equal.\n",
      "\n",
      "More precisely, $\\mathbb{Z}/n\\mathbb{Z}$ has the elements $0,1,\\ldots,n-1$.\n",
      "\n",
      "...\n",
      "\n",
      "\n",
      "# See Also\n",
      "- [[reference_with_tag_labels_Exercise 1|reference_with_tag_labels_Z_nZ_is_a_ring]]\n",
      "# Meta\n",
      "## References\n",
      "\n",
      "## Citations and Footnotes\n",
      "[^1]: Kim, Definition 2\n"
     ]
    }
   ],
   "source": [
    "with (tempfile.TemporaryDirectory(prefix='temp_dir', dir=os.getcwd()) as temp_dir,\n",
    "      mock.patch('__main__.pipelines.token_classification.TokenClassificationPipeline') as mock_pipeline):\n",
    "    temp_vault = Path(temp_dir) / 'test_vault_6'\n",
    "    shutil.copytree(_test_directory() / 'test_vault_6', temp_vault)\n",
    "\n",
    "    mock_pipeline.tokenizer.model_max_length = 512\n",
    "\n",
    "    vn = VaultNote(temp_vault, name='reference_with_tag_labels_Definition 2')\n",
    "    print(\"Text before:\\n\\n\")\n",
    "    print(vn.text())\n",
    "    print(\"\\n\\n\\nText after:\\n\")\n",
    "    auto_mark_def_and_notats(vn, mock_pipeline)\n",
    "    print(vn.text())\n",
    "    mf = MarkdownFile.from_vault_note(vn)\n",
    "    assert mf.has_tag('_auto/def_and_notat_identified')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: more examples with pipeline mocking actual outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trouver_py310_venv",
   "language": "python",
   "name": "trouver_py310_venv"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
