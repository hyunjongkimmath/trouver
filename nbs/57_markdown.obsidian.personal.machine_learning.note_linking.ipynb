{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp markdown.obsidian.personal.machine_learning.note_linking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# markdown.obsidian.personal.machine_learning.note_linking\n",
    "> Functions for gathering note linking data and to use models trained with said data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import ast\n",
    "from abc import ABC, abstractmethod\n",
    "import copy\n",
    "from datasets import Dataset\n",
    "from enum import Enum\n",
    "from itertools import combinations\n",
    "from pathlib import Path\n",
    "from os import PathLike\n",
    "import random\n",
    "import re\n",
    "from typing import Literal, Optional, TypedDict, TypeVar, Union\n",
    "\n",
    "from fastcore.basics import patch\n",
    "import torch\n",
    "from transformers import Pipeline\n",
    "\n",
    "\n",
    "from trouver.helper import latex_str_is_likely_in_latex_str, latex_str_in_latex_str_fuzz_metric\n",
    "from trouver.helper.numbers import modify_int_by_at_most_at_most_offset, modify_int_by_at_most_at_most_value\n",
    "from trouver.helper.regex import find_regex_in_text, latex_indices\n",
    "from trouver.markdown.markdown.file import MarkdownFile\n",
    "from trouver.helper.latex import (\n",
    "    augment_text, choose_modification_methods_at_random, remove_font_styles_at_random, change_font_styles_at_random, change_greek_letters_at_random, remove_math_keywords, random_latex_command_removal, random_word_removal, dollar_sign_manipulation, random_char_modification\n",
    "    )\n",
    "from trouver.markdown.obsidian.footnotes import identify_available_footnote_numbers\n",
    "from trouver.markdown.obsidian.links import links_from_text, LinkType, ObsidianLink, MARKDOWNLINK_CAPTURE_PATTERN\n",
    "from trouver.markdown.obsidian.personal.information_notes import index_note_of_note\n",
    "from trouver.markdown.obsidian.personal.machine_learning.note_data import (\n",
    "    NoteLinkEnum, NoteData, note_data_order_cmp, randomly_modify, InfoNoteData, NotatNoteData, note_data_from_index_note, note_data_from_reference, find_reverse_links, get_main_note_content_of_notat_note_data, _note_data_from_vault_note_on_the_fly\n",
    "    )\n",
    "from trouver.markdown.obsidian.personal.notation.in_standard_information_note import notation_notes_linked_in_see_also_section\n",
    "from trouver.markdown.obsidian.personal.notation.parse import NotationNoteParsed, parse_notation_note, notation_in_note, main_of_notation\n",
    "from trouver.markdown.obsidian.personal.note_processing import process_standard_information_note, ProcessNoteError\n",
    "from trouver.markdown.obsidian.personal.note_type import (\n",
    "    PersonalNoteTypeEnum, assert_note_is_of_type, note_is_of_type, type_of_note\n",
    ")\n",
    "\n",
    "\n",
    "from trouver.markdown.obsidian.personal.notes import (\n",
    "    notes_linked_in_note,  notes_linked_in_notes_linked_in_note)\n",
    "from trouver.markdown.obsidian.personal.reference import index_note_for_reference, all_paths_to_notes_in_reference_folder\n",
    "from trouver.markdown.obsidian.vault import VaultNote\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from unittest.mock import MagicMock\n",
    "from unittest.mock import patch as mock_patch\n",
    "\n",
    "from fastcore.test import *\n",
    "\n",
    "from nbdev.showdoc import show_doc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sieve instances of pairs for building a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, it is difficult to manually create all links that ought to be linked. In particular, while it can be easy to extract \"positive\" instances of links (by virtue of simply finding explicit links), it is more difficult to obtain \"negative\" instances of links with certainty. The general method for obtaining \"negative\" instances is nevertheless to randomly sample pairs of notes and consider such a pair as \"negative\" if there is no link between them; some notes are \"well focused\" on in practice (and in particular has many links to other notes); if there are no links between two such notes, then it is likely that there are not supposed to be links between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class NotePairData(TypedDict):\n",
    "    origin_note: NoteData\n",
    "    relied_note: NoteData\n",
    "    # linked_type: NoteLinkEnum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def link_types_for_note_pair_data(\n",
    "        pair_data: NotePairData\n",
    "        ) -> set[NoteLinkEnum]:\n",
    "    relied_note_name: str = pair_data['relied_note'].note_name\n",
    "    directly_linked_notes_from_origin = pair_data['origin_note'].directly_linked_notes\n",
    "    if relied_note_name in directly_linked_notes_from_origin:\n",
    "        return set(directly_linked_notes_from_origin[relied_note_name])\n",
    "    else:\n",
    "        return set([NoteLinkEnum.NO_LINK])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _high_count_note_data(\n",
    "        info_note_data: dict[str, InfoNoteData],\n",
    "        notat_note_data: dict[str, NotatNoteData],\n",
    "        ) -> tuple[set[str], set[str]]:\n",
    "    \"\"\"\n",
    "    Helper function to `sieve_note_data_pairs`.\n",
    "    \"\"\"\n",
    "\n",
    "    high_count_info_notes: set[str] = set([\n",
    "        name for name, data_point in info_note_data.items()\n",
    "        if len(data_point.reverse_linked_notes) > 4\n",
    "        and len(info_note_data[name].directly_linked_notes) > 2])\n",
    "    high_count_notat_notes: set[str] = set([\n",
    "        name for name, data_point in notat_note_data.items()\n",
    "        if len(data_point.reverse_linked_notes) > 4])\n",
    "\n",
    "    return (high_count_info_notes, high_count_notat_notes)\n",
    "\n",
    "\n",
    "def _mid_count_note_data(\n",
    "        info_note_data: dict[str, InfoNoteData],\n",
    "        notat_note_data: dict[str, NotatNoteData],\n",
    "        high_count_info_notes: set[str],\n",
    "        high_count_notat_notes: set[str],\n",
    "        ) -> tuple[set[str], set[str]]:\n",
    "    \"\"\"\n",
    "    Helper function to `sieve_note_data_pairs`.\n",
    "    \"\"\"\n",
    "\n",
    "    mid_count_info_notes: set[str] = set([\n",
    "        name for name, data_point in info_note_data.items()\n",
    "        if len(data_point.reverse_linked_notes) > 2\n",
    "        and len(info_note_data[name].directly_linked_notes) > 1\n",
    "        and name not in high_count_info_notes])\n",
    "    mid_count_notat_notes: list[str] = set([\n",
    "        name for name, data_point in notat_note_data.items()\n",
    "        if len(data_point.reverse_linked_notes) > 2\n",
    "        and name not in high_count_notat_notes])\n",
    "\n",
    "    return (mid_count_info_notes, mid_count_notat_notes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _positive_instances_from_high_or_mid_count_notes(\n",
    "        high_count_notes: set[str],\n",
    "        mid_count_notes: set[str],\n",
    "        note_data: dict[str, NoteData],\n",
    "        ) -> list[tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Helper function to `sieve_note_data_pairs`.\n",
    "    \"\"\"\n",
    "    chosen_pairs: list[tuple[str, str]] = []\n",
    "    # Get all \"positive\" note links from high count notes to high or mid count notes.\n",
    "    for high_count_note_name in list(high_count_notes):\n",
    "    # for high_count_note_name, high_count_data_point in high_count_notes.items():\n",
    "        high_count_data_point = note_data[high_count_note_name]\n",
    "        for other_note, _ in high_count_data_point.directly_linked_notes.items():\n",
    "            if other_note in high_count_notes or other_note in mid_count_notes:\n",
    "                chosen_pairs.append((high_count_note_name, other_note))\n",
    "    # Get all \"positive\" note links from mid count notes to high count notes.\n",
    "    for mid_count_note_name in list(mid_count_notes):\n",
    "        mid_count_data_point = note_data[mid_count_note_name]\n",
    "    # for mid_count_note_name, mid_count_data_point in mid_count_notes.items():\n",
    "        for other_note, _ in mid_count_data_point.directly_linked_notes.items():\n",
    "            if other_note in high_count_notes:\n",
    "                chosen_pairs.append((mid_count_note_name, other_note))\n",
    "    return chosen_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _negative_instances_from_high_or_mid_count_notes(\n",
    "        high_count_notes: set[str],\n",
    "        mid_count_notes: set[str],\n",
    "        note_data: dict[str, NoteData],\n",
    "        num_pairs: int # The approximate number of pairs to sample.\n",
    "        ) -> list[tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Get \"negative\" pair instances from high or mid count notes, i.e. pairs where\n",
    "    the origin note seem to not link to relied note.\n",
    "    \"\"\"\n",
    "    high_count_weights = [\n",
    "        (len(note_data[note_name].reverse_linked_notes)**0.5)\n",
    "        for note_name in list(high_count_notes)]\n",
    "    mid_count_weights = [\n",
    "        (len(note_data[note_name].reverse_linked_notes)**0.5)\n",
    "        for note_name in list(mid_count_notes)]\n",
    "    high_to_high_samples = int(0.5 * num_pairs)\n",
    "    high_to_mid_samples = int(0.25 * num_pairs)\n",
    "    mid_to_high_samples = int(0.25 * num_pairs)\n",
    "    high_count_notes_list = list(high_count_notes)\n",
    "    mid_count_notes_list = list(mid_count_notes)\n",
    "\n",
    "    sample_pairs: set[tuple[str, str]] = set()\n",
    "\n",
    "    origin_notes = random.choices(\n",
    "        high_count_notes_list, weights=high_count_weights, k=high_to_high_samples\n",
    "        ) if high_count_notes_list else []\n",
    "    relied_notes = random.choices(\n",
    "        high_count_notes_list, weights=high_count_weights, k=high_to_high_samples\n",
    "        ) if high_count_notes_list else []\n",
    "    for origin_note, relied_note in zip(origin_notes, relied_notes):\n",
    "        if (origin_note == relied_note\n",
    "                or relied_note in note_data[origin_note].directly_linked_notes):\n",
    "            continue\n",
    "        else:\n",
    "            sample_pairs.add((origin_note, relied_note))\n",
    "\n",
    "    origin_notes = random.choices(\n",
    "        high_count_notes_list, weights=high_count_weights, k=high_to_mid_samples\n",
    "        ) if high_count_notes_list else []\n",
    "    relied_notes = random.choices(\n",
    "        mid_count_notes_list, weights=mid_count_weights, k=high_to_mid_samples\n",
    "        ) if mid_count_notes_list else []\n",
    "    for origin_note, relied_note in zip(origin_notes, relied_notes):\n",
    "        if (origin_note == relied_note\n",
    "                or relied_note in note_data[origin_note].directly_linked_notes):\n",
    "            continue\n",
    "        else:\n",
    "            sample_pairs.add((origin_note, relied_note))\n",
    "\n",
    "    origin_notes = random.choices(\n",
    "        mid_count_notes_list, weights=mid_count_weights, k=mid_to_high_samples\n",
    "        ) if mid_count_notes_list else []\n",
    "    relied_notes = random.choices(\n",
    "        high_count_notes_list, weights=high_count_weights, k=mid_to_high_samples\n",
    "        ) if high_count_notes_list else []\n",
    "    for origin_note, relied_note in zip(origin_notes, relied_notes):\n",
    "        if (origin_note == relied_note\n",
    "                or relied_note in note_data[origin_note].directly_linked_notes):\n",
    "            continue\n",
    "        else:\n",
    "            sample_pairs.add((origin_note, relied_note))\n",
    "\n",
    "    return list(sample_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _similar_notation_pairs(\n",
    "        notat_note_data: dict[str, NotatNoteData],\n",
    "        # ) -> list[tuple[str, str]]:\n",
    "        ) -> dict[str, set[str]]: # The keys are names of notation notes and the values are sets of names of notation notes whose notations are similar to the one explained in the key notation note.\n",
    "    \"\"\"\n",
    "    Identify pairs of names of notation notes whose notations are similar.\n",
    "\n",
    "    Helper function to sieve_note_data_pairs.\n",
    "\n",
    "    The similarity is measured by Jaro-Winkler, which works well on short\n",
    "    strings.\n",
    "    \"\"\"\n",
    "    jarowinkler = JaroWinkler()\n",
    "    # similar_notation_pairs: list[tuple[str, str]] = []\n",
    "    similar_notation_dict: dict[str, set[str]] = {}\n",
    "    for notat_name_1, notat_name_2 in combinations(notat_note_data, 2):\n",
    "        notat_data_1, notat_data_2 = notat_note_data[notat_name_1], notat_note_data[notat_name_2]\n",
    "        notat_str_1 = notat_data_1.parsed.notation_str\n",
    "        notat_str_2 = notat_data_2.parsed.notation_str\n",
    "        similarity = jarowinkler.similarity(notat_str_1, notat_str_2)\n",
    "        reverse_similarity = jarowinkler.similarity(notat_str_1[::-1], notat_str_2[::-1]) \n",
    "        if similarity > 0.9 or reverse_similarity > 0.9:\n",
    "            _update_dict(similar_notation_dict, notat_name_1, notat_name_2)\n",
    "            _update_dict(similar_notation_dict, notat_name_2, notat_name_1)\n",
    "    return similar_notation_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _random_pair_replacing_notation_notes_with_similar_notation_notes(\n",
    "        original_pair: tuple[str, str],\n",
    "        similar_notation_dict: set[str, set[str]]\n",
    "        ) -> tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Helper function to `_random_pair_replacing_notation_notes_with_similar_notation_notes`.\n",
    "    \"\"\"\n",
    "    origin_note_name = original_pair[0]\n",
    "    relied_note_name = original_pair[1]\n",
    "    if random.random() > 0.5:\n",
    "        if origin_note_name in similar_notation_dict:\n",
    "            origin_note_name = random.choice(list(similar_notation_dict[origin_note_name]))\n",
    "    if random.random() > 0.5:\n",
    "        if relied_note_name in similar_notation_dict:\n",
    "            relied_note_name = random.choice(list(similar_notation_dict[relied_note_name]))\n",
    "    return (origin_note_name, relied_note_name)\n",
    "    \n",
    "\n",
    "    \n",
    "def _pairs_with_notation_notes_replaced_with_similar_notation_notes(\n",
    "        sieved_pairs: set[tuple[str, str]],\n",
    "        count: int, # The approximate number of pairs to attempt to obtain.\n",
    "        similar_notation_dict: set[str, set[str]], # An output of `_similar_notation_pairs`\n",
    "        # notat_note_data: dict[str, NotatNoteData],\n",
    "        ) -> list[tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Return modified versions of entries of `sieved_pairs` drawn at random\n",
    "    where notation note names are replaced by names of notation notes whose \n",
    "    introduced notations are similar, in accordance to `similar_notation_dict`.\n",
    "\n",
    "    Helper function to `sieve_note_data_pairs`.\n",
    "    \"\"\"\n",
    "    sieved_pairs_list = list(sieved_pairs)\n",
    "    new_pairs: list[tuple[str, str]] = []\n",
    "    for _ in range(count):\n",
    "        original_pair = random.choice(sieved_pairs_list)\n",
    "        new_pair = _random_pair_replacing_notation_notes_with_similar_notation_notes(\n",
    "            original_pair, similar_notation_dict)\n",
    "        new_pairs.append(new_pair)\n",
    "    return new_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _pair_is_admissible(\n",
    "        origin_note: str,\n",
    "        relied_note: str,\n",
    "        note_data: dict[str, NoteData],\n",
    "        info_note_data: dict[str, InfoNoteData],\n",
    "        notat_note_data: dict[str, InfoNoteData],\n",
    "        ) -> bool:\n",
    "    origin_note_has_tags = note_data[origin_note].tags is not None\n",
    "    if not origin_note_has_tags:\n",
    "        return True\n",
    "    if (('_auto/links_added' in note_data[origin_note].tags and relied_note in info_note_data)\n",
    "            or ('_auto/notations_added' in note_data[origin_note].tags and relied_note in notat_note_data)):\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def sieve_note_data_pairs(\n",
    "        info_note_data: dict[str, InfoNoteData],\n",
    "        notat_note_data: dict[str, NotatNoteData],\n",
    "        ) -> list[NotePairData]:\n",
    "    note_data: dict[str, NoteData] = {}\n",
    "    note_data.update(info_note_data)\n",
    "    note_data.update(notat_note_data)\n",
    "    high_count_info_notes, high_count_notat_notes = _high_count_note_data(\n",
    "        info_note_data, notat_note_data) # set[str]\n",
    "    high_count_notes: set[str] = high_count_info_notes.union(high_count_notat_notes)\n",
    "    mid_count_info_notes, mid_count_notat_notes = _mid_count_note_data(\n",
    "        info_note_data, notat_note_data, high_count_info_notes, high_count_notat_notes)\n",
    "    mid_count_notes: set[str] = mid_count_info_notes.union(mid_count_notat_notes)\n",
    "\n",
    "    positive_pairs: list[tuple[str, str]] = _positive_instances_from_high_or_mid_count_notes(\n",
    "        high_count_notes, mid_count_notes, note_data)\n",
    "    negative_pairs: list[tuple[str, str]] = _negative_instances_from_high_or_mid_count_notes(\n",
    "        high_count_notes, mid_count_notes, note_data, len(positive_pairs)*3)\n",
    "    sieved_pairs: set[tuple[str, str]] = set(positive_pairs)\n",
    "    sieved_pairs.update(negative_pairs)\n",
    "\n",
    "    similar_notation_dict: dict[str, set[str]] = _similar_notation_pairs(notat_note_data)\n",
    "    # similar_notation_names: list[tuple[str, str]] = _similar_notation_pairs(notat_note_data)\n",
    "    sieved_pairs.update(_pairs_with_notation_notes_replaced_with_similar_notation_notes(\n",
    "        sieved_pairs, len(positive_pairs), similar_notation_dict))\n",
    "    sieved_pairs_list = list(sieved_pairs)\n",
    "    note_pair_data_list: list[NotePairData] = []\n",
    "    for origin_note, relied_note in sieved_pairs_list:\n",
    "        if not _pair_is_admissible(\n",
    "                origin_note, relied_note, note_data, info_note_data, notat_note_data):\n",
    "            continue\n",
    "        note_pair_data_list.append(\n",
    "            NotePairData(\n",
    "                origin_note=note_data[origin_note],\n",
    "                relied_note=note_data[relied_note]))\n",
    "    return note_pair_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_pairs = sieve_note_data_pairs(info_note_data, notat_note_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count = 0\n",
    "# for data_pair in data_pairs:\n",
    "#     if data_pair['relied_note'].note_name in data_pair['origin_note'].directly_linked_notes:\n",
    "#         if data_pair['relied_note'].note_name == 'hotta_takeuchi_tanisaki_dmpsrt_notation_X_smooth_non_singular_algebraic_variety_over_C':\n",
    "#             print(data_pair['origin_note'].note_name)\n",
    "#             print(data_pair['relied_note'].note_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index = 2\n",
    "# print(data_pairs[index]['origin_note'].note_name)\n",
    "# print(data_pairs[index]['relied_note'].note_name)\n",
    "# print(data_pairs[index]['origin_note'].directly_linked_notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_pairs[8]['origin_note'].note_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting a note pair into a string and data augmentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _erase_position_metadata(\n",
    "        augmentation: Literal['high', 'mid',' low'] | None,\n",
    "        ) -> bool:\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    rand_value = random.random()\n",
    "    if augmentation == 'high':\n",
    "        return rand_value < 0.3\n",
    "    elif augmentation == 'mid':\n",
    "        return rand_value < 0.2\n",
    "    elif augmentation == 'low':\n",
    "        return rand_value < 0.1\n",
    "    return False\n",
    "    \n",
    "    \n",
    "def string_from_note_pair(\n",
    "            pair_data: NotePairData,\n",
    "            format: Literal['bert', 't5'],\n",
    "            # note_data: dict[str, NoteData],\n",
    "        ) -> str:\n",
    "    origin_data = pair_data['origin_note']\n",
    "    relied_data = pair_data['relied_note']\n",
    "    origin_data_string = origin_data.data_string(format)\n",
    "    relied_data_string = relied_data.data_string(format)\n",
    "    if format == 'bert':\n",
    "        return f'{origin_data_string}\\n\\n[SEP]\\n\\n{relied_data_string}'\n",
    "    else:\n",
    "        return f'{origin_data_string}\\n\\n</s>\\n\\n{relied_data_string}'\n",
    "\n",
    "\n",
    "def augment_note_pair(\n",
    "        pair_data: NotePairData,\n",
    "        augmentation: Optional[Literal['high', 'mid', 'low']] = None,\n",
    "        include_position_data_for_origin: bool = True,\n",
    "        include_position_data_for_relied: bool = True,\n",
    "        ) -> NotePairData:\n",
    "    \"\"\"\n",
    "    Return an augmented copy of `pair_data`.\n",
    "    \"\"\"\n",
    "    origin_data = pair_data['origin_note'].deepcopy()\n",
    "    relied_data = pair_data['relied_note'].deepcopy()\n",
    "    erase_position_data_for_origin_data = _erase_position_metadata(augmentation) or not include_position_data_for_origin\n",
    "    erase_position_data_for_relied_data = _erase_position_metadata(augmentation) or not include_position_data_for_relied\n",
    "    if augmentation is not None:\n",
    "        origin_data.randomly_modify(augmentation, erase_position_data_for_origin_data)\n",
    "        relied_data.randomly_modify(augmentation, erase_position_data_for_relied_data)\n",
    "    return NotePairData(\n",
    "        origin_note=origin_data, relied_note=relied_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForSeq2SeqLM, AutoModelForTokenClassification, AutoTokenizer, pipeline\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained('hyunjongkimmath/notation_summarizations_model')\n",
    "# tokenizer = AutoTokenizer.from_pretrained('hyunjongkimmath/notation_summarizations_model')\n",
    "# summarizer = pipeline('summarization', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class NoteLinkingDataPoint(TypedDict):\n",
    "    \"\"\"\n",
    "    A dict object that is \n",
    "    \"\"\"\n",
    "    origin_note_name: str\n",
    "    relied_note_name: str\n",
    "    input_text: str\n",
    "    # Keys derived from NoteLinkEnum (excluding NO_LINK)\n",
    "    link_types: list[str] # The str are the names of `NoteLinkEnum`.\n",
    "    # info_to_info_in_content: bool\n",
    "    # info_to_info_in_see_also: bool\n",
    "    # info_to_info_via_notat: bool\n",
    "    # info_to_notat_via_embedding: bool\n",
    "    # notat_to_info: bool\n",
    "    # notat_to_info_via_notat: bool\n",
    "    # notat_to_notat: bool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def dict_data_point_from_pair(\n",
    "        pair_data: NotePairData,\n",
    "        format: Literal['bert', 't5'],\n",
    "        # note_data: dict[str, NoteData],\n",
    "        ) -> NoteLinkingDataPoint:\n",
    "    \"\"\"\n",
    "    Obtain a `NoteLinkingDataPoint` object from a `NotePairData` object.\n",
    "    \"\"\"\n",
    "    origin_note_name = pair_data['origin_note'].note_name\n",
    "    relied_note_name = pair_data['relied_note'].note_name\n",
    "    input_text = string_from_note_pair(pair_data, format)    \n",
    "    \n",
    "    link_types: list[NoteLinkEnum] = list(\n",
    "        link_types_for_note_pair_data(pair_data))\n",
    "    link_types: list[str] = [value.name for value in link_types]\n",
    "    return NoteLinkingDataPoint(\n",
    "        origin_note_name=origin_note_name,\n",
    "        relied_note_name=relied_note_name,\n",
    "        input_text=input_text,\n",
    "        link_types=link_types,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def dataset_from_note_data(\n",
    "        info_note_data: dict[str, InfoNoteData],\n",
    "        notat_note_data: dict[str, NotatNoteData],\n",
    "        augment: bool,\n",
    "        format: Literal['bert', 't5'],\n",
    "        ) -> Dataset:\n",
    "    note_data_pairs: list[NotePairData] = sieve_note_data_pairs(\n",
    "        info_note_data, notat_note_data)\n",
    "    dict_data: list[NoteLinkingDataPoint] = []\n",
    "    for pair_data in note_data_pairs:\n",
    "        dict_data.append(dict_data_point_from_pair(\n",
    "            pair_data, format))\n",
    "        if not augment:\n",
    "            continue\n",
    "        for augmentation in ['low', 'mid', 'high']:\n",
    "            augmented_pair_data = augment_note_pair(\n",
    "                pair_data, augmentation)\n",
    "            dict_data.append(dict_data_point_from_pair(\n",
    "                augmented_pair_data, format))\n",
    "    return Dataset.from_list(dict_data)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get predictions from model pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MultiLabelPipeline(Pipeline):\n",
    "    \"\"\"\n",
    "    Implementing this `Pipeline` class is necessary because HuggingFAce's standard\n",
    "    `text-classification` pipeline uses softmax, which is suitable for single-label or\n",
    "    multi-class classification; a sigmoid activation function is more suitable for\n",
    "    multi-label classification.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, tokenizer, **kwargs):\n",
    "        super().__init__(model=model, tokenizer=tokenizer, **kwargs)\n",
    "\n",
    "    def _sanitize_parameters(self, **kwargs):\n",
    "        return {}, {}, {}\n",
    "\n",
    "    def preprocess(self, inputs, **kwargs):\n",
    "        return self.tokenizer(inputs, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    def _forward(self, model_inputs, **kwargs):\n",
    "        return self.model(**model_inputs)\n",
    "\n",
    "    def postprocess(self, model_outputs, **kwargs):\n",
    "        logits = model_outputs.logits\n",
    "        probabilities = torch.sigmoid(logits)  # Use sigmoid for multi-label\n",
    "        return probabilities.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def prediction_by_note_linking_model(\n",
    "        origin_data: NoteData, # The `NoteData` object representing the \"origin note\", i.e.  the note from which a link to the \"relied note\" is considered.\n",
    "        relied_data: NoteData, # The `NoteData` object representing the \"relied note\", i.e.  the note to which a link from the \"origin note\" is considered.\n",
    "        predictor: MultiLabelPipeline,\n",
    "        format: Literal['bert', 't5'] = 'bert', # Specifies how to format the input to `predictor`.\n",
    "        as_floats: bool = True, # If `True`, then return the predictions as floats indicating how likely it is that there should be a linking from the origin note to the relied note of each type.\n",
    "        threshold: float | dict[str, float] = 0.5, # Either a float value or a dictionary whose keys are the possible labels and whose values are floats. If a label is not one of the dictionary's key, then the default threshold value of 0.5 is used for that label. A float value exceeding this threshold corresponds to a prediction that a link of the given type should exist. This is only used if `as_floats` is `True`.\n",
    "        ) -> Union[dict[str, float], dict[str, bool]]: # A `dict` whose keys are the `labels` and whose values are either `float`s between 0.0 and 1.0 indicating how likely it is that there should be a linking from the origin note to the relied note of the type corresponding to the label. \n",
    "    r\"\"\"\n",
    "    Predict how likely/whether a note to should to another note for a specified reason.\n",
    "    \"\"\"\n",
    "    pair_data = NotePairData(origin_note=origin_data, relied_note=relied_data)\n",
    "    input_text = string_from_note_pair(pair_data, format)\n",
    "    preds: list[float] = predictor(input_text)[0]\n",
    "    id2label: dict[int, str] = predictor.model.config.id2label\n",
    "    output: Union[dict[str, float], dict[str, bool]] = {}\n",
    "    for id, label in id2label.items():\n",
    "        if as_floats:\n",
    "            output[label] = preds[id]\n",
    "        else:\n",
    "            if isinstance(threshold, float):\n",
    "                label_threshold = threshold\n",
    "            elif label in threshold:\n",
    "                label_threshold = threshold[label]\n",
    "            else:\n",
    "                label_threshold = 0.5\n",
    "            output[label] = preds[id] > label_threshold\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NO_LINK': False, 'INFO_TO_INFO_IN_CONTENT': False, 'INFO_TO_INFO_IN_SEE_ALSO': False, 'INFO_TO_INFO_VIA_NOTAT': False, 'INFO_TO_NOTAT_VIA_EMBEDDING': True, 'NOTAT_TO_INFO': False, 'NOTAT_TO_INFO_VIA_NOTAT': True, 'NOTAT_TO_NOTAT': False}\n",
      "{'NO_LINK': False, 'INFO_TO_INFO_IN_CONTENT': True, 'INFO_TO_INFO_IN_SEE_ALSO': False, 'INFO_TO_INFO_VIA_NOTAT': False, 'INFO_TO_NOTAT_VIA_EMBEDDING': False, 'NOTAT_TO_INFO': True, 'NOTAT_TO_INFO_VIA_NOTAT': True, 'NOTAT_TO_NOTAT': True}\n"
     ]
    }
   ],
   "source": [
    "with (mock_patch('__main__.string_from_note_pair') as mock_string_from_note_pair):\n",
    "    mock_origin_data = MagicMock()\n",
    "    mock_relied_data = MagicMock()\n",
    "    mock_predictor = MagicMock()\n",
    "    mock_predictor.model = MagicMock()\n",
    "    mock_predictor.model.config = MagicMock()\n",
    "    mock_predictor.model.config.id2label = {\n",
    "        0: 'NO_LINK',\n",
    "        1: 'INFO_TO_INFO_IN_CONTENT',\n",
    "        2: 'INFO_TO_INFO_IN_SEE_ALSO',\n",
    "        3: 'INFO_TO_INFO_VIA_NOTAT',\n",
    "        4: 'INFO_TO_NOTAT_VIA_EMBEDDING',\n",
    "        5: 'NOTAT_TO_INFO',\n",
    "        6: 'NOTAT_TO_INFO_VIA_NOTAT',\n",
    "        7: 'NOTAT_TO_NOTAT'}\n",
    "    \n",
    "    mock_predictor.return_value = [[0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6, 0.0]]\n",
    "    output = prediction_by_note_linking_model(\n",
    "        mock_origin_data, mock_relied_data, mock_predictor, as_floats=False, threshold=0.5)\n",
    "    print(output)\n",
    "    test_is(output['NO_LINK'], False)\n",
    "    test_is(output['INFO_TO_INFO_IN_CONTENT'], False)\n",
    "    test_is(output['INFO_TO_NOTAT_VIA_EMBEDDING'], True)\n",
    "    test_is(output['NOTAT_TO_INFO_VIA_NOTAT'], True)\n",
    "\n",
    "    mock_predictor.return_value = [[0.0, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]]\n",
    "    output = prediction_by_note_linking_model(\n",
    "        mock_origin_data, mock_relied_data, mock_predictor, as_floats=False, threshold={\n",
    "            'INFO_TO_INFO_VIA_NOTAT': 0.65,\n",
    "            'INFO_TO_INFO_IN_CONTENT': 0.10\n",
    "        })\n",
    "    print(output)\n",
    "    test_is(output['INFO_TO_INFO_VIA_NOTAT'], False)\n",
    "    test_is(output['INFO_TO_INFO_IN_CONTENT'], True)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def predict_note_linking(\n",
    "        origin_note: VaultNote, \n",
    "        relied_notes: VaultNote| list[VaultNote],\n",
    "        predictor: MultiLabelPipeline,\n",
    "        format: Literal['bert', 't5'] = 'bert', # Specifies how to format the input to `predictor`.\n",
    "        note_data: Optional[dict[str, NoteData]] = None, # For the purposes of predicting note linking, the note data only requires the positional data, so getting the note data via `note_data_from_index_note` should suffice (without having to use `find_reverse_links`, although `get_main_note_content_of_notat_note_data` should still be necessary).\n",
    "        omit_no_link_predictions: bool = True, # if `True` omit predictions of `NoteLinkEnum.NO_LINK`\n",
    "        threshold: float | dict[float]= 0.5, # See also `prediction_by_note_linking_model`. Either a float value or a dictionary whose keys are the possible labels and whose values are floats. If a label is not one of the dictionary's key, then the default threshold value of 0.5 is used for that label. A float value exceeding this threshold corresponds to a prediction that a link of the given type should exist. This is only used if `as_floats` is `True`.\n",
    "        ) -> dict[str, list[NoteLinkEnum]]: # The keys are the names of relied notes. The values are lists of `NoteLinkEnum` that specify the linking types from origin note to the relied note.\n",
    "    # TODO: add threshold parameter\n",
    "    if isinstance(relied_notes, VaultNote):\n",
    "        relied_notes: list[VaultNote] = [relied_notes]\n",
    "    if note_data and origin_note.name in note_data:\n",
    "        origin_note_data = note_data[origin_note.name]\n",
    "    else:\n",
    "        try:\n",
    "            origin_note_data = _note_data_from_vault_note_on_the_fly(\n",
    "                origin_note, reference='', note_data=note_data)\n",
    "        except Exception as e:\n",
    "            print(f\"An error ocurred while trying to get data for  `origin_note`: {origin_note}\")\n",
    "            print(e)\n",
    "            return\n",
    "    output_dict: dict[str, list[NoteLinkEnum]] = {}\n",
    "    for relied_note in relied_notes:\n",
    "        if relied_note.name == origin_note.name:\n",
    "            continue\n",
    "        if note_data and relied_note.name in note_data:\n",
    "            relied_note_data = note_data[relied_note.name]\n",
    "        else:\n",
    "            try:\n",
    "                relied_note_data = _note_data_from_vault_note_on_the_fly(\n",
    "                    relied_note, reference='', note_data=note_data)\n",
    "            except Exception as e:\n",
    "                print(f\"An error ocurred while trying to get data for  `relied_note`: {relied_note}\")\n",
    "                print(e)\n",
    "                continue\n",
    "        if relied_note_data == None:\n",
    "            print(relied_note)\n",
    "        preds: dict[str, bool] = prediction_by_note_linking_model(\n",
    "            origin_note_data, relied_note_data, predictor, format,\n",
    "            as_floats=False,\n",
    "            threshold=threshold)\n",
    "        output_dict[relied_note.name] = []\n",
    "        for enum_name, link_flag in preds.items():\n",
    "            if omit_no_link_predictions and enum_name == \"NO_LINK\":\n",
    "                continue\n",
    "            elif link_flag:\n",
    "                output_dict[relied_note.name].append(NoteLinkEnum[enum_name])\n",
    "    return output_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'relied_note_name': []}\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "with (mock_patch('__main__.prediction_by_note_linking_model') as mock_prediction_by_note_linking_model, \\\n",
    "          mock_patch('__main__._note_data_from_vault_note_on_the_fly') as mock_note_data_from_vault_note_on_the_fly):\n",
    "     mock_origin_note = MagicMock()\n",
    "     mock_relied_note = MagicMock()\n",
    "     mock_origin_note.name = 'origin_note_name'\n",
    "     mock_relied_note.name = 'relied_note_name'\n",
    "     relied_notes = [mock_relied_note]\n",
    "     mock_predictor = MagicMock()\n",
    "\n",
    "     mock_prediction_by_note_linking_model.return_value = {\n",
    "          'INFO_TO_INFO_IN_CONTENT': False,\n",
    "          'INFO_TO_INFO_IN_SEE_ALSO': False,\n",
    "          'INFO_TO_INFO_VIA_NOTAT': True,\n",
    "          'INFO_TO_NOTAT_VIA_EMBEDDING': False,\n",
    "          'NOTAT_TO_INFO': False,\n",
    "          'NOTAT_TO_INFO_VIA_NOTAT': False,\n",
    "          'NOTAT_TO_NOTAT': False,\n",
    "          'NO_LINK': False}\n",
    "     mock_note_data_from_vault_note_on_the_fly.side_effect = [MagicMock(), MagicMock()]\n",
    "     output = predict_note_linking(\n",
    "          mock_origin_note, relied_notes, mock_predictor, omit_no_link_predictions=True)\n",
    "     test_eq(\n",
    "          output,\n",
    "          {'relied_note_name': [NoteLinkEnum.INFO_TO_INFO_VIA_NOTAT]})\n",
    "\n",
    "\n",
    "     mock_prediction_by_note_linking_model.return_value = {\n",
    "          'INFO_TO_INFO_IN_CONTENT': False,\n",
    "          'INFO_TO_INFO_IN_SEE_ALSO': False,\n",
    "          'INFO_TO_INFO_VIA_NOTAT': False,\n",
    "          'INFO_TO_NOTAT_VIA_EMBEDDING': False,\n",
    "          'NOTAT_TO_INFO': False,\n",
    "          'NOTAT_TO_INFO_VIA_NOTAT': False,\n",
    "          'NOTAT_TO_NOTAT': False,\n",
    "          'NO_LINK': True}\n",
    "     mock_note_data_from_vault_note_on_the_fly.side_effect = [MagicMock(), MagicMock()]\n",
    "     output = predict_note_linking(\n",
    "          mock_origin_note, relied_notes, mock_predictor, omit_no_link_predictions=True)\n",
    "     test_eq(\n",
    "          output,\n",
    "          {'relied_note_name': []})\n",
    "     print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link cache note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the model will take a lot of time --- not only does each prediction take about a few seconds, but also the predictions need to be made on pairs of notes and hence the total time needed for predictions grows quadratically with the number of notes. As such, \"link cache notes\" will be made to record predictions.\n",
    "\n",
    "The link cache note will be saved in the root directory of its reference folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def link_cache_note(\n",
    "        vault: PathLike,\n",
    "        reference: str,\n",
    "        create_if_does_not_exist: bool = True,\n",
    "        ) -> VaultNote: # The `VaultNote` object representing the link cache note.\n",
    "    \"\"\"\n",
    "    Return a `VaultNote` object representing the link cache note in a reference of a vault.\n",
    "    \"\"\"\n",
    "    ind_note: VaultNote = index_note_for_reference(vault, reference, update_cache=True)\n",
    "    reference_folder: Path = ind_note.path(relative=True).parent\n",
    "    vn = VaultNote(vault, rel_path=reference_folder / f'_link_cache_{reference}.md')\n",
    "    if create_if_does_not_exist and not vn.exists():\n",
    "        vn.create()\n",
    "    return vn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The link cache note will be formatted as follows:\n",
    "\n",
    "```\n",
    "- [[origin_note_name_1]]\n",
    "    - [[relied_note_name_1]]: [<comma_separated_link_types_1>]\n",
    "    - [[relied_note_name_2]]: [<comma_separated_link_types_2>]\n",
    "    ...\n",
    "<blank space for separation>\n",
    "- [[origin_note_name_2]]\n",
    "    - ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def separate_blocks(\n",
    "        text: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Splits text into blocks separated by one or more blank lines.\n",
    "    Returns a list of blocks (strings) with whitespace stripped.\n",
    "    \"\"\"\n",
    "    blocks = []\n",
    "    current_block = []\n",
    "    \n",
    "    for line in text.splitlines():\n",
    "        if line.strip() == '':  # Blank line\n",
    "            if current_block:  # Only add if we have content\n",
    "                blocks.append('\\n'.join(current_block))\n",
    "                current_block = []\n",
    "        else:\n",
    "            current_block.append(line)\n",
    "    \n",
    "    # Add the last block if there's content remaining\n",
    "    if current_block:\n",
    "        blocks.append('\\n'.join(current_block))\n",
    "    \n",
    "    return blocks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['First line\\nSecond line', 'Third block starts here\\nWith multiple lines', 'Final block']\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"First line\n",
    "Second line\n",
    "\n",
    "Third block starts here\n",
    "With multiple lines\n",
    "\n",
    "Final block\"\"\"\n",
    "\n",
    "blocks = separate_blocks(text)\n",
    "print(blocks)\n",
    "# for i, block in enumerate(blocks, 1):\n",
    "#     print(f\"Block {i}:\\n{block}\\n{'-'*20}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def parse_link_cache_note(\n",
    "        link_cache_note: VaultNote,\n",
    "        ) -> dict[str, dict[str, list[NoteLinkEnum]]]: # The first key is the name of an \"origin note\". The second key is the name of a \"relied note\" with respect to the origin note. The value is a list of the link types from the origin note to the relied note.\n",
    "    \"\"\"\n",
    "    See also `write_link_cache_note`, which is essentially the opposite of this function.\n",
    "    \"\"\"\n",
    "    text = link_cache_note.text()\n",
    "    blocks = separate_blocks(text)\n",
    "    link_types: dict[str, dict[str, list[NoteLinkEnum]]] = {}\n",
    "    for block in blocks:\n",
    "        lines: list[str] = block.splitlines()\n",
    "        first_line_link: ObsidianLink = links_from_text(lines[0])[0]\n",
    "        origin_note_name = first_line_link.file_name\n",
    "        link_types[origin_note_name] = {}\n",
    "        for line in lines[1:]:\n",
    "            link: ObsidianLink = links_from_text(line)[0]\n",
    "            relied_note_name = link.file_name\n",
    "            ind = line.index(':')\n",
    "            note_type_list = ast.literal_eval(line[ind+2:])\n",
    "            link_types[origin_note_name][relied_note_name] = [\n",
    "                NoteLinkEnum[note_type_str] for note_type_str in note_type_list]\n",
    "    return link_types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mock_patch('__main__.VaultNote') as mock_vault_note:\n",
    "    mock_link_cache_note = mock_vault_note.return_value\n",
    "    mock_link_cache_note.text.return_value = '''\n",
    "- [[origin_note_1]]\n",
    "    - [[relied_note_1]]: ['INFO_TO_INFO_IN_CONTENT', 'INFO_TO_INFO_VIA_NOTAT']\n",
    "    - [[relied_note_2]]: ['INFO_TO_NOTAT_VIA_EMBEDDING']\n",
    "\n",
    "- [[origin_note_2]]\n",
    "    - [[relied_note_3]]: ['NOTAT_TO_NOTAT']\n",
    "    - [[relied_note_4]]: ['NOTAT_TO_INFO', 'NOTAT_TO_INFO_VIA_NOTAT']\n",
    "'''\n",
    "    parse_link_cache_note(mock_link_cache_note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def write_link_cache_note(\n",
    "        link_types: dict[str, dict[str, list[NoteLinkEnum]]],\n",
    "        cache_note: VaultNote,\n",
    "        ) -> None:\n",
    "    \"\"\"\n",
    "    Overwrite the contents of the note represented by `link_cache_note` using the data\n",
    "    from `link_types`.\n",
    "\n",
    "    `link_cache_notes` is assumed to exist.\n",
    "\n",
    "    See also `parse_link_cache_note`, which is essentially the opposite of this function.\n",
    "    \"\"\"\n",
    "    chunks: list[str] = []\n",
    "    for origin_note_name, relied_dict in link_types.items():\n",
    "        chunk_text = f\"- [[{origin_note_name}]]\\n\"\n",
    "        for relied_note_name, link_type_list in relied_dict.items():\n",
    "            chunk_text = f'{chunk_text}    - [[{relied_note_name}]]: {str([link_type.name for link_type in link_type_list])}\\n'\n",
    "        chunks.append(chunk_text)\n",
    "    cache_note.write('\\n\\n'.join(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- [[origin_note_1]]\n",
      "    - [[relied_note_1]]: ['INFO_TO_INFO_IN_CONTENT', 'INFO_TO_INFO_VIA_NOTAT']\n",
      "    - [[relied_note_2]]: ['INFO_TO_NOTAT_VIA_EMBEDDING']\n",
      "\n",
      "\n",
      "- [[origin_note_2]]\n",
      "    - [[relied_note_3]]: ['NOTAT_TO_NOTAT']\n",
      "    - [[relied_note_4]]: ['NOTAT_TO_INFO', 'NOTAT_TO_INFO_VIA_NOTAT']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with mock_patch('__main__.VaultNote') as mock_vault_note:\n",
    "    mock_link_cache_note = mock_vault_note.return_value\n",
    "    link_types = {\n",
    "        'origin_note_1': {\n",
    "            'relied_note_1': [\n",
    "                NoteLinkEnum.INFO_TO_INFO_IN_CONTENT, NoteLinkEnum.INFO_TO_INFO_VIA_NOTAT],\n",
    "            'relied_note_2': [\n",
    "                NoteLinkEnum.INFO_TO_NOTAT_VIA_EMBEDDING]},\n",
    "        'origin_note_2': {\n",
    "            'relied_note_3': [\n",
    "                NoteLinkEnum.NOTAT_TO_NOTAT],\n",
    "            'relied_note_4': [\n",
    "                NoteLinkEnum.NOTAT_TO_INFO, NoteLinkEnum.NOTAT_TO_INFO_VIA_NOTAT] }\n",
    "    }\n",
    "    write_link_cache_note(link_types, mock_link_cache_note)\n",
    "    args, _ = mock_link_cache_note.write.call_args\n",
    "    written_content = args[0]\n",
    "    print(written_content)\n",
    "    test_eq(\n",
    "        written_content,\n",
    "        '''- [[origin_note_1]]\n",
    "    - [[relied_note_1]]: ['INFO_TO_INFO_IN_CONTENT', 'INFO_TO_INFO_VIA_NOTAT']\n",
    "    - [[relied_note_2]]: ['INFO_TO_NOTAT_VIA_EMBEDDING']\n",
    "\n",
    "\n",
    "- [[origin_note_2]]\n",
    "    - [[relied_note_3]]: ['NOTAT_TO_NOTAT']\n",
    "    - [[relied_note_4]]: ['NOTAT_TO_INFO', 'NOTAT_TO_INFO_VIA_NOTAT']\n",
    "'''\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def consolidate_note_linking_predictions_into_cache(\n",
    "        origin_note: VaultNote | str,\n",
    "        predictions: dict[str, list[NoteLinkEnum]], # An output of `predict_note_linking``\n",
    "        cache: dict[str, dict[str, list[NoteLinkEnum]]], # See `parse_link_cache_note`. The first key is the name of an \"origin note\". The second key is the name of a \"relied note\" with respect to the origin note. The value is a list of the link types from the origin note to the relied note.\n",
    "        ):\n",
    "    \"\"\"\n",
    "    Consolidate the outputs of `predict_note_linking` into a link cache.\n",
    "    \"\"\"\n",
    "    if isinstance(origin_note, VaultNote):\n",
    "        origin_note = origin_note.name\n",
    "    if origin_note not in cache:\n",
    "        cache[origin_note] = {}\n",
    "    for relied_note_name, predicted_link_enums in predictions.items():\n",
    "        if origin_note == relied_note_name:\n",
    "            continue \n",
    "        predicted_link_enums = set(predicted_link_enums)\n",
    "        predicted_link_enums = predicted_link_enums - {NoteLinkEnum.NO_LINK}\n",
    "        if relied_note_name not in cache[origin_note]:\n",
    "            cache[origin_note][relied_note_name] = []\n",
    "        cached_link_enums = set(cache[origin_note][relied_note_name])\n",
    "        link_enums = cached_link_enums | predicted_link_enums\n",
    "        cache[origin_note][relied_note_name] = list(link_enums)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'origin_note_name': {'relied_note_name_1': [<NoteLinkEnum.INFO_TO_INFO_IN_SEE_ALSO: 2>, <NoteLinkEnum.INFO_TO_INFO_IN_CONTENT: 1>, <NoteLinkEnum.INFO_TO_INFO_VIA_NOTAT: 3>], 'relied_note_name_2': [], 'relied_note_name_3': [<NoteLinkEnum.INFO_TO_NOTAT_VIA_EMBEDDING: 4>]}}\n"
     ]
    }
   ],
   "source": [
    "predictions = {\n",
    "    'relied_note_name_1': [NoteLinkEnum.INFO_TO_INFO_IN_CONTENT, NoteLinkEnum.INFO_TO_INFO_VIA_NOTAT],\n",
    "    'relied_note_name_2': [],\n",
    "    'relied_note_name_3': [NoteLinkEnum.INFO_TO_NOTAT_VIA_EMBEDDING]}\n",
    "cache = {'origin_note_name': {'relied_note_name_1': [NoteLinkEnum.INFO_TO_INFO_IN_CONTENT, NoteLinkEnum.INFO_TO_INFO_IN_SEE_ALSO]}}\n",
    "\n",
    "consolidate_note_linking_predictions_into_cache('origin_note_name', predictions, cache)\n",
    "\n",
    "print(cache)\n",
    "test_eq(\n",
    "    set(cache['origin_note_name']['relied_note_name_1']), \n",
    "    set([NoteLinkEnum.INFO_TO_INFO_IN_CONTENT, NoteLinkEnum.INFO_TO_INFO_VIA_NOTAT, NoteLinkEnum.INFO_TO_INFO_IN_SEE_ALSO]))\n",
    "\n",
    "test_eq(\n",
    "    set(cache['origin_note_name']['relied_note_name_2']), \n",
    "    set([]))\n",
    "\n",
    "test_eq(\n",
    "    set(cache['origin_note_name']['relied_note_name_3']), \n",
    "    set([NoteLinkEnum.INFO_TO_NOTAT_VIA_EMBEDDING]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def consolidate_caches(\n",
    "        cache_1: dict[str, dict[str, list[NoteLinkEnum]]], # See `parse_link_cache_note`. The first key is the name of an \"origin note\". The second key is the name of a \"relied note\" with respect to the origin note. The value is a list of the link types from the origin note to the relied note.\n",
    "        cache_2: dict[str, dict[str, list[NoteLinkEnum]]],\n",
    "        ) -> dict[str, dict[str, list[NoteLinkEnum]]]:\n",
    "    new_cache: dict[str, dict[str, list[NoteLinkEnum]]] = copy.deepcopy(cache_1)\n",
    "    for origin_note_name, origin_note_dict in cache_2.items():\n",
    "        consolidate_note_linking_predictions_into_cache(\n",
    "            origin_note_name, origin_note_dict, new_cache)\n",
    "    return new_cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<NoteLinkEnum.INFO_TO_INFO_IN_SEE_ALSO: 2>, <NoteLinkEnum.INFO_TO_INFO_IN_CONTENT: 1>]\n"
     ]
    }
   ],
   "source": [
    "# Create two caches with some overlapping data\n",
    "cache_a = {\n",
    "    'Note_A': {'Note_B': [NoteLinkEnum.INFO_TO_INFO_IN_CONTENT]}\n",
    "}\n",
    "cache_b = {\n",
    "    'Note_A': {'Note_B': [NoteLinkEnum.INFO_TO_INFO_IN_SEE_ALSO]},\n",
    "    'Note_C': {'Note_D': [NoteLinkEnum.NOTAT_TO_INFO]}\n",
    "}\n",
    "\n",
    "# Consolidate them\n",
    "merged = consolidate_caches(cache_a, cache_b)\n",
    "\n",
    "# Verify the merge happened\n",
    "print(merged['Note_A']['Note_B']) \n",
    "# Output: [<NoteLinkEnum...CONTENT>, <NoteLinkEnum...SEE_ALSO>]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import *\n",
    "\n",
    "# --- Basic Edge Cases ---\n",
    "# Empty Inputs\n",
    "test_eq(consolidate_caches({}, {}), {})\n",
    "\n",
    "# Idempotency (Duplicates)\n",
    "dup_cache = {'A': {'B': [NoteLinkEnum.INFO_TO_INFO_IN_CONTENT]}}\n",
    "res = consolidate_caches(dup_cache, dup_cache)\n",
    "test_eq(len(res['A']['B']), 1)\n",
    "\n",
    "# --- Complex Scenario Tests ---\n",
    "# Base setup for the main test scenario\n",
    "cache_1 = {\n",
    "    'origin_note_1': {\n",
    "        'relied_note_1': [NoteLinkEnum.INFO_TO_INFO_IN_CONTENT],\n",
    "    },\n",
    "    'origin_note_2': { # Only exists in cache_1\n",
    "        'relied_note_1': [NoteLinkEnum.INFO_TO_INFO_IN_CONTENT],\n",
    "    },\n",
    "    'origin_note_3': { # Exists in both, but relied_note_2 is unique to cache_1\n",
    "        'relied_note_2': [NoteLinkEnum.NOTAT_TO_INFO],\n",
    "    }\n",
    "}\n",
    "cache_2 = {\n",
    "    'origin_note_1': { # Exists in both, relied_note_1 exists in both\n",
    "        'relied_note_1': [NoteLinkEnum.INFO_TO_INFO_IN_SEE_ALSO]\n",
    "    },\n",
    "    'origin_note_3': { # Exists in both, but relied_note_1 is unique to cache_2\n",
    "        'relied_note_1': [NoteLinkEnum.NOTAT_TO_INFO_VIA_NOTAT]\n",
    "    }\n",
    "}\n",
    "new_cache = consolidate_caches(cache_1, cache_2)\n",
    "\n",
    "# Case 1: Deep Merge (Union of Lists)\n",
    "test_eq(\n",
    "    set(new_cache['origin_note_1']['relied_note_1']), \n",
    "    {NoteLinkEnum.INFO_TO_INFO_IN_SEE_ALSO, NoteLinkEnum.INFO_TO_INFO_IN_CONTENT}\n",
    ")\n",
    "\n",
    "# Case 2: Preservation of Left-Only Data\n",
    "test_eq(\n",
    "    new_cache['origin_note_2']['relied_note_1'], [NoteLinkEnum.INFO_TO_INFO_IN_CONTENT]\n",
    ")\n",
    "\n",
    "# Case 3: Partial Merge (Left Unique Key in Shared Parent)\n",
    "test_eq(\n",
    "    new_cache['origin_note_3']['relied_note_2'], [NoteLinkEnum.NOTAT_TO_INFO]\n",
    ")\n",
    "\n",
    "# Case 4: Partial Merge (Right Unique Key in Shared Parent)\n",
    "test_eq(\n",
    "    new_cache['origin_note_3']['relied_note_1'], [NoteLinkEnum.NOTAT_TO_INFO_VIA_NOTAT]\n",
    ")\n",
    "\n",
    "# Case 6: Completely New Origin Key (Right-Only Top Level)\n",
    "cache_new_origin = {'Z': {'Y': [NoteLinkEnum.NOTAT_TO_INFO]}}\n",
    "res_new = consolidate_caches(cache_1, cache_new_origin)\n",
    "test_eq(res_new['Z']['Y'], [NoteLinkEnum.NOTAT_TO_INFO])\n",
    "test_eq(len(res_new), 4) # origin_1, origin_2, origin_3 + Z\n",
    "\n",
    "# Case 7: Empty Inputs (Identity)\n",
    "test_eq(consolidate_caches(cache_1, {}), cache_1)\n",
    "test_eq(consolidate_caches({}, cache_2), cache_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def remove_blank_or_no_link_data_from_cache(\n",
    "        cache: dict[str, dict[str, list[NoteLinkEnum]]], # See `parse_link_cache_note`. The first key is the name of an \"origin note\". The second key is the name of a \"relied note\" with respect to the origin note. The value is a list of the link types from the origin note to the relied note.\n",
    "        ) -> dict[str, dict[str, list[NoteLinkEnum]]]: # A new cache, with lists that are either blank or which only contain `NoteLinkEnum.NO_LINK` are removed and with blank dict values are also removed..\n",
    "    new_cache: dict[str, dict[str, list[NoteLinkEnum]]] = {} \n",
    "    for origin_note_name, origin_dict in cache.items():\n",
    "        cleaned_dict: dict[str, list[NoteLinkEnum]] = {}\n",
    "        for relied_note_name, listy in origin_dict.items():\n",
    "            if not listy or (len(set(listy)) == 1 and listy[0] == NoteLinkEnum.NO_LINK):\n",
    "                continue\n",
    "            cleaned_dict[relied_note_name] = listy\n",
    "        if cleaned_dict:\n",
    "            new_cache[origin_note_name] = cleaned_dict\n",
    "    return new_cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = {\n",
    "    'origin_note_1': {},\n",
    "    'origin_note_2': {\n",
    "        'relied_note_1': [],\n",
    "        'relied_note_2': [NoteLinkEnum.NO_LINK] \n",
    "    },\n",
    "    'origin_note_3': {\n",
    "        'relied_note_1': [NoteLinkEnum.INFO_TO_INFO_IN_CONTENT, NoteLinkEnum.INFO_TO_INFO_IN_SEE_ALSO]\n",
    "    }}\n",
    "output = remove_blank_or_no_link_data_from_cache(cache)\n",
    "test_eq(\n",
    "    output, \n",
    "    {'origin_note_3':\n",
    "     {'relied_note_1':\n",
    "      [NoteLinkEnum.INFO_TO_INFO_IN_CONTENT, NoteLinkEnum.INFO_TO_INFO_IN_SEE_ALSO]}}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def remove_nonexistent_note_names_from_cache(\n",
    "        cache: dict[str, dict[str, list[NoteLinkEnum]]], # See `parse_link_cache_note`. The first key is the name of an \"origin note\". The second key is the name of a \"relied note\" with respect to the origin note. The value is a list of the link types from the origin note to the relied note.\n",
    "        vault: PathLike\n",
    "        ) -> dict[str, dict[str, list[NoteLinkEnum]]]:\n",
    "    \"\"\"\n",
    "    Remove names of nonexistent notes in `cache`.\n",
    "    \"\"\"\n",
    "    cache_copy = copy.deepcopy(cache)\n",
    "    keys = cache_copy.keys()\n",
    "    for origin_note_name in list(keys):\n",
    "        origin_note = VaultNote(vault, name=origin_note_name)\n",
    "        if not origin_note.exists():\n",
    "            cache_copy.pop(origin_note_name)\n",
    "    for origin_note_name, origin_dict in cache_copy.items():\n",
    "        keys = origin_dict.keys()\n",
    "        for relied_note_name in list(keys):\n",
    "            relied_note = VaultNote(vault, name=relied_note_name)\n",
    "            if not relied_note.exists():\n",
    "                origin_dict.pop(relied_note_name)\n",
    "    return cache_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sieve note pairs to predict on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the number of pairs of notes grows quadratically in the number of notes, it takes too much time to make predictions one-by-one. It should be useful to prioritize certain pairs over others.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def sieve_potential_relied_notes(\n",
    "        vault: PathLike,\n",
    "        reference: str,\n",
    "        origin_note: VaultNote, # an info note\n",
    "        note_data: dict[str, NoteData],\n",
    "        # potential_relied_notes: list[VaultNote],\n",
    "        appendix_notes: list[VaultNote], # notes whose index notes are appendix notes\n",
    "        cache: dict[str, dict[str, list[NoteLinkEnum]]],\n",
    "        notation_similarity_threshold: float = 0.8, # The threshold that the similarity metric of a notion must exceed for the name of a notation note to be included in the output.\n",
    "        skip_already_made_predictions: bool = True,\n",
    "        ) -> set[str]: # Names of potential relied notes that may be good to predict note linking from `origin_note` for.`\n",
    "    if origin_note.name not in note_data:\n",
    "        print(f'`origin_note` was not in `note_data`. Perhaps a `origin_note` has been renamed at some point and it may be necessary to reload `note_data`. `origin_note`: {origin_note}.')\n",
    "        return set()\n",
    "    index_note: VaultNote = index_note_for_reference(vault, reference, update_cache=True)\n",
    "    info_notes: list[VaultNote] = notes_linked_in_notes_linked_in_note(index_note, as_dict=False)\n",
    "    appendix_note_names: set[str] = set([appendix_note.name for appendix_note in appendix_notes])\n",
    "\n",
    "    relied_note_names = set()\n",
    "\n",
    "    # Add an info not if it \n",
    "    # 1. is in the appendix or precedes `origin_note`, is a definition/notation note\n",
    "    # 2. is in the same section and precedes `origin_note` and is a context note.\n",
    "    # TODO: Automatically add a def/notat note if it precedes `origin_note` in a section by a little.\n",
    "    # Add a notation note if it \n",
    "    # 1. looks similar to a substr in a latex str in the origin_note.\n",
    "    for info_note in info_notes + appendix_notes:\n",
    "        if not info_note.exists():\n",
    "            continue\n",
    "        if info_note.name not in note_data:\n",
    "            # If this happens, it may be the case that `info_note` has been\n",
    "            # renamed, but this has not been reflected in `note_data`.\n",
    "            continue\n",
    "        # Ignore `info_note` if it was already predicted on or it precedes `origin_note` and is not an appendix note. \n",
    "        if (skip_already_made_predictions\n",
    "                and origin_note.name in cache\n",
    "                and info_note.name in cache[origin_note.name]):\n",
    "            continue\n",
    "        if (note_data_order_cmp(note_data[origin_note.name], note_data[info_note.name]) <= 0\n",
    "                and info_note.name not in appendix_note_names):\n",
    "            continue\n",
    "\n",
    "        mf = MarkdownFile.from_vault_note(info_note)\n",
    "        # ignore non-definition/notation notes.\n",
    "        if not (mf.has_tag('_auto/_meta/definition') or mf.has_tag('_auto/_meta/notation') or mf.has_tag('_meta/definition') or mf.has_tag('_meta/notation')):\n",
    "            continue\n",
    "        # admit context notes in the same section as `origin_note` that also precede `origin_note`.\n",
    "        elif (mf.has_tag(\"_auto/_meta/context\") or mf.has_tag('_meta/context')\n",
    "                and note_data_order_cmp(note_data[origin_note.name], note_data[info_note.name]) >= 0\n",
    "                and note_data[origin_note.name].section_num == note_data[info_note.name].section_num):\n",
    "            relied_note_names.add(info_note.name)\n",
    "            continue\n",
    "        relied_note_names.add(info_note.name)\n",
    "        # For each info note with notations, try to see if the notations resemble notations used in `origin_note`.\n",
    "        notat_notes: list[VaultNote] = notation_notes_linked_in_see_also_section(\n",
    "            info_note, vault, as_vault_notes=True)\n",
    "        if skip_already_made_predictions:\n",
    "            notat_notes = [\n",
    "                notat_note for notat_note in notat_notes\n",
    "                if not (origin_note.name in cache and notat_note.name in cache[origin_note.name])]\n",
    "        notat_note_candidates: list[VaultNote] = []\n",
    "        for notat_note in notat_notes:\n",
    "            individual_notat_note: list[VaultNote] = similar_notat_notes_in_note(\n",
    "                origin_note, notat_note, threshold=notation_similarity_threshold)\n",
    "            if not individual_notat_note:\n",
    "                continue\n",
    "            relied_note_names.add(info_note.name)\n",
    "            relied_note_names.add(notat_note.name)\n",
    "\n",
    "        # For each info note with definitions, try to see if the definitions resemble phrases used in `origin_note`. \n",
    "\n",
    "\n",
    "    \n",
    "    # # 2. find all context notes in the same section as origin_note\n",
    "    # origin_index_note = index_note_of_note(origin_note)\n",
    "    # section_notes: dict[str, VaultNote] = notes_linked_in_note(\n",
    "    #     origin_index_note, as_dict=True)\n",
    "    # relied_note_names.update(section_notes.keys())\n",
    "\n",
    "    return relied_note_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _predict_one_direction_and_consolidate_cache(\n",
    "        origin_note: VaultNote,\n",
    "        relied_note: VaultNote,\n",
    "        cache: dict[str, dict[str, list[NoteLinkEnum]]],\n",
    "        predictor: MultiLabelPipeline, \n",
    "        format: Literal['bert', 't5'],\n",
    "        note_data: dict[str, NoteData] | None,\n",
    "        skip_already_made_predictions: bool,\n",
    "        threshold: float | dict[str, float],\n",
    "        ) -> None:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if (skip_already_made_predictions\n",
    "            and origin_note.name in cache and relied_note.name in cache[origin_note.name]):\n",
    "        return\n",
    "    outputs: dict[str, list[NoteLinkEnum]] = predict_note_linking(\n",
    "        origin_note, relied_note, predictor, format, note_data, threshold=threshold)\n",
    "    consolidate_note_linking_predictions_into_cache(origin_note, outputs, cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def predict_on_relied_note_and_related_notat_notes(\n",
    "        origin_note: VaultNote,\n",
    "        relied_note: VaultNote,\n",
    "        cache: dict[str, dict[str, list[NoteLinkEnum]]], # The current cache of predictions, see `parse_link_cache_note` for example; this is used to skip predictions that have already been made. Moreover, the cache is updated based on the predictions made. \n",
    "        predictor: MultiLabelPipeline,\n",
    "        format: Literal['bert', 't5'] = 'bert',\n",
    "        note_data: Optional[dict[str, NoteData]] = None,\n",
    "        skip_already_made_predictions: bool = True,\n",
    "        predict_reverse_too: bool = False,\n",
    "        threshold: float | dict[str, float] = 0.5,\n",
    "        ) -> None:\n",
    "    \"\"\"\n",
    "    Update `cache` by making predictions from `origin_note` to `relied_note` (and vice versa).\n",
    "    Moreover, \n",
    "    \"\"\"\n",
    "    # predict `origin_note` to `relied_note``\n",
    "\n",
    "    _predict_one_direction_and_consolidate_cache(\n",
    "        origin_note, relied_note, cache, predictor,\n",
    "        format, note_data, skip_already_made_predictions, threshold)\n",
    "    if predict_reverse_too:\n",
    "        _predict_one_direction_and_consolidate_cache(\n",
    "            relied_note, origin_note, cache, predictor, format, note_data,\n",
    "            skip_already_made_predictions, threshold)\n",
    "\n",
    "    # For each relied note that is 1. an info note, 2. got predicted to be a relied note via info_to_info_via_notat, and 3. has a notation, predict whether the relevant notation notes ought to be linked.\n",
    "    if not origin_note.name in cache:\n",
    "        return\n",
    "    relied_note_names: list[str] = list(cache[origin_note.name])\n",
    "    for relied_note_name in relied_note_names:\n",
    "        relied_note_link_types = cache[origin_note.name][relied_note_name]\n",
    "        if not relied_note_link_types:\n",
    "            continue\n",
    "        if not NoteLinkEnum.INFO_TO_INFO_VIA_NOTAT in relied_note_link_types:\n",
    "            continue\n",
    "        relied_note = VaultNote(origin_note.vault, name=relied_note_name)\n",
    "        notat_notes: list[VaultNote] = notation_notes_linked_in_see_also_section(\n",
    "            relied_note, origin_note.vault, as_vault_notes=True)\n",
    "        for notat_note in notat_notes:\n",
    "            _predict_one_direction_and_consolidate_cache(\n",
    "                origin_note, notat_note, cache, predictor, format, note_data,\n",
    "                skip_already_made_predictions, threshold)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify notation notes that should be embedded as footnotes in information notes or linked in other notation notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def similar_notat_notes_in_note(\n",
    "        origin_note: VaultNote, # Either an info or a notat note\n",
    "        notation_notes: VaultNote | list[VaultNote], # The notation notes that are considered to be \n",
    "        threshold: float = 0.8, \n",
    "        ) -> list[VaultNote]: # The notation notes whose notations are determined to be similar to notations used in `origin_note`.\n",
    "    \"\"\"\n",
    "    Determine which notation notes introduce notations which resemble notations used\n",
    "    in `origin_note`.\n",
    "\n",
    "    This is a fuzzy function purely based on the str value of the notation and the text of `origin_note` and does not use ML predictions. \n",
    "    \"\"\"\n",
    "    if isinstance(notation_notes, VaultNote):\n",
    "        notation_notes = [notation_notes]\n",
    "\n",
    "    text = origin_note.text()\n",
    "    indices = latex_indices(text)\n",
    "    latex_texts_in_origin_note: list[str] = []\n",
    "    for start, end in indices:\n",
    "        latex_text = text[start:end]\n",
    "        latex_text = latex_text.strip('$ ')\n",
    "        latex_texts_in_origin_note.append(latex_text)\n",
    "    matching_notat_notes: list[VaultNote] = []\n",
    "    for notation_note in notation_notes:\n",
    "        if notation_note.name == origin_note.name:\n",
    "            continue\n",
    "        notation: str = notation_in_note(notation_note, include_dollar_signs=False)\n",
    "        for latex_text in latex_texts_in_origin_note:\n",
    "            if latex_str_is_likely_in_latex_str(notation, latex_text, threshold=threshold):\n",
    "                matching_notat_notes.append(notation_note)\n",
    "                break\n",
    "    return matching_notat_notes\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| TODO: test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def locate_footnote_embedded_notation_link(\n",
    "        origin_note: VaultNote, # An info note \n",
    "        notation_note: VaultNote, # The notation notes that are considered to be \n",
    "        locate_by: Literal['first', 'best'] = 'best', # If `'first'`, then the first latex string for which the `latex_str_in_latex_str_fuzz_metric` score exceed threshold is used as the location. If `'best'` or if no such latex string exists, then the latex string giving the greatest score is used as the location.\n",
    "        threshold: float = 0.8,\n",
    "        ) -> int | None: # The index in `origin_note.text()` at which the footnote to an embedded link to `notation_note` should be added. If the main note of `notation_note` is `origin_note`, then `None`.\n",
    "    \"\"\"\n",
    "    Determine where in `origin_note` a footnote to an embedded link to `notation_note` should be added.\n",
    "\n",
    "    Such a location would be at the end of the closing of a latex string in the text. \n",
    "\n",
    "    This is a fuzzy function purely based on the str value of the notation and the text of `origin_note` and does not use ML predictions. \n",
    "    \"\"\"\n",
    "    main_note = main_of_notation(notation_note, as_note=False)\n",
    "    if main_note and origin_note.name == main_note:\n",
    "        return None\n",
    "    notation: str = notation_in_note(notation_note, include_dollar_signs=False)\n",
    "    text = origin_note.text()\n",
    "    indices = latex_indices(text)\n",
    "    scores: dict[int, float] = {} # Keys are end indices and values are scores of how likely it seems that the latex str seems to use the notation.\n",
    "    for start, end in indices:\n",
    "        latex_text = text[start:end]\n",
    "        latex_text = latex_text.strip('$ ')\n",
    "        score: float = latex_str_in_latex_str_fuzz_metric(notation, latex_text)\n",
    "        if score > threshold and locate_by == 'first':\n",
    "            return end\n",
    "        else:\n",
    "            scores[end] = score\n",
    "    max_key = max(scores, key=scores.get)\n",
    "    return max_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222\n",
      "For each integer $m$ and each transitive $G \\leq S_m$, there are constants $C(G), Q(G)$, and $e(G)$ such that, for all $q>Q(G)$ coprime to $\\#G$ and all $X>0$, \n",
      "\n",
      "$$N_G(\\mathbb{F}_q(t),X) \\leq C(G) X^{a(G)} \\log(X)^{e(G)}$$\n"
     ]
    }
   ],
   "source": [
    "mock_origin_note = MagicMock()\n",
    "mock_notation_note = MagicMock()\n",
    "mock_origin_note.text.return_value = r\"\"\"For each integer $m$ and each transitive $G \\leq S_m$, there are constants $C(G), Q(G)$, and $e(G)$ such that, for all $q>Q(G)$ coprime to $\\#G$ and all $X>0$, \n",
    "\n",
    "$$N_G(\\mathbb{F}_q(t),X) \\leq C(G) X^{a(G)} \\log(X)^{e(G)}$$\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "mock_notation_note.text.return_value = r\"\"\"---\n",
    "detect_regex: \n",
    "latex_in_original: [\"a(G)\"]\n",
    "tags: [_meta/notation_note_named]\n",
    "---\n",
    "$a(G)$ [[ellenberg_tran_westerland_fnfcqsamcff_1. Introduction_ellenberg_tran_westerland_fnfcqsamcff|denotes]] $[\\min_{G \\setminus \\{1 \\}} ind(g)]^{-1}$ where $G$ is a transitive subgroup of $S_m$ and \n",
    "\n",
    "![[ellenberg_tran_westerland_fnfcqsamcff_1. Introduction_ellenberg_tran_westerland_fnfcqsamcff#^38959b]]\n",
    "\n",
    "for $g \\in S_m$.\n",
    "\n",
    "For instance, if $G = S_m$, the minimal index is $1$, realized by transpositions, and so $a(S_m) = 1$.\n",
    "- [$ind(g)$](ellenberg_tran_westerland_fnfcqsamcff_notation_ind_g_index_of_element_of_S_m.md)\"\"\"\n",
    "\n",
    "with (mock_patch('__main__.latex_str_in_latex_str_fuzz_metric')\n",
    "        as mock_latex_str_in_latex_str_fuzz_metric,\n",
    "        mock_patch('__main__.main_of_notation') as mock_main_of_notation,\n",
    "        mock_patch('__main__.notation_in_note') as mock_notation_in_note,\n",
    "        ):\n",
    "    mock_latex_str_in_latex_str_fuzz_metric.side_effect = [0, 0, 0, 0, 0, 0, 0, 1]\n",
    "    mock_main_note = MagicMock()\n",
    "    mock_main_of_notation.return_value = mock_main_note\n",
    "    mock_notation_in_note.return_value = '$a(G)$'\n",
    "    output = locate_footnote_embedded_notation_link(\n",
    "        mock_origin_note, mock_notation_note, locate_by='best', threshold=0.8)\n",
    "    print(output)\n",
    "    print(mock_origin_note.text.return_value[:output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _where_to_add_notation_links(\n",
    "        origin_note: VaultNote,\n",
    "        relied_notes: list[VaultNote],\n",
    "        locate_by: Literal['first', 'best'] = 'best',\n",
    "        threshold: float = 0.8,\n",
    "        ) -> dict[int, list[VaultNote]]:\n",
    "    \"\"\"\n",
    "    Helper function to `add_notation_note_embedded_footnotes_to_info_note`.\n",
    "    \"\"\"\n",
    "    where_to_add: dict[int, list[VaultNote]] = {} # Keys are end indices of latex str in `origin_note` and values are lists of VaultNote objects representing notation notes for which the embedded links should be added.\n",
    "    for relied_note in relied_notes:\n",
    "        location: int | None = locate_footnote_embedded_notation_link(\n",
    "            origin_note, relied_note, locate_by, threshold)\n",
    "        if location is None:\n",
    "            continue\n",
    "        if not location in where_to_add:\n",
    "            where_to_add[location] = []\n",
    "        where_to_add[location].append(relied_note)\n",
    "    return where_to_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _add_notation_note_embedded_footnotes(\n",
    "        text: str,\n",
    "        where_to_add: dict[int, list[VaultNote]],\n",
    "        ) -> str:\n",
    "    \"\"\"\n",
    "    Helper function to `add_notation_note_embedded_footnotes_to_info_note`.\n",
    "    \"\"\"\n",
    "    reverse_sorted_locations: list[int] = sorted(where_to_add.keys(), reverse=True)\n",
    "    # iterate in reverse to make sure that the modifications made along the way do not change\n",
    "    # the indices of the locations.\n",
    "    for location in reverse_sorted_locations:\n",
    "        notation_notes_to_link = where_to_add[location]\n",
    "        available_footnote_numbers: list[int] = identify_available_footnote_numbers(\n",
    "            text, count=len(notation_notes_to_link))\n",
    "        footnote_text = ''.join([f'[^{num}]' for num in available_footnote_numbers])\n",
    "        footnote_mentions = '\\n'.join(\n",
    "            [f'[^{num}]: ![[{notat_note.name}]]'\n",
    "             for num, notat_note in zip(available_footnote_numbers, notation_notes_to_link)])\n",
    "        new_line_index = text.find('\\n', location+1)\n",
    "        if new_line_index == -1:\n",
    "            new_line_index = len(text)\n",
    "        pieces = [text[0:location], text[location:new_line_index], text[new_line_index:]]\n",
    "\n",
    "        if location > 1 and text[location-2] == '$': # latex str ends with '$$'\n",
    "            # pieces.append(f'\\n\\n{footnote_text}\\n\\n{footnote_mentions}\\n\\n')\n",
    "            pieces.insert(2, f'\\n\\n{footnote_text}\\n\\n{footnote_mentions}\\n\\n')\n",
    "            text = ''.join(pieces)\n",
    "            # start a new line to add the footnotes and then start another\n",
    "            # to add the footnote mentions.\n",
    "        else: # latex str ends with '$'\n",
    "            pieces.insert(2, f'\\n\\n{footnote_mentions}\\n\\n')\n",
    "            pieces.insert(1, footnote_text)\n",
    "            text = ''.join(pieces)\n",
    "            # new_line_index = \n",
    "    return text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$$asdf$$\n",
      "\n",
      "[^1]\n",
      "\n",
      "[^1]: ![[notat_note_name]]\n",
      "\n",
      "\n",
      "asdf asdf $asdf$[^1] asdf asdf \n",
      "\n",
      "[^1]: ![[notat_note_name]]\n",
      "\n",
      "\n",
      "\n",
      "fjfjfjfj\n",
      "\n",
      "---\n",
      "cssclass: clean-embeds\n",
      "aliases: []\n",
      "tags: [_meta/literature_note, _reference/18785, _meta/concept, _meta/proof]\n",
      "---\n",
      "# Topic[^1]\n",
      "\n",
      "Theorem 2.1. The map $\\mathrm{q} \\mapsto \\mathrm{q} \\cap A$ defines a bijection from the set of prime ideals of $S^{-1} A$[^2] and the set of prime ideals of A that do not intersect $S .$ The inverse map is $\\mathfrak{p} \\mapsto \\mathfrak{p} S^{-1} A$.\n",
      "\n",
      "[^2]: ![[18785_notation_S_minus_1_A_localization_of_a_commutative_ring_with_respect_to_a_multiplicative_subset]]\n",
      "\n",
      "\n",
      "\n",
      "Proof. See [1, Cor.11.20] or [2, Prop. 3.11.iv].\n",
      "\n",
      "# See Also\n",
      "\n",
      "# Meta\n",
      "## References\n",
      "![[_reference_18785]]\n",
      "\n",
      "## Citations and Footnotes\n",
      "[^1]: Sutherland, Theorem 2.1, Page 11\n",
      "\n",
      "\n",
      "For each integer $m$ and each transitive $G \\leq S_m$, there are constants $C(G), Q(G)$, and $e(G)$ such that, for all $q>Q(G)$ coprime to $\\#G$ and all $X>0$, \n",
      "\n",
      "$$N_G(\\mathbb{F}_q(t),X) \\leq C(G) X^{a(G)} \\log(X)^{e(G)}$$\n",
      "\n",
      "\n",
      "[^1]\n",
      "\n",
      "[^1]: ![[notat_note_name]]\n",
      "\n",
      "\n",
      "blah blah\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "text = r\"\"\"$$asdf$$\"\"\"\n",
    "mock_notat_note = MagicMock()\n",
    "mock_notat_note.name = 'notat_note_name'\n",
    "where_to_add = {8: [mock_notat_note]}\n",
    "print(_add_notation_note_embedded_footnotes(text, where_to_add))\n",
    "\n",
    "text = r\"\"\"asdf asdf $asdf$ asdf asdf \n",
    "\n",
    "fjfjfjfj\n",
    "\"\"\"\n",
    "mock_notat_note = MagicMock()\n",
    "mock_notat_note.name = 'notat_note_name'\n",
    "where_to_add = {16: [mock_notat_note]}\n",
    "output = _add_notation_note_embedded_footnotes(text, where_to_add)\n",
    "print(output)\n",
    "\n",
    "\n",
    "text = r\"\"\"---\n",
    "cssclass: clean-embeds\n",
    "aliases: []\n",
    "tags: [_meta/literature_note, _reference/18785, _meta/concept, _meta/proof]\n",
    "---\n",
    "# Topic[^1]\n",
    "\n",
    "Theorem 2.1. The map $\\mathrm{q} \\mapsto \\mathrm{q} \\cap A$ defines a bijection from the set of prime ideals of $S^{-1} A$ and the set of prime ideals of A that do not intersect $S .$ The inverse map is $\\mathfrak{p} \\mapsto \\mathfrak{p} S^{-1} A$.\n",
    "\n",
    "Proof. See [1, Cor.11.20] or [2, Prop. 3.11.iv].\n",
    "\n",
    "# See Also\n",
    "\n",
    "# Meta\n",
    "## References\n",
    "![[_reference_18785]]\n",
    "\n",
    "## Citations and Footnotes\n",
    "[^1]: Sutherland, Theorem 2.1, Page 11\"\"\"\n",
    "\n",
    "mock_notat_note.name = '18785_notation_S_minus_1_A_localization_of_a_commutative_ring_with_respect_to_a_multiplicative_subset'\n",
    "where_to_add = {254: [mock_notat_note]}\n",
    "output = _add_notation_note_embedded_footnotes(text, where_to_add)\n",
    "print(output)\n",
    "\n",
    "\n",
    "text = r\"\"\"\n",
    "\n",
    "For each integer $m$ and each transitive $G \\leq S_m$, there are constants $C(G), Q(G)$, and $e(G)$ such that, for all $q>Q(G)$ coprime to $\\#G$ and all $X>0$, \n",
    "\n",
    "$$N_G(\\mathbb{F}_q(t),X) \\leq C(G) X^{a(G)} \\log(X)^{e(G)}$$\n",
    "\n",
    "blah blah\n",
    "\n",
    "\"\"\"\n",
    "mock_notat_note = MagicMock()\n",
    "mock_notat_note.name = 'notat_note_name'\n",
    "where_to_add = {224: [mock_notat_note]}\n",
    "output = _add_notation_note_embedded_footnotes(text, where_to_add)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def add_notation_note_embedded_footnotes_to_info_note(\n",
    "        origin_note: VaultNote, # An info note\n",
    "        relied_notes: Optional[VaultNote | list[VaultNote]] = None, # notation notes to add embedded footnotes for.\n",
    "        cache: Optional[dict[str, dict[str, list[NoteLinkEnum]]]] = None, # The cache from which to identify the notation notes to add embedded footnotes for.\n",
    "        locate_by: Literal['first', 'best'] = 'best',\n",
    "        threshold: float = 0.8,\n",
    "        ):\n",
    "    \"\"\"\n",
    "    Modify the contents of `origin_note` to add footnotes to embedded links to `relied_notes`\n",
    "\n",
    "    One of `relied_notes` or `cache` must be passed.\n",
    "    \"\"\"\n",
    "    if relied_notes is None and cache is None:\n",
    "        raise ValueError(\"Expected `relied_note` or `cache` to be specified, but both were `None`.\")\n",
    "    if relied_notes is None:\n",
    "        if origin_note.name not in cache:\n",
    "            print(f'`origin_note.name` is not in `cache`. `origin_note` is {origin_note}.')\n",
    "            return\n",
    "        cache = remove_nonexistent_note_names_from_cache(cache, origin_note.vault)\n",
    "        relied_notes: list[VaultNote] = []\n",
    "        for relied_note_name, link_enums in cache[origin_note.name].items():\n",
    "            relied_note = VaultNote(origin_note.vault, name=relied_note_name)\n",
    "            if not note_is_of_type(relied_note, PersonalNoteTypeEnum.NOTATION_NOTE):\n",
    "                continue\n",
    "            if NoteLinkEnum.INFO_TO_NOTAT_VIA_EMBEDDING in link_enums:\n",
    "                relied_notes.append(relied_note)\n",
    "    if isinstance(relied_notes, VaultNote):\n",
    "        relied_notes = [relied_notes]\n",
    "\n",
    "    # Try to only add embedded links to notation notes that do not already exist in `origin_note`\n",
    "    origin_note_text = origin_note.text()\n",
    "    embedded_links_in_text: list[ObsidianLink] = links_from_text(\n",
    "        origin_note_text, ObsidianLink(\n",
    "            is_embedded=True, file_name=-1, anchor=-1, custom_text=-1, link_type=LinkType.WIKILINK))\n",
    "    embedded_note_names_in_text: set[str] = set([link.file_name for link in embedded_links_in_text])\n",
    "    new_relied_notes: list[VaultNote] = [\n",
    "        relied_note for relied_note in relied_notes if relied_note.name not in embedded_note_names_in_text]\n",
    "\n",
    "    where_to_add: dict[int, list[VaultNote]] = _where_to_add_notation_links(\n",
    "        origin_note, new_relied_notes, locate_by, threshold)\n",
    "    new_text = _add_notation_note_embedded_footnotes(origin_note.text(), where_to_add)\n",
    "    origin_note.write(new_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# origin_note = VaultNote(vault, name='18785_Theorem 2.1')\n",
    "# notat_note = VaultNote(vault, name='18785_notation_S_minus_1_A_localization_of_a_commutative_ring_with_respect_to_a_multiplicative_subset')\n",
    "# add_notation_note_embedded_footnotes_to_info_note(\n",
    "#     origin_note, notat_note,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(origin_note.text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notation Summarization using `NoteData` classes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions above surrounding the `NoteData` classes should actually be close to providing the means for gathering data for other ML tasks, such as the summarization task (thus far provided by `25_markdown.obisidian.personal.machine_learning.notation.summarization.ipynb`) and the definition naming task (thus far provided by `35_markdown.obsidian.personal.machine_learning.definition_and_notation_naming.ipynb`), and even improve upon them by providing contextual data (given by the notes linked by a given note)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SummarizationDataPoint(TypedDict):\n",
    "    input: str\n",
    "    output: str\n",
    "    notat_note_name: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def summarization_data(\n",
    "        notat_note_data_point: NotatNoteData,\n",
    "        info_note_data: dict[str, InfoNoteData], # For getting data from the linked notes.\n",
    "        notat_note_data: dict[str, NotatNoteData], # For getting data from the linked notes.\n",
    "        format: Literal['bert', 't5'],\n",
    "        augmentation: Optional[Literal['high', 'mid', 'low']] = None,\n",
    "        ) -> SummarizationDataPoint:\n",
    "    \"\"\"\n",
    "    Compile the summarization data from a `NotatNoteData` . \n",
    "\n",
    "    `notat_note_data_point` must have a nonblank value for its `note_content` attribute.\n",
    "\n",
    "    The summarization data consists of the notation note's main note content and (optionally)\n",
    "    the position data (which is the position data of the main note) along with (also optionally)\n",
    "    the content and/or position data of info notes that either the notation note or its\n",
    "    main note depend on (via the `NoteLinkEnum.NOTAT_TO_INFO_VIA_NOTAT` or \n",
    "    `NoteLinkEnum.INFO_TO_INFO_IN_CONTENT` enum items).\n",
    "\n",
    "    The augmentations are not applied to `notat_note_data_point` but are rather applied to\n",
    "    (copies of) the info notes that the notation note or its main note depends on. Use\n",
    "    `augment_notat_note_data_for_summarization` to augment that `NotatNoteData` object.\n",
    "\n",
    "    **Raises**\n",
    "    - `ValueError`\n",
    "        - if `notat_note_data_point.note_content` is not a nonblank `str`.\n",
    "    \"\"\"\n",
    "    content = notat_note_data_point.note_content\n",
    "    if content is None or content.strip() == '':\n",
    "        raise ValueError(f\"Expected `notat_note_data_point.content` to be a non blank string but was {notat_note_data_point.content}. The relevant notation note name is {notat_note_data_point.note_name}.\")\n",
    "    # Temporarily blank out the `note_content` attribute for the `.data_string` method\n",
    "    # To exclude whatever is in the `note_content` attribute.\n",
    "    notat_note_data_point.note_content = None\n",
    "    notat_note_string = notat_note_data_point.data_string(format)\n",
    "    notat_note_data_point.note_content = content\n",
    "\n",
    "    info_note_names_to_consider: set[str] = set()\n",
    "    for relied_note_name, link_types in notat_note_data_point.directly_linked_notes.items():\n",
    "        if NoteLinkEnum.NOTAT_TO_INFO_VIA_NOTAT in link_types:\n",
    "            info_note_names_to_consider.add(relied_note_name)\n",
    "    if (notat_note_data_point.main_note\n",
    "            and notat_note_data_point.main_note in info_note_data):\n",
    "        main_note_data_point = info_note_data[notat_note_data_point.main_note]\n",
    "        for relied_note_name, link_types in main_note_data_point.directly_linked_notes.items():\n",
    "            if NoteLinkEnum.INFO_TO_INFO_IN_CONTENT in link_types:\n",
    "                info_note_names_to_consider.add(relied_note_name)\n",
    "\n",
    "    parts: list[str] = [notat_note_string]\n",
    "    for relied_note_name in list(info_note_names_to_consider):\n",
    "        if not relied_note_name in info_note_data:\n",
    "            continue\n",
    "        relied_note_data_point = info_note_data[relied_note_name]\n",
    "        relied_note_data_point = relied_note_data_point.deepcopy()\n",
    "        if augmentation:\n",
    "            erase_position_metadata = _erase_position_metadata(augmentation)\n",
    "            relied_note_data_point.randomly_modify(augmentation, erase_position_metadata)\n",
    "        parts.append(relied_note_data_point.data_string(format))\n",
    "\n",
    "    sep_str = '\\n\\n[SEP]\\n\\n' if format == 'bert' else '\\n\\n</s>\\n\\n'\n",
    "    input = sep_str.join(parts)\n",
    "    output = notat_note_data_point.note_content\n",
    "    return SummarizationDataPoint(\n",
    "        input=input, output=output,\n",
    "        notat_note_name=notat_note_data_point.note_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def augment_notat_note_data_for_summarization(\n",
    "        notat_note_data_point: NotatNoteData,\n",
    "        augmentation: Literal['high', 'mid' ,'low'],\n",
    "        info_note_data: dict[str, InfoNoteData], # For getting data from the linked notes.\n",
    "        modify_links: bool = True, # If `True`, randomly modify the linking data from that of `notat_note_data_point`\n",
    "        ) -> NotatNoteData:\n",
    "    \"\"\"\n",
    "    Return a modified copy of `notat_note_data` augmented for providing\n",
    "    notation summarization data.\n",
    "\n",
    "    The `note_content` attribute of the outputted copy should not be modified\n",
    "    as it serves as the output of the training data.\n",
    "    \"\"\"\n",
    "    notat_note_data_copy = notat_note_data_point.deepcopy()\n",
    "    erase_position_metadata = _erase_position_metadata(augmentation)\n",
    "    content = notat_note_data_copy.note_content\n",
    "    notat_note_data_copy.randomly_modify(augmentation, erase_position_metadata)\n",
    "    notat_note_data_copy.note_content = content\n",
    "\n",
    "    if modify_links:\n",
    "        if augmentation == 'high':\n",
    "            num_rand_info_note_data_to_add = 3\n",
    "            key_deletion_prob = 0.10\n",
    "        elif augmentation == 'mid':\n",
    "            num_rand_info_note_data_to_add = 2\n",
    "            key_deletion_prob = 0.05\n",
    "        elif augmentation == 'low':\n",
    "            num_rand_info_note_data_to_add = 1\n",
    "            key_deletion_prob = 0.02\n",
    "        else:\n",
    "            num_rand_info_note_data_to_add = 0\n",
    "            key_deletion_prob = 0\n",
    "        # Delete \"links\" at random.\n",
    "        keys = list(notat_note_data_copy.directly_linked_notes)\n",
    "        for key in keys:\n",
    "            if random.random() < key_deletion_prob:\n",
    "                notat_note_data_copy.directly_linked_notes.pop(key)\n",
    "        # Add \"links\" to info notes at random.\n",
    "        random_info_note_names_to_add = random.choices(\n",
    "            list(info_note_data), k=min(len(info_note_data), num_rand_info_note_data_to_add))\n",
    "        for random_info_note_name in random_info_note_names_to_add:\n",
    "            _update_dict(\n",
    "                notat_note_data_copy.directly_linked_notes, \n",
    "                random_info_note_name,\n",
    "                NoteLinkEnum.NOTAT_TO_INFO_VIA_NOTAT)\n",
    "    return notat_note_data_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def notat_note_data_admissible_for_summarization_data(\n",
    "        notat_note_data_point: NotatNoteData\n",
    "        ) -> bool:  # `True` if the notation note data does not have the `_auto/notation_summary` tag, and the content of the notation note is essentially note blank.\n",
    "    if notat_note_data_point.tags and '_auto/notation_summary' in notat_note_data_point.tags:\n",
    "        return False\n",
    "    if notat_note_data_point.note_content is None:\n",
    "        return False \n",
    "    return bool(notat_note_data_point.note_content.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for name, data_point in notat_note_data.items():\n",
    "#     if notat_note_data_admissible_for_summarization_data(data_point):\n",
    "#         print(name)\n",
    "#         break\n",
    "\n",
    "# summ_data = summarization_data(data_point, info_note_data, notat_note_data, 'bert')\n",
    "# print(summ_data['input'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notat_note_data_admissible_for_summarization_data(notat_note_data['achter_pries_imht_notation_T_bar_gamma_smooth_trielliptic_curves_with_inertia_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _add_augmented_data_points(\n",
    "        info_note_data: dict[str, InfoNoteData],\n",
    "        notat_note_data: dict[str, NotatNoteData],\n",
    "        format: Literal['bert', 't5'],\n",
    "        data_point: NotatNoteData,\n",
    "        dict_data: list[SummarizationDataPoint],\n",
    "        ):\n",
    "    \"\"\"\n",
    "    Helper function to `summarization_dataset_from_note_data`.\n",
    "    \"\"\"\n",
    "    data_point_without_links = data_point.deepcopy()\n",
    "    data_point_without_links.directly_linked_notes = {}\n",
    "    for augmentation in ['low', 'mid', 'high']:\n",
    "        for modify_links, base_data_point in zip([True, False], [data_point, data_point_without_links]):\n",
    "            aug_data_point: NotatNoteData = augment_notat_note_data_for_summarization(\n",
    "                base_data_point, augmentation, info_note_data, modify_links)\n",
    "            dict_data.append(summarization_data(\n",
    "                aug_data_point, info_note_data, notat_note_data, format, augmentation))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def summarization_dataset_from_note_data(\n",
    "        info_note_data: dict[str, InfoNoteData],\n",
    "        notat_note_data: dict[str, NotatNoteData],\n",
    "        augment: bool,\n",
    "        format: Literal['bert', 't5'],\n",
    "        ) -> Dataset:\n",
    "    admissible_notat_note_names: list[str] = []\n",
    "    for notat_note_name, data_point in notat_note_data.items():\n",
    "        if notat_note_data_admissible_for_summarization_data(data_point):\n",
    "            admissible_notat_note_names.append(notat_note_name)\n",
    "    dict_data: list[SummarizationDataPoint] = []\n",
    "    for admissible_notat_note_name in admissible_notat_note_names:\n",
    "        data_point: NotatNoteData = notat_note_data[admissible_notat_note_name]\n",
    "        dict_data.append(summarization_data(\n",
    "            data_point, info_note_data, notat_note_data, format, augmentation=None))\n",
    "        if not augment:\n",
    "            continue\n",
    "        _add_augmented_data_points(info_note_data, notat_note_data, format, data_point, dict_data)\n",
    "    return Dataset.from_list(dict_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
