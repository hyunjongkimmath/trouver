{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp helper.arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import datetime\n",
    "import json\n",
    "from typing import Callable, Optional, Union\n",
    "import os\n",
    "from os import PathLike\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "import arxiv\n",
    "from arxiv import Client, Search, Result\n",
    "from pathvalidate import sanitize_filename\n",
    "\n",
    "from trouver.helper.files_and_folders import file_is_compressed, uncompress_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import tempfile\n",
    "from unittest.mock import patch, MagicMock\n",
    "import shutil\n",
    "\n",
    "\n",
    "from fastcore.test import *\n",
    "from fastcore.test import test_is\n",
    "from nbdev.showdoc import show_doc\n",
    "\n",
    "from trouver.helper.tests import _test_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# helper.arxiv\n",
    "> Functions for downloading (the source code of) articles from arXiv  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def arxiv_id(\n",
    "        arxiv_id_or_url: str,\n",
    "        ) -> str:\n",
    "    \"\"\"\n",
    "    Return the arxiv id from a str which is either of the arxiv id itself or the url\n",
    "    to the arxiv article.\n",
    "\n",
    "    **Raises**\n",
    "    - `ValueError`\n",
    "        - If the input does not contain a valid arXiv ID.\n",
    "    \"\"\"\n",
    "    id_pattern = r'(\\d{4}\\.\\d{4,5}(?:v\\d+)?)'\n",
    "    \n",
    "    # Check if input is a URL and extract the ID\n",
    "    if 'arxiv.org' in arxiv_id_or_url:\n",
    "        match = re.search(id_pattern, arxiv_id_or_url)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid arXiv URL provided.\")\n",
    "    \n",
    "    # If it's not a URL, assume it's an ID and validate it\n",
    "    elif re.match(id_pattern, arxiv_id_or_url):\n",
    "        return arxiv_id_or_url\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Invalid input. Please provide a valid arXiv ID or URL.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_eq(arxiv_id(\"1234.5678\"), \"1234.5678\")\n",
    "test_eq(arxiv_id(\"https://arxiv.org/abs/1234.5678\"), \"1234.5678\")\n",
    "test_eq(arxiv_id(\"1234.5678v1\"), \"1234.5678v1\")\n",
    "test_eq(arxiv_id(\"https://arxiv.org/abs/1234.5678v1\"), \"1234.5678v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `arxiv_search` function can be used to obtain an `arxiv.Search` object, which is used for downloading arxiv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def arxiv_search(\n",
    "        arxiv_ids: Union[str, list[str]], # The ID of a single arXiv article or multiple arxiv articles\n",
    "        client: Optional[Client] = None,  # an arxiv API Client. If `None`, create one on the spot.\n",
    "        results: bool = True, # If `True` return a `Result` object. otherwise, return a `Search`` object`.\n",
    "        ) -> Union[Result, Search]:\n",
    "    if not client:\n",
    "        client = Client()\n",
    "    if not isinstance(arxiv_ids, list):\n",
    "        arxiv_ids = [arxiv_ids]\n",
    "    search = Search(id_list=arxiv_ids)\n",
    "    if results:\n",
    "        return client.results(search)\n",
    "    return search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<itertools.islice>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| notest\n",
    "# Specify the arXiv ID of the paper you want to download\n",
    "# arxiv_id = \"2106.10586\"  # Replace with your desired arXiv ID\n",
    "arxiv_id = \"2106.10586\"  # Replace with your desired arXiv ID\n",
    "\n",
    "# Create a search object with the specified arXiv ID\n",
    "# client = Client()\n",
    "# search = Search(id_list=[arxiv_id])\n",
    "# results = client.results(search)\n",
    "results = arxiv_search(arxiv_id, results=True)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[arxiv.Result(entry_id='http://arxiv.org/abs/2106.10586v4', updated=datetime.datetime(2024, 6, 28, 1, 36, 47, tzinfo=datetime.timezone.utc), published=datetime.datetime(2021, 6, 19, 23, 50, 56, tzinfo=datetime.timezone.utc), title='Global $\\\\mathbb{A}^1$ degrees of covering maps between modular curves', authors=[arxiv.Result.Author('Hyun Jong Kim'), arxiv.Result.Author('Sun Woo Park')], summary=\"Given a projective smooth curve $X$ over any field $k$, we discuss two\\nnotions of global $\\\\mathbb{A}^1$ degree of a finite morphism of smooth curves\\n$f: X \\\\to \\\\mathbb{P}^1_k$ satisfying certain conditions. One originates from\\ncomputing the Euler number of the pullback of the line bundle\\n$\\\\mathscr{O}_{\\\\mathbb{P}^1}(1)$ as a generalization of Kass and Wickelgren's\\nconstruction of Euler numbers. The other originates from the construction of\\nglobal $\\\\mathbb{A}^1$ degree of morphisms of projective curves by Kass, Levine,\\nSolomon, and Wickelgren as a generalization of Morel's construction of\\n$\\\\mathbb{A}^1$-Brouwer degree of a morphism $f: \\\\mathbb{P}^1_k \\\\to\\n\\\\mathbb{P}^1_k$. We prove that under certain conditions on $N$, both notions of\\nglobal $\\\\mathbb{A}^1$ degrees of covering maps between modular curves $X_0(N)\\n\\\\to X(1)$, $X_1(N) \\\\to X(1)$, and $X(N) \\\\to X(1)$ agree to be equal to sums of\\nhyperbolic elements $\\\\langle 1 \\\\rangle + \\\\langle -1 \\\\rangle$ in the\\nGrothendieck-Witt ring $\\\\mathrm{GW}(k)$ for any field $k$ whose characteristic\\nis coprime to $N$ and the pullback of $\\\\mathscr{O}_{\\\\mathbb{P}^1}(1)$ is\\nrelatively oriented.\", comment='35 pages. Modified various statements to more precisely speak of\\n  \"relatively oriented\" maps or vector bundles instead of \"relatively\\n  orientable\" maps or vector bundles where appropriate --- the former phrasing\\n  suggests that a relative orientation is fixed. Additional minor edits', journal_ref=None, doi=None, primary_category='math.AG', categories=['math.AG', 'math.NT', '14F42, 14G35'], links=[arxiv.Result.Link('http://arxiv.org/abs/2106.10586v4', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2106.10586v4', title='pdf', rel='related', content_type=None)])]\n"
     ]
    }
   ],
   "source": [
    "#| notest\n",
    "listy = list(results)\n",
    "print(listy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://arxiv.org/abs/2106.10586v4'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| notest\n",
    "listy[0].entry_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_result_2 = arxiv.Result(\n",
    "    entry_id='http://arxiv.org/abs/2106.10586v4',\n",
    "    updated=datetime.datetime(2024, 6, 28, 1, 36, 47, tzinfo=datetime.timezone.utc),\n",
    "    published=datetime.datetime(2021, 6, 19, 23, 50, 56, tzinfo=datetime.timezone.utc),\n",
    "    title='Global $\\\\mathbb{A}^1$ degrees of covering maps between modular curves',\n",
    "    authors=[arxiv.Result.Author('Hyun Jong Kim'), arxiv.Result.Author('Sun Woo Park')],\n",
    "    summary=\"Given a projective smooth curve $X$ over any field $k$, we discuss two\\nnotions of global $\\\\mathbb{A}^1$ degree of a finite morphism of smooth curves\\n$f: X \\\\to \\\\mathbb{P}^1_k$ satisfying certain conditions. One originates from\\ncomputing the Euler number of the pullback of the line bundle\\n$\\\\mathscr{O}_{\\\\mathbb{P}^1}(1)$ as a generalization of Kass and Wickelgren's\\nconstruction of Euler numbers. The other originates from the construction of\\nglobal $\\\\mathbb{A}^1$ degree of morphisms of projective curves by Kass, Levine,\\nSolomon, and Wickelgren as a generalization of Morel's construction of\\n$\\\\mathbb{A}^1$-Brouwer degree of a morphism $f: \\\\mathbb{P}^1_k \\\\to\\n\\\\mathbb{P}^1_k$. We prove that under certain conditions on $N$, both notions of\\nglobal $\\\\mathbb{A}^1$ degrees of covering maps between modular curves $X_0(N)\\n\\\\to X(1)$, $X_1(N) \\\\to X(1)$, and $X(N) \\\\to X(1)$ agree to be equal to sums of\\nhyperbolic elements $\\\\langle 1 \\\\rangle + \\\\langle -1 \\\\rangle$ in the\\nGrothendieck-Witt ring $\\\\mathrm{GW}(k)$ for any field $k$ whose characteristic\\nis coprime to $N$ and the pullback of $\\\\mathscr{O}_{\\\\mathbb{P}^1}(1)$ is\\nrelatively oriented.\",\n",
    "    comment='35 pages. Modified various statements to more precisely speak of\\n  \"relatively oriented\" maps or vector bundles instead of \"relatively\\n  orientable\" maps or vector bundles where appropriate --- the former phrasing\\n  suggests that a relative orientation is fixed. Additional minor edits',\n",
    "    journal_ref=None,\n",
    "    doi=None,\n",
    "    primary_category='math.AG',\n",
    "    categories=['math.AG', 'math.NT', '14F42, 14G35'],\n",
    "    links=[arxiv.Result.Link('http://arxiv.org/abs/2106.10586v4', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2106.10586v4', title='pdf', rel='related', content_type=None)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def extract_metadata(\n",
    "        results: Union[list[Result], Result],\n",
    "        ) -> list[dict]: # Each dict corresponds to the metadata for each result.\n",
    "    \"\"\"\n",
    "    Return the metadata from the arxiv search results\n",
    "    \"\"\"\n",
    "    if not isinstance(results, list):\n",
    "        results = [results]\n",
    "    metadata_list = []\n",
    "    for result in results:\n",
    "        metadata = {\n",
    "            \"arxiv_id\": result.get_short_id(),\n",
    "            \"authors\": [author.name for author in result.authors],\n",
    "            \"title\": result.title,\n",
    "            \"summary\": result.summary,\n",
    "            \"primary_category\": result.primary_category,\n",
    "            \"categories\": result.categories,\n",
    "            \"published\": result.published,\n",
    "            \"updated\": result.updated,\n",
    "            \"doi\": result.doi,\n",
    "            \"comment\": result.comment,\n",
    "            \"journal_ref\": result.journal_ref,\n",
    "            \"links\": result.links\n",
    "        }\n",
    "        metadata_list.append(metadata)\n",
    "    return metadata_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'arxiv_id': '1605.08386v1',\n",
       "  'authors': ['Caprice Stanley', 'Tobias Windisch'],\n",
       "  'title': 'Heat-bath random walks with Markov bases',\n",
       "  'summary': 'Graphs on lattice points are studied whose edges come from a finite set of\\nallowed moves of arbitrary length. We show that the diameter of these graphs on\\nfibers of a fixed integer matrix can be bounded from above by a constant. We\\nthen study the mixing behaviour of heat-bath random walks on these graphs. We\\nalso state explicit conditions on the set of moves so that the heat-bath random\\nwalk, a generalization of the Glauber dynamics, is an expander in fixed\\ndimension.',\n",
       "  'primary_category': 'math.CO',\n",
       "  'categories': ['math.CO',\n",
       "   'math.ST',\n",
       "   'stat.TH',\n",
       "   'Primary: 05C81, Secondary: 37A25, 11P21'],\n",
       "  'published': datetime.datetime(2016, 5, 26, 17, 59, 46, tzinfo=datetime.timezone.utc),\n",
       "  'updated': datetime.datetime(2016, 5, 26, 17, 59, 46, tzinfo=datetime.timezone.utc),\n",
       "  'doi': None,\n",
       "  'comment': '20 pages, 3 figures',\n",
       "  'journal_ref': None,\n",
       "  'links': [arxiv.Result.Link('http://arxiv.org/abs/1605.08386v1', title=None, rel='alternate', content_type=None),\n",
       "   arxiv.Result.Link('http://arxiv.org/pdf/1605.08386v1', title='pdf', rel='related', content_type=None)]}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mock_result = arxiv.Result(\n",
    "    entry_id='http://arxiv.org/abs/1605.08386v1',\n",
    "    updated=datetime.datetime(2016, 5, 26, 17, 59, 46, tzinfo=datetime.timezone.utc),\n",
    "    published=datetime.datetime(2016, 5, 26, 17, 59, 46, tzinfo=datetime.timezone.utc),\n",
    "    title='Heat-bath random walks with Markov bases',\n",
    "    authors=[arxiv.Result.Author('Caprice Stanley'), arxiv.Result.Author('Tobias Windisch')],\n",
    "    summary='Graphs on lattice points are studied whose edges come from a finite set of\\nallowed moves of arbitrary length. We show that the diameter of these graphs on\\nfibers of a fixed integer matrix can be bounded from above by a constant. We\\nthen study the mixing behaviour of heat-bath random walks on these graphs. We\\nalso state explicit conditions on the set of moves so that the heat-bath random\\nwalk, a generalization of the Glauber dynamics, is an expander in fixed\\ndimension.',\n",
    "    comment='20 pages, 3 figures',\n",
    "    journal_ref=None,\n",
    "    doi=None,\n",
    "    primary_category='math.CO',\n",
    "    categories=['math.CO', 'math.ST', 'stat.TH', 'Primary: 05C81, Secondary: 37A25, 11P21'],\n",
    "    links=[arxiv.Result.Link('http://arxiv.org/abs/1605.08386v1',\n",
    "                             title=None, rel='alternate', content_type=None),\n",
    "           arxiv.Result.Link('http://arxiv.org/pdf/1605.08386v1', title='pdf', rel='related',\n",
    "                             content_type=None),])\n",
    "extract_metadata(mock_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ArxivMetadataEncoder(json.JSONEncoder):\n",
    "    \"\"\"\n",
    "    `json` encoder to accomapny the `extract_metadta` function when using `json.dump`. \n",
    "    \"\"\"\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, datetime.datetime):\n",
    "            return obj.isoformat()\n",
    "        elif isinstance(obj, arxiv.Result.Link):\n",
    "            return obj.href\n",
    "        return super().default(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"timestamp\": \"2024-12-05T17:29:43.843085\",\n",
      "    \"link\": \"https://example.com\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Your dictionary with datetime and arxiv.Result.Link objects\n",
    "data = {\n",
    "    \"timestamp\": datetime.datetime.now(),\n",
    "    \"link\": arxiv.Result.Link(href=\"https://example.com\", title=\"Example\")\n",
    "}\n",
    "\n",
    "# Convert the dictionary to JSON\n",
    "json_data = json.dumps(data, cls=ArxivMetadataEncoder, indent=4)\n",
    "print(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading arxiv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def extract_last_names(\n",
    "        authors: list[str]\n",
    "        ):\n",
    "    last_names = []\n",
    "    for author in authors:\n",
    "        # Split the name into parts\n",
    "        parts = author.split()\n",
    "        # Handle special cases like \"de Jong\"\n",
    "        if len(parts) > 2 and parts[-2].lower() in ['de', 'van', 'von', 'del', 'della', 'di', 'da', 'dos']:\n",
    "            last_name = f\"{parts[-2]} {parts[-1]}\"\n",
    "        else:\n",
    "            last_name = parts[-1]\n",
    "        \n",
    "        # Remove any commas or periods\n",
    "        last_name = re.sub(r'[,.]', '', last_name)\n",
    "        last_names.append(last_name)\n",
    "    return last_names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `extract_last_names` function is a convenient helper function for naming downloaded arxiv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Smith', 'Garcia-Lopez', 'de Jong', 'Li']\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "authors = [\"John Smith\", \"Maria Garcia-Lopez\", \"Pieter de Jong\", \"Xin Li\"]\n",
    "last_names = extract_last_names(authors)\n",
    "print(last_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def folder_name_for_source(\n",
    "        result: Result,\n",
    "        lowercase: bool = True\n",
    "        ) -> str:\n",
    "    family_names = extract_last_names([author.name for author in result.authors])\n",
    "    if len(family_names) > 4:\n",
    "        family_names_text = f'{family_names[0]}_et_al'\n",
    "    else:\n",
    "        family_names_text = '_'.join(family_names)\n",
    "    if lowercase:\n",
    "        output = f'{family_names_text.lower()}_{create_acronym(result.title)}'\n",
    "    else:\n",
    "        output = f'{family_names_text}_{create_acronym(result.title)}'\n",
    "    return output\n",
    "\n",
    "\n",
    "def create_acronym(title):\n",
    "    # Words to exclude from acronym\n",
    "    exclude_words = set(['a', 'an', 'the', 'on', 'and', 'of', 'to', 'over', 'in', 'for', 'with', 'by', 'at', 'from'])\n",
    "    \n",
    "    # Split the title into words\n",
    "    words = re.findall(r'\\b[\\w-]+\\b', title)\n",
    "    \n",
    "    acronym = ''\n",
    "    for word in words:\n",
    "        if word.lower() not in exclude_words:\n",
    "            if len(word) == 1 and word.isupper():\n",
    "                # Keep single uppercase letters (likely mathematical symbols) as is\n",
    "                acronym += word\n",
    "            elif word.lower() in ['i', 'ii', 'iii', 'iv', 'v', 'vi', 'vii', 'viii', 'ix', 'x']:\n",
    "                # Handle Roman numerals\n",
    "                acronym += word.lower()\n",
    "            elif '-' in word:\n",
    "                # Handle hyphenated words\n",
    "                parts = word.split('-')\n",
    "                acronym += ''.join(part[0].lower() if not (len(part) == 1 and part.isupper()) else part[0] for part in parts)\n",
    "            else:\n",
    "                # Take the first letter of other words, always lowercase\n",
    "                acronym += word[0].lower()\n",
    "    \n",
    "    return acronym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`folder_name_for_source` and `create_acronym` are convenient helper functions for naming folders newly created when downloading source code for arxiv files; the author of `trouver` roughly uses these conventions for organizing source code files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Lectures on K3 surfaces\n",
      "Acronym: lks\n",
      "\n",
      "Title: Positivity in Algebraic Geometry I\n",
      "Acronym: pagI\n",
      "\n",
      "Title: On the Cohomology of Finite Groups\n",
      "Acronym: cfg\n",
      "\n",
      "Title: An Introduction to A-infinity Algebras\n",
      "Acronym: iAia\n",
      "\n",
      "Title: Quantum Field Theory and the Standard Model\n",
      "Acronym: qftsm\n",
      "\n",
      "Title: Category O for gl(n,C) and the Cohomology of Flag Varieties\n",
      "Acronym: cOgnCcfv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test cases\n",
    "titles = [\n",
    "    \"Lectures on K3 surfaces\",\n",
    "    \"Positivity in Algebraic Geometry I\",\n",
    "    \"On the Cohomology of Finite Groups\",\n",
    "    \"An Introduction to A-infinity Algebras\",\n",
    "    \"Quantum Field Theory and the Standard Model\",\n",
    "    \"Category O for gl(n,C) and the Cohomology of Flag Varieties\"\n",
    "]\n",
    "\n",
    "for title in titles:\n",
    "    print(f\"Title: {title}\")\n",
    "    print(f\"Acronym: {create_acronym(title)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heat-bath random walks with Markov bases\n",
      "stanley_windisch_hbrwmb\n"
     ]
    }
   ],
   "source": [
    "mock_result = arxiv.Result(\n",
    "    entry_id='http://arxiv.org/abs/1605.08386v1',\n",
    "    updated=datetime.datetime(2016, 5, 26, 17, 59, 46, tzinfo=datetime.timezone.utc),\n",
    "    published=datetime.datetime(2016, 5, 26, 17, 59, 46, tzinfo=datetime.timezone.utc),\n",
    "    title='Heat-bath random walks with Markov bases',\n",
    "    authors=[arxiv.Result.Author('Caprice Stanley'), arxiv.Result.Author('Tobias Windisch')],\n",
    "    summary='Graphs on lattice points are studied whose edges come from a finite set of\\nallowed moves of arbitrary length. We show that the diameter of these graphs on\\nfibers of a fixed integer matrix can be bounded from above by a constant. We\\nthen study the mixing behaviour of heat-bath random walks on these graphs. We\\nalso state explicit conditions on the set of moves so that the heat-bath random\\nwalk, a generalization of the Glauber dynamics, is an expander in fixed\\ndimension.',\n",
    "    comment='20 pages, 3 figures',\n",
    "    journal_ref=None,\n",
    "    doi=None,\n",
    "    primary_category='math.CO',\n",
    "    categories=['math.CO', 'math.ST', 'stat.TH', 'Primary: 05C81, Secondary: 37A25, 11P21'],\n",
    "    links=[arxiv.Result.Link('http://arxiv.org/abs/1605.08386v1',\n",
    "                             title=None, rel='alternate', content_type=None),\n",
    "           arxiv.Result.Link('http://arxiv.org/pdf/1605.08386v1', title='pdf', rel='related',\n",
    "                             content_type=None),])\n",
    "extract_metadata(mock_result)\n",
    "print(mock_result.title)\n",
    "output = folder_name_for_source(mock_result)\n",
    "print(output)\n",
    "assert ' ' not in output\n",
    "assert output.startswith('stanley_windisch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def file_name_for_pdf(\n",
    "        result: Result\n",
    "        ) -> str:\n",
    "    family_names = extract_last_names([author.name for author in result.authors])\n",
    "    if len(family_names) > 4:\n",
    "        family_names_text = f'{family_names[0]} et al'\n",
    "    else:\n",
    "        family_names_text = ', '.join(family_names)\n",
    "    output = f'{family_names_text} - {result.title}'\n",
    "    return sanitize_filename(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`file_name_for_pdf` could be a good convention for naming downloaded pdf files of arxiv articles. Pass this as the `file_or_folder_names` parameter for `download_from_results`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Stanley, Windisch - Heat-bath random walks with Markov bases'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mock_result = arxiv.Result(\n",
    "    entry_id='http://arxiv.org/abs/1605.08386v1',\n",
    "    updated=datetime.datetime(2016, 5, 26, 17, 59, 46, tzinfo=datetime.timezone.utc),\n",
    "    published=datetime.datetime(2016, 5, 26, 17, 59, 46, tzinfo=datetime.timezone.utc),\n",
    "    title='Heat-bath random walks with Markov bases',\n",
    "    authors=[arxiv.Result.Author('Caprice Stanley'), arxiv.Result.Author('Tobias Windisch')],\n",
    "    summary='Graphs on lattice points are studied whose edges come from a finite set of\\nallowed moves of arbitrary length. We show that the diameter of these graphs on\\nfibers of a fixed integer matrix can be bounded from above by a constant. We\\nthen study the mixing behaviour of heat-bath random walks on these graphs. We\\nalso state explicit conditions on the set of moves so that the heat-bath random\\nwalk, a generalization of the Glauber dynamics, is an expander in fixed\\ndimension.',\n",
    "    comment='20 pages, 3 figures',\n",
    "    journal_ref=None,\n",
    "    doi=None,\n",
    "    primary_category='math.CO',\n",
    "    categories=['math.CO', 'math.ST', 'stat.TH', 'Primary: 05C81, Secondary: 37A25, 11P21'],\n",
    "    links=[arxiv.Result.Link('http://arxiv.org/abs/1605.08386v1',\n",
    "                             title=None, rel='alternate', content_type=None),\n",
    "           arxiv.Result.Link('http://arxiv.org/pdf/1605.08386v1', title='pdf', rel='related',\n",
    "                             content_type=None),])\n",
    "file_name_for_pdf(mock_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def download_from_results(\n",
    "        results: Result | list[Result],\n",
    "        dir: PathLike, # The directory into which to download the files\n",
    "        source: bool = True, # If `True`, download the source file. Otherweise, download a pdf file.\n",
    "        # filename: Optional[str] = None, # The file name to save the file as. If `None`, then the filename is set to the arXiv id of the article.\n",
    "        decompress_compressed_file: bool = True, # If `True`and if `source` is `True`, then decompress the source file after downloading it.\n",
    "        file_or_folder_names: None | str | list[str] | Callable[Result, str] = folder_name_for_source, # If `None`, then the file/folder is named the arxiv id. If a `str` (in which case `results` must be a single `Result` or a `list[Result]` of length 1) or `list[str]` (whose length must equal that of `results`), then each file/folder is named by the specified corresponding `str`. If `Callable[Result, str]`, then each file/folder is named using the specified `Callable` \n",
    "        delete_compressed_file: bool = True, # If `True` and if `source` and `decompress_compressed_file` are `True`, then delete the compressed source file after downloading and then uncompressing it.\n",
    "        download_metadata: bool = True, # If `True`, and if `source` is `True`, then create a file called `metadata.json` and put it into the newly created folder, unless a file called `metadata.json` already exists, in which case, a unique file name is created  \n",
    "        verbose: bool = False,\n",
    "        ) -> list[Path]: # Each `Path` is the folder in which the source files are newly downloaded or the path to the pdf file that is newly downloaded.\n",
    "    \"\"\"\n",
    "    Download either the source files or pdfs of the arxiv article encoded in the results.\n",
    "\n",
    "    - If `source = True` and `decompress_compressed_file = True`, then \n",
    "        - Download the source file/folder into a newly created folder (whose name is specified\n",
    "          by `file_or_folder_names`) within `dir` and decompress the source (if applicable) in\n",
    "          this newly created folder.\n",
    "        - If `delete_compressed_file = True`, then delete the compressed file.\n",
    "    - If `source = False`, then just download a pdf.\n",
    "\n",
    "    For `file_or_folder_names`, the recommanded `Callable` arguments are `folder_name_for_source`\n",
    "    for downloading source files and `file_name_for_pdf` for downloading pdf files.\n",
    "\n",
    "    \"\"\"\n",
    "    if not isinstance(results, list):\n",
    "        results = [results]\n",
    "    if file_or_folder_names is not None and not isinstance(file_or_folder_names, (list,str)):\n",
    "        file_or_folder_names = [file_or_folder_names(result) for result in results]\n",
    "    elif file_or_folder_names is None:\n",
    "        file_or_folder_names = [None for result in results]\n",
    "    elif isinstance(file_or_folder_names, str):\n",
    "        file_or_folder_names = [file_or_folder_names]\n",
    "    downloaded_paths = []\n",
    "    for result, file_or_folder_name in zip(results, file_or_folder_names):\n",
    "        if not source:\n",
    "            downloaded_paths.append(\n",
    "                _download_pdf_with_name(result, dir, file_or_folder_name))\n",
    "            continue\n",
    "        downloaded_paths.append(\n",
    "            _download_source(\n",
    "                result, dir, file_or_folder_name, decompress_compressed_file,\n",
    "                delete_compressed_file, download_metadata, verbose))\n",
    "    return downloaded_paths\n",
    "\n",
    "\n",
    "\n",
    "def _download_pdf_with_name(\n",
    "        result: Result,\n",
    "        dir: PathLike,\n",
    "        file_or_folder_name: str|None # The name of the pdf file. If `None`, then the default name for the pdf file is used.\n",
    "        ) -> Path:\n",
    "    \"\"\"\n",
    "    Downloads the pdf for `result`.\n",
    "\n",
    "    Helper function to `download_from_results`\n",
    "    \"\"\"\n",
    "    if file_or_folder_name is not None:\n",
    "        return Path(result.download_pdf(dir, filename=f'{file_or_folder_name}.pdf'))\n",
    "    else:\n",
    "        return Path(result.download_pdf(dir))\n",
    "    \n",
    "\n",
    "            \n",
    "\n",
    "def _create_folder_for_source_download(\n",
    "        result: Result,\n",
    "        dir: PathLike,\n",
    "        file_or_folder_name: str|None # The name of the folder in which the source should be downloaded. If `None`, then the arxiv id for `result` is used.\n",
    "        ) -> Path: # The newly created folder \n",
    "    \"\"\"\n",
    "    Creates a new folder inside `dir` in which to download the source for `result`.\n",
    "\n",
    "    Helper function to `download_from_results`\n",
    "    \"\"\"\n",
    "    if file_or_folder_name is None:\n",
    "        file_or_folder_name = result.entry_id\n",
    "    new_folder = Path(dir) / file_or_folder_name\n",
    "    if os.path.isdir(new_folder): #If folder exists\n",
    "        # TODO: warn that folder exists\n",
    "        if file_or_folder_name == result.entry_id:\n",
    "            file_or_folder_name = f'{file_or_folder_name}_dupl'    \n",
    "        else:\n",
    "            file_or_folder_name = f'{file_or_folder_name}_{result.get_short_id()}'\n",
    "        new_folder = Path(dir) / file_or_folder_name\n",
    "    while os.path.isdir(new_folder): #If folder still exists\n",
    "        file_or_folder_name = f'{file_or_folder_name}_dupl'\n",
    "        new_folder = Path(dir) / file_or_folder_name\n",
    "    os.mkdir(new_folder)\n",
    "    return new_folder\n",
    "\n",
    "\n",
    "def _download_source(\n",
    "        result: Result,\n",
    "        dir: PathLike,\n",
    "        file_or_folder_name: str,\n",
    "        decompress_compressed_file: bool,\n",
    "        delete_compressed_file: bool,\n",
    "        download_metadata: bool,\n",
    "        verbose: bool\n",
    "        ) -> Path:\n",
    "    \"\"\"\n",
    "    Download source file into folder, decompress (as needed) the source, and download metadata\n",
    "\n",
    "    Helper function to `download_from_results`\n",
    "    \"\"\"\n",
    "    new_source_folder = _create_folder_for_source_download(result, dir, file_or_folder_name)\n",
    "    source_file_path = result.download_source(new_source_folder)\n",
    "    source_file_path = Path(new_source_folder) / source_file_path\n",
    "    if verbose:\n",
    "        print(source_file_path)\n",
    "    if decompress_compressed_file and file_is_compressed(source_file_path):\n",
    "        uncompressed = uncompress_file(source_file_path)\n",
    "        if delete_compressed_file:\n",
    "            os.remove(source_file_path)\n",
    "        if len(uncompressed) == 1 and file_is_compressed(uncompressed[0]):\n",
    "            uncompressed_again = uncompress_file(uncompressed[0])\n",
    "            if delete_compressed_file:\n",
    "                os.remove(uncompressed[0])\n",
    "    if not download_metadata:\n",
    "        return new_source_folder\n",
    "    metadata_file_name = _unique_metadata_file_name(new_source_folder)\n",
    "    metadata = extract_metadata(result)[0] \n",
    "    with open(new_source_folder / metadata_file_name, 'w') as json_file:\n",
    "        json.dump(metadata, json_file, cls=ArxivMetadataEncoder, indent=4)\n",
    "    return new_source_folder\n",
    "\n",
    "\n",
    "def _unique_metadata_file_name(\n",
    "        new_source_folder: PathLike # The folder in which to make the metadata file.\n",
    "        ) -> str: #s tr: A unique file name for the metadata file within `new_source_folder`.\n",
    "\n",
    "    \"\"\"\n",
    "    Identify a name to name the metadata file within `new_source_folder`; \n",
    "    the default name is `metadata.json` unless there is already a file with that name.\n",
    "    \n",
    "    If `metadata.json` exists, it appends a numeric suffix to create a unique file name \n",
    "    (e.g., `metadata_1.json`, `metadata_2.json`, etc.).\n",
    "    \"\"\"\n",
    "    # Ensure the input is a Path object\n",
    "    folder = Path(new_source_folder)\n",
    "    \n",
    "    # Default file name\n",
    "    base_name = \"metadata\"\n",
    "    extension = \".json\"\n",
    "    candidate = folder / f\"{base_name}{extension}\"\n",
    "    \n",
    "    # Check if the default file name exists\n",
    "    counter = 1\n",
    "    while candidate.exists():\n",
    "        # TODO: warn that metadata.json exists\n",
    "        # Generate a new candidate with a numeric suffix\n",
    "        candidate = folder / f\"{base_name}_{counter}{extension}\"\n",
    "        counter += 1\n",
    "    \n",
    "    return str(candidate.name)  # Return only the file name, not the full path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`download_from_results` downloads an arxiv article (the source or a pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_result_1 = arxiv.Result(\n",
    "    entry_id='http://arxiv.org/abs/1605.08386v1',\n",
    "    updated=datetime.datetime(2016, 5, 26, 17, 59, 46, tzinfo=datetime.timezone.utc),\n",
    "    published=datetime.datetime(2016, 5, 26, 17, 59, 46, tzinfo=datetime.timezone.utc),\n",
    "    title='Heat-bath random walks with Markov bases',\n",
    "    authors=[arxiv.Result.Author('Caprice Stanley'), arxiv.Result.Author('Tobias Windisch')],\n",
    "    summary='Graphs on lattice points are studied whose edges come from a finite set of\\nallowed moves of arbitrary length. We show that the diameter of these graphs on\\nfibers of a fixed integer matrix can be bounded from above by a constant. We\\nthen study the mixing behaviour of heat-bath random walks on these graphs. We\\nalso state explicit conditions on the set of moves so that the heat-bath random\\nwalk, a generalization of the Glauber dynamics, is an expander in fixed\\ndimension.',\n",
    "    comment='20 pages, 3 figures',\n",
    "    journal_ref=None,\n",
    "    doi=None,\n",
    "    primary_category='math.CO',\n",
    "    categories=['math.CO', 'math.ST', 'stat.TH', 'Primary: 05C81, Secondary: 37A25, 11P21'],\n",
    "    links=[arxiv.Result.Link('http://arxiv.org/abs/1605.08386v1',\n",
    "                            title=None, rel='alternate', content_type=None),\n",
    "        arxiv.Result.Link('http://arxiv.org/pdf/1605.08386v1', title='pdf', rel='related',\n",
    "                            content_type=None),])\n",
    "mock_result_2 = arxiv.Result(\n",
    "    entry_id='http://arxiv.org/abs/2106.10586v4',\n",
    "    updated=datetime.datetime(2024, 6, 28, 1, 36, 47, tzinfo=datetime.timezone.utc),\n",
    "    published=datetime.datetime(2021, 6, 19, 23, 50, 56, tzinfo=datetime.timezone.utc),\n",
    "    title='Global $\\\\mathbb{A}^1$ degrees of covering maps between modular curves',\n",
    "    authors=[arxiv.Result.Author('Hyun Jong Kim'), arxiv.Result.Author('Sun Woo Park')],\n",
    "    summary=\"Given a projective smooth curve $X$ over any field $k$, we discuss two\\nnotions of global $\\\\mathbb{A}^1$ degree of a finite morphism of smooth curves\\n$f: X \\\\to \\\\mathbb{P}^1_k$ satisfying certain conditions. One originates from\\ncomputing the Euler number of the pullback of the line bundle\\n$\\\\mathscr{O}_{\\\\mathbb{P}^1}(1)$ as a generalization of Kass and Wickelgren's\\nconstruction of Euler numbers. The other originates from the construction of\\nglobal $\\\\mathbb{A}^1$ degree of morphisms of projective curves by Kass, Levine,\\nSolomon, and Wickelgren as a generalization of Morel's construction of\\n$\\\\mathbb{A}^1$-Brouwer degree of a morphism $f: \\\\mathbb{P}^1_k \\\\to\\n\\\\mathbb{P}^1_k$. We prove that under certain conditions on $N$, both notions of\\nglobal $\\\\mathbb{A}^1$ degrees of covering maps between modular curves $X_0(N)\\n\\\\to X(1)$, $X_1(N) \\\\to X(1)$, and $X(N) \\\\to X(1)$ agree to be equal to sums of\\nhyperbolic elements $\\\\langle 1 \\\\rangle + \\\\langle -1 \\\\rangle$ in the\\nGrothendieck-Witt ring $\\\\mathrm{GW}(k)$ for any field $k$ whose characteristic\\nis coprime to $N$ and the pullback of $\\\\mathscr{O}_{\\\\mathbb{P}^1}(1)$ is\\nrelatively oriented.\",\n",
    "    comment='35 pages. Modified various statements to more precisely speak of\\n  \"relatively oriented\" maps or vector bundles instead of \"relatively\\n  orientable\" maps or vector bundles where appropriate --- the former phrasing\\n  suggests that a relative orientation is fixed. Additional minor edits',\n",
    "    journal_ref=None,\n",
    "    doi=None,\n",
    "    primary_category='math.AG',\n",
    "    categories=['math.AG', 'math.NT', '14F42, 14G35'],\n",
    "    links=[arxiv.Result.Link('http://arxiv.org/abs/2106.10586v4', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2106.10586v4', title='pdf', rel='related', content_type=None)])\n",
    "\n",
    "single_result = mock_result_1\n",
    "multiple_results = [mock_result_1, mock_result_2] \n",
    "folder_name_1 = folder_name_for_source(mock_result_1)\n",
    "folder_name_2 = folder_name_for_source(mock_result_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "with tempfile.TemporaryDirectory(prefix='temp_dir', dir=os.getcwd()) as temp_dir:\n",
    "    temp_vault = Path(temp_dir) / 'arxiv_file_download_example_folder'\n",
    "    shutil.copytree(_test_directory() / 'arxiv_file_download_example_folder', temp_vault)\n",
    "    # 1. Single Result vs. List of Results\n",
    "    # Test with single Result\n",
    "    downloaded_paths = download_from_results(mock_result_1, temp_vault, source=True)\n",
    "    assert (temp_vault / folder_name_1).exists()\n",
    "    assert downloaded_paths\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also pass multiple results to `download_from_results`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "with tempfile.TemporaryDirectory(prefix='temp_dir', dir=os.getcwd()) as temp_dir:\n",
    "    temp_vault = Path(temp_dir) / 'arxiv_file_download_example_folder'\n",
    "    shutil.copytree(_test_directory() / 'arxiv_file_download_example_folder', temp_vault)\n",
    "    # Test with multiple Results\n",
    "    download_from_results(multiple_results, temp_vault, source=True)\n",
    "    # os.startfile(temp_vault)\n",
    "    # input()\n",
    "    assert (temp_vault / folder_name_1).exists()\n",
    "    assert (temp_vault / folder_name_2).exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifying `source=False` downloads the pdf instead of the source files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "with tempfile.TemporaryDirectory(prefix='temp_dir', dir=os.getcwd()) as temp_dir:\n",
    "    temp_vault = Path(temp_dir) / 'arxiv_file_download_example_folder'\n",
    "    shutil.copytree(_test_directory() / 'arxiv_file_download_example_folder', temp_vault)\n",
    "    # 2. Source vs. PDF download\n",
    "    download_from_results(single_result, temp_vault, source=False)\n",
    "    assert (temp_vault / f'{folder_name_1}.pdf').exists()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By specifying `source=True` and `decompress_compressed_file=False`, we can just download the compressed file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "with tempfile.TemporaryDirectory(prefix='temp_dir', dir=os.getcwd()) as temp_dir:\n",
    "    temp_vault = Path(temp_dir) / 'arxiv_file_download_example_folder'\n",
    "    shutil.copytree(_test_directory() / 'arxiv_file_download_example_folder', temp_vault)\n",
    "    # 3. Decompression options\n",
    "    download_from_results(single_result, temp_vault, source=True, decompress_compressed_file=False)\n",
    "    tar_gz_files = glob.glob(str(temp_vault / folder_name_1 / '*.tar.gz')) \n",
    "    assert len(tar_gz_files) > 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The folder or pdf file can get a custon name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "with tempfile.TemporaryDirectory(prefix='temp_dir', dir=os.getcwd()) as temp_dir:\n",
    "    temp_vault = Path(temp_dir) / 'arxiv_file_download_example_folder'\n",
    "    shutil.copytree(_test_directory() / 'arxiv_file_download_example_folder', temp_vault)\n",
    "    # 4. File/folder naming\n",
    "    download_from_results(single_result, temp_vault, file_or_folder_names='custom_name')\n",
    "    assert (temp_vault / 'custom_name').exists()\n",
    "\n",
    "    download_from_results(multiple_results, temp_vault, file_or_folder_names=['name1', 'name2'])\n",
    "    assert (temp_vault / 'name1').exists()\n",
    "    assert (temp_vault / 'name2').exists()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`delete_compresed_file` can be set to `False` to preserve the compressed file after decomppressing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "with tempfile.TemporaryDirectory(prefix='temp_dir', dir=os.getcwd()) as temp_dir:\n",
    "    temp_vault = Path(temp_dir) / 'arxiv_file_download_example_folder'\n",
    "    shutil.copytree(_test_directory() / 'arxiv_file_download_example_folder', temp_vault)\n",
    "    # 5. Compressed file handling\n",
    "    download_from_results(single_result, temp_vault, delete_compressed_file=False)\n",
    "    tar_gz_files = glob.glob(str(temp_vault / folder_name_1 / '*.tar.gz')) \n",
    "    assert len(tar_gz_files) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, if the source is downloaded into a folder, then the metadata of the arxiv article is stored in a `json` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "with tempfile.TemporaryDirectory(prefix='temp_dir', dir=os.getcwd()) as temp_dir:\n",
    "    temp_vault = Path(temp_dir) / 'arxiv_file_download_example_folder'\n",
    "    shutil.copytree(_test_directory() / 'arxiv_file_download_example_folder', temp_vault)\n",
    "\n",
    "    # 6. Metadata file\n",
    "    download_from_results(single_result, temp_vault, download_metadata=True)\n",
    "    assert (temp_vault / folder_name_1 / 'metadata.json').exists()\n",
    "\n",
    "    # 7. Edge cases\n",
    "    download_from_results([], temp_vault)  # Empty list\n",
    "    # Test with non-existent arxiv ID (should handle gracefully)\n",
    "    # non_existent = next(arxiv.Search(id_list=['0000.00000']).results())\n",
    "    # download_from_results(non_existent, temp_vault)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tempfile.TemporaryDirectory(prefix='temp_dir', dir=os.getcwd()) as temp_dir:\n",
    "    temp_vault = Path(temp_dir) / 'arxiv_file_download_example_folder'\n",
    "    shutil.copytree(_test_directory() / 'arxiv_file_download_example_folder', temp_vault)\n",
    "    # 8. Folder creation (duplicate handling)\n",
    "    download_from_results(single_result, temp_vault)\n",
    "    download_from_results(single_result, temp_vault)  # Should create a duplicate folder\n",
    "    assert (temp_vault / folder_name_1).exists()\n",
    "\n",
    "    # 9. File types (if you have examples of different source types)\n",
    "    # This would require specific known arxiv IDs with different source types\n",
    "\n",
    "    # 10. Error handling\n",
    "    with ExceptionExpected(Exception):\n",
    "    # with pytest.raises(Exception):  # Replace with specific exception\n",
    "        download_from_results(single_result, '/non/existent/path')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trouver_py310_venv",
   "language": "python",
   "name": "trouver_py310_venv"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
