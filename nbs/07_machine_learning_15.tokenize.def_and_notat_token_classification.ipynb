{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp machine_learning.tokenize.def_and_notat_token_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# machine_learning.tokenize.def_and_notat_token_classification\n",
    "> Functions for gathering and processing tokenization data and for using ML models trained with such data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previous, `trouver` just had functionalities for using ML models to identify newly introduced notations in text and for gathering data to train such models. Moreover, such models were merely classification models, and using these models to identify newly introduced notations had a lot of computational redundancies.\n",
    "\n",
    "This module aims to provide the same functionalities for both definitions and notations by training and using token classification models instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a new module dedicated to definition and notation identification and move approparite functions over there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from collections.abc import Callable\n",
    "import copy\n",
    "from itertools import pairwise\n",
    "import os \n",
    "from os import PathLike\n",
    "from pathlib import Path\n",
    "import random\n",
    "from typing import Literal, Optional, TypedDict, Union\n",
    "import warnings\n",
    "\n",
    "import bs4\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "import regex\n",
    "from transformers import BatchEncoding, pipelines, PreTrainedTokenizer, PreTrainedTokenizerFast\n",
    "\n",
    "from trouver.helper import is_not_space_and_not_punc, split_string_at_indices\n",
    "from trouver.helper.definition_and_notation import double_asterisk_indices, notation_asterisk_indices\n",
    "from trouver.helper.html import (\n",
    "    add_HTML_tag_data_to_raw_text, add_space_to_lt_symbols_without_space, remove_html_tags_in_text, StrAndHTMLTagsWithIndices,\n",
    "    HTMLTagWithIndices)\n",
    "from trouver.helper.latex.core import _is_balanced_braces, _first_curly_bracket, _last_curly_bracket\n",
    "from trouver.helper.latex.augment import (\n",
    "    augment_text, change_font_styles_at_random, change_greek_letters_at_random, dollar_sign_manipulation, push_dollar_sign, random_char_modification, random_latex_command_removal, random_word_removal, remove_font_styles_at_random, remove_math_keywords)\n",
    "\n",
    "from trouver.helper.regex import latex_indices, replace_string_by_indices\n",
    "from trouver.obsidian.file import MarkdownFile, MarkdownLineEnum\n",
    "from trouver.personal_vault.note_processing import process_standard_information_note\n",
    "from trouver.obsidian.vault import VaultNote\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unittest import mock\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "from datasets import ClassLabel, Dataset, Features, Sequence, Value\n",
    "from transformers import AutoTokenizer\n",
    "from fastcore.test import *\n",
    "\n",
    "from trouver.helper.tests import _test_directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse data from information notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def convert_double_asterisks_to_html_tags(\n",
    "        text: str\n",
    "        ) -> str:\n",
    "    \"\"\"\n",
    "    Replace the double asterisks, which signify definitions and notations,\n",
    "    in `text` with HTML tags.\n",
    "    \"\"\"\n",
    "    double_asts = double_asterisk_indices(text)\n",
    "    replacement_html_tags = [\n",
    "        _html_tag_from_double_ast(text[start:end])\n",
    "        for start, end in double_asts]\n",
    "    return replace_string_by_indices(\n",
    "        text, double_asts, replacement_html_tags)\n",
    "\n",
    "\n",
    "def _html_tag_from_double_ast(\n",
    "        double_ast_string: str # Starts and ends with double asts\n",
    "        ) -> str:\n",
    "    \"\"\"\n",
    "    Get the HTML tag representing definition or notation data from\n",
    "    a string surrounded by double asterisks.\n",
    "\n",
    "    This is used in the `_convert_double_asterisks_to_html_tags` function.\n",
    "    \"\"\"\n",
    "    no_asts = double_ast_string[2:-2]\n",
    "    if notation_asterisk_indices(double_ast_string):\n",
    "        return f'<span notation=\"\">{no_asts}</span>'\n",
    "    else:\n",
    "        return f'<b definition=\"\">{no_asts}</b>'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<b definition=\"\">hi</b>. Here is a notation <span notation=\"\">$asdf$</span>\n"
     ]
    }
   ],
   "source": [
    "print(convert_double_asterisks_to_html_tags(\"**hi**. Here is a notation **$asdf$**\"))\n",
    "test_eq(convert_double_asterisks_to_html_tags(\"**hi**. Here is a notation **$asdf$**\"), '<b definition=\"\">hi</b>. Here is a notation <span notation=\"\">$asdf$</span>')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def raw_text_with_html_tags_from_markdownfile(\n",
    "        mf: MarkdownFile,\n",
    "        vault: PathLike\n",
    "        ) -> str:\n",
    "    \"\"\"\n",
    "    Process the `MarkdownFile`, replacing the double asterisk surrounded\n",
    "    text indicating definitions and notations to be HTML tags instead.\n",
    "    \"\"\"\n",
    "    mf = process_standard_information_note(\n",
    "        mf, vault, remove_double_asterisks=False,\n",
    "        remove_html_tags=False)\n",
    "    return convert_double_asterisks_to_html_tags(str(mf))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Some kind of potato[^2]\\n\\n[^2]: Some footnote\\n\\nSome link\\n'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "\n",
    "# TODO: \n",
    "# I want to make sure that footnotes are getting properly removed.\n",
    "mf = MarkdownFile.from_string(\n",
    "    r\"\"\"---\n",
    "aliases: []\n",
    "tags: []\n",
    "---\n",
    "# Something  \n",
    "\n",
    "Some kind of potato[^2]\n",
    "\n",
    "[^2]: Some footnote\n",
    "\n",
    "[[link_to_note|Some link]]\n",
    "\n",
    "\n",
    "# See Also\n",
    "# Meta\n",
    "## References and Citations\n",
    "\"\"\") \n",
    "raw_text_with_html_tags_from_markdownfile(mf, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "mf = MarkdownFile.from_string(\n",
    "    r\"\"\"---\n",
    "aliases: []\n",
    "tags: []\n",
    "---\n",
    "# Galois group of a separable and normal finite field extension\n",
    "\n",
    "Let $L/K$ be a separable and normal finite field extension. Its <b definition=\"Galois group of a separable and normal finite field extension\">Galois group</b> <span notation=\"\">$\\operatorname{Gal}(L/K)$</span> is...\n",
    "\n",
    "# Galois group of a separable and normal profinite field extension\n",
    "\n",
    "In fact, the notion of a Galois group can be defined for profinite field extensions. Given a separable and normal profinite field extension $L/K$, say that\n",
    "$L = \\varinjlim_i L_i$ where $L_i/K$ are finite extensions. Its **Galois group** **$\\operatorname{Gal}(L/K)$**\n",
    "\n",
    "# See Also\n",
    "# Meta\n",
    "## References and Citations\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following example, let `mf` be the following `MarkdownFile`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "aliases: []\n",
      "tags: []\n",
      "---\n",
      "# Galois group of a separable and normal finite field extension\n",
      "\n",
      "Let $L/K$ be a separable and normal finite field extension. Its <b definition=\"Galois group of a separable and normal finite field extension\">Galois group</b> <span notation=\"\">$\\operatorname{Gal}(L/K)$</span> is...\n",
      "\n",
      "# Galois group of a separable and normal profinite field extension\n",
      "\n",
      "In fact, the notion of a Galois group can be defined for profinite field extensions. Given a separable and normal profinite field extension $L/K$, say that\n",
      "$L = \\varinjlim_i L_i$ where $L_i/K$ are finite extensions. Its **Galois group** **$\\operatorname{Gal}(L/K)$**\n",
      "\n",
      "# See Also\n",
      "# Meta\n",
      "## References and Citations\n"
     ]
    }
   ],
   "source": [
    "print(str(mf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `raw_text_with_html_tags_from_markdownfile` function processes the `MarkdownFile` much in the same way as the `process_standard_information_note` function, except it 1. preserves HTML tags, and 2. replaces text surrounded by double asterisks `**` with HTML tags signifiying whether the text displays a definition or a notation.\n",
    "\n",
    "In the below example, note that the `vault` parameter is set to `None`; this is fine for this example becaues the `process_standard_information_note` function only needs a `vault` argument when embedded links need to be replaced with text (via the `MarkdownFile.replace_embedded_links_with_text` function), but `mf` has no embedded links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let $L/K$ be a separable and normal finite field extension. Its <b definition=\"Galois group of a separable and normal finite field extension\">Galois group</b> <span notation=\"\">$\\operatorname{Gal}(L/K)$</span> is...\n",
      "\n",
      "In fact, the notion of a Galois group can be defined for profinite field extensions. Given a separable and normal profinite field extension $L/K$, say that\n",
      "$L = \\varinjlim_i L_i$ where $L_i/K$ are finite extensions. Its <b definition=\"\">Galois group</b> <span notation=\"\">$\\operatorname{Gal}(L/K)$</span>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(raw_text_with_html_tags_from_markdownfile(mf, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "assert '**' not in raw_text_with_html_tags_from_markdownfile(mf, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class HTMLData(TypedDict):\n",
    "    note_name: str\n",
    "    raw_text: str\n",
    "    tags: list[HTMLTagWithIndices]\n",
    "    # list[bs4.element.Tag]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def html_data_from_note(\n",
    "        note_or_mf: Union[VaultNote, MarkdownFile], # Either a `VaultNote`` object to a note or a `MarkdownFile` object from which to extra html data.\n",
    "        vault: Optional[PathLike] = None, # If vault to use when processing the `MarkdownFile` objects (if `note_of_mf` is a `VaultNote`, then this `MarkdownFile` object is created from the text of the note), cf. the `process_standard_information_note` function.\n",
    "        note_name: Optional[str] = None, # If `note_or_mf` is a `MarkdownFile`, `note_name` should be the name of the note from which the `MarkdownFile` comes from if applicable. If `note_or_mf` is a `VaultNote` object, then `note_name` is ignored and `note_or_mf.name` is used instead.\n",
    "        ) -> Union[HTMLData, None]: # The keys to the dict are \"note_name\", \"raw_text\", \"tags\". However, `None` is returned if `note` does not exist or the note is marked with auto-generated, unverified data.\n",
    "    # TODO: implement obtaining multiple datapoints from a single note\n",
    "    # Via typos for example.\n",
    "    # TODO: implement various data augmentation techniques\n",
    "    \"\"\"Obtain html data for token classification from the information note.\n",
    "\n",
    "    Currently, the token types mainly revolve around definitions and\n",
    "    notations.\n",
    "\n",
    "    If `note` has the tag `_auto/def_and_notat_identified`, then the data\n",
    "    in the note is assumed to be auto-generated and not verified and\n",
    "    `None` is returned.\n",
    "\n",
    "    **Returns**\n",
    "    - Union[dict, None]\n",
    "        - The keys-value pairs are \n",
    "            - `\"note_name\"` - The name of the note\n",
    "            - `\"raw_text\"` - The raw text to include in the data.\n",
    "            - `\"tags\"` - The list with HTML tags carrying definition/notation\n",
    "              data and their locations in the Raw text. See the second output to\n",
    "              the function `remove_html_tags_in_text`.\n",
    "                - Each element of the list is a tuple consisting of a ``bs4.element.Tag``\n",
    "                  and two ints.\n",
    "    \"\"\"\n",
    "    if isinstance(note_or_mf, VaultNote) and not note_or_mf.exists():\n",
    "        return None\n",
    "    if isinstance(note_or_mf, VaultNote):\n",
    "        mf = MarkdownFile.from_vault_note(note_or_mf)\n",
    "        note_name = note_or_mf.name\n",
    "        if vault is None:\n",
    "            vault = note_or_mf.vault\n",
    "    else: # isinstance(note_or_mf, MarkdownFile):\n",
    "        mf = note_or_mf.copy(deep=False)\n",
    "    if mf.has_tag('_auto/def_and_notat_identified'):\n",
    "        return None\n",
    "    raw_text_with_tags = raw_text_with_html_tags_from_markdownfile(mf, vault)\n",
    "    raw_text, tags_and_locations = remove_html_tags_in_text(raw_text_with_tags)\n",
    "\n",
    "    return HTMLData(note_name=note_name, raw_text=raw_text, tags=tags_and_locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following example, we mock a `VaultNote` whose content is that of `mf` in the example for the `raw_text_with_html_tags_from_markdownfile` function. Note that there is some text surrounded by double within `mf` surrounded by double asterisks `**` and some text surrounded by HTML tags to indicate definitions and notations introduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is the text from mf:\n",
      "\n",
      "---\n",
      "aliases: []\n",
      "tags: []\n",
      "---\n",
      "# Galois group of a separable and normal finite field extension\n",
      "\n",
      "Let $L/K$ be a separable and normal finite field extension. Its <b definition=\"Galois group of a separable and normal finite field extension\">Galois group</b> <span notation=\"\">$\\operatorname{Gal}(L/K)$</span> is...\n",
      "\n",
      "# Galois group of a separable and normal profinite field extension\n",
      "\n",
      "In fact, the notion of a Galois group can be defined for profinite field extensions. Given a separable and normal profinite field extension $L/K$, say that\n",
      "$L = \\varinjlim_i L_i$ where $L_i/K$ are finite extensions. Its **Galois group** **$\\operatorname{Gal}(L/K)$**\n",
      "\n",
      "# See Also\n",
      "# Meta\n",
      "## References and Citations\n",
      "{'note_name': \"Note's name\", 'raw_text': 'Let $L/K$ be a separable and normal finite field extension. Its Galois group $\\\\operatorname{Gal}(L/K)$ is...\\n\\nIn fact, the notion of a Galois group can be defined for profinite field extensions. Given a separable and normal profinite field extension $L/K$, say that\\n$L = \\\\varinjlim_i L_i$ where $L_i/K$ are finite extensions. Its Galois group $\\\\operatorname{Gal}(L/K)$\\n', 'tags': [HTMLTagWithIndices(tag=<b definition=\"Galois group of a separable and normal finite field extension\">Galois group</b>, start=64, end=76), HTMLTagWithIndices(tag=<span notation=\"\">$\\operatorname{Gal}(L/K)$</span>, start=77, end=102), HTMLTagWithIndices(tag=<b definition=\"\">Galois group</b>, start=330, end=342), HTMLTagWithIndices(tag=<span notation=\"\">$\\operatorname{Gal}(L/K)$</span>, start=343, end=368)]}\n",
      "[HTMLTagWithIndices(tag=<b definition=\"Galois group of a separable and normal finite field extension\">Galois group</b>, start=64, end=76), HTMLTagWithIndices(tag=<span notation=\"\">$\\operatorname{Gal}(L/K)$</span>, start=77, end=102), HTMLTagWithIndices(tag=<b definition=\"\">Galois group</b>, start=330, end=342), HTMLTagWithIndices(tag=<span notation=\"\">$\\operatorname{Gal}(L/K)$</span>, start=343, end=368)]\n"
     ]
    }
   ],
   "source": [
    "mf = MarkdownFile.from_string(\n",
    "    r\"\"\"---\n",
    "aliases: []\n",
    "tags: []\n",
    "---\n",
    "# Galois group of a separable and normal finite field extension\n",
    "\n",
    "Let $L/K$ be a separable and normal finite field extension. Its <b definition=\"Galois group of a separable and normal finite field extension\">Galois group</b> <span notation=\"\">$\\operatorname{Gal}(L/K)$</span> is...\n",
    "\n",
    "# Galois group of a separable and normal profinite field extension\n",
    "\n",
    "In fact, the notion of a Galois group can be defined for profinite field extensions. Given a separable and normal profinite field extension $L/K$, say that\n",
    "$L = \\varinjlim_i L_i$ where $L_i/K$ are finite extensions. Its **Galois group** **$\\operatorname{Gal}(L/K)$**\n",
    "\n",
    "# See Also\n",
    "# Meta\n",
    "## References and Citations\n",
    "\"\"\")\n",
    "\n",
    "with (mock.patch('__main__.VaultNote') as mock_VaultNote,\n",
    "      mock.patch('__main__.MarkdownFile.from_vault_note') as mock_from_vault_note,\n",
    "      mock.patch('__main__.isinstance') as mock_isinstance):\n",
    "    mock_VaultNote.exists.return_value = True\n",
    "    mock_VaultNote.name = \"Note's name\"\n",
    "    mock_from_vault_note.return_value = mf\n",
    "    mock_isinstance.return_value = True\n",
    "\n",
    "    print(f\"The following is the text from mf:\\n\\n{str(mf)}\")\n",
    "\n",
    "    html_data = html_data_from_note(mock_VaultNote, None)\n",
    "    print(html_data)\n",
    "\n",
    "    test_eq(html_data['note_name'], \"Note's name\")\n",
    "    assert '**' not in html_data['raw_text']\n",
    "    assert '<' not in html_data['raw_text']  # Test the lack of HTML tags in the raw text\n",
    "\n",
    "    print(html_data['tags'])\n",
    "    test_eq(len(html_data['tags']), 4)\n",
    "    assert isinstance(html_data['tags'][0][0], bs4.element.Tag)\n",
    "    assert html_data['tags'][0][0].has_attr('definition')\n",
    "    assert not html_data['tags'][0][0].has_attr('notation')\n",
    "    assert html_data['tags'][1][0].has_attr('notation')\n",
    "    assert not html_data['tags'][1][0].has_attr('definition')\n",
    "    assert html_data['tags'][2][0].has_attr('definition')\n",
    "    assert not html_data['tags'][2][0].has_attr('notation')\n",
    "    assert html_data['tags'][3][0].has_attr('notation')\n",
    "    assert not html_data['tags'][3][0].has_attr('definition')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also just pass a `MarkdonwFile` object instead of a `VaultNote` object. In this case, we can specify the `note_name` parameter to indicate which note the `MarkdownFile` object came from, if applicable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'note_name': \"Note's name\", 'raw_text': 'Let $L/K$ be a separable and normal finite field extension. Its Galois group $\\\\operatorname{Gal}(L/K)$ is...\\n\\nIn fact, the notion of a Galois group can be defined for profinite field extensions. Given a separable and normal profinite field extension $L/K$, say that\\n$L = \\\\varinjlim_i L_i$ where $L_i/K$ are finite extensions. Its Galois group $\\\\operatorname{Gal}(L/K)$\\n', 'tags': [HTMLTagWithIndices(tag=<b definition=\"Galois group of a separable and normal finite field extension\">Galois group</b>, start=64, end=76), HTMLTagWithIndices(tag=<span notation=\"\">$\\operatorname{Gal}(L/K)$</span>, start=77, end=102), HTMLTagWithIndices(tag=<b definition=\"\">Galois group</b>, start=330, end=342), HTMLTagWithIndices(tag=<span notation=\"\">$\\operatorname{Gal}(L/K)$</span>, start=343, end=368)]}\n",
      "[HTMLTagWithIndices(tag=<b definition=\"Galois group of a separable and normal finite field extension\">Galois group</b>, start=64, end=76), HTMLTagWithIndices(tag=<span notation=\"\">$\\operatorname{Gal}(L/K)$</span>, start=77, end=102), HTMLTagWithIndices(tag=<b definition=\"\">Galois group</b>, start=330, end=342), HTMLTagWithIndices(tag=<span notation=\"\">$\\operatorname{Gal}(L/K)$</span>, start=343, end=368)]\n"
     ]
    }
   ],
   "source": [
    "html_data = html_data_from_note(mf, vault=None, note_name=\"Note's name\")\n",
    "print(html_data)\n",
    "\n",
    "test_eq(html_data['note_name'], \"Note's name\")\n",
    "assert '**' not in html_data['raw_text']\n",
    "assert '<' not in html_data['raw_text']  # Test the lack of HTML tags in the raw text\n",
    "\n",
    "print(html_data['tags'])\n",
    "test_eq(len(html_data['tags']), 4)\n",
    "assert isinstance(html_data['tags'][0][0], bs4.element.Tag)\n",
    "assert html_data['tags'][0][0].has_attr('definition')\n",
    "assert not html_data['tags'][0][0].has_attr('notation')\n",
    "assert html_data['tags'][1][0].has_attr('notation')\n",
    "assert not html_data['tags'][1][0].has_attr('definition')\n",
    "assert html_data['tags'][2][0].has_attr('definition')\n",
    "assert not html_data['tags'][2][0].has_attr('notation')\n",
    "assert html_data['tags'][3][0].has_attr('notation')\n",
    "assert not html_data['tags'][3][0].has_attr('definition')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we do not specify `note_name`, then `None` is used for the `'Note name'` key in the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'note_name': None, 'raw_text': 'Let $L/K$ be a separable and normal finite field extension. Its Galois group $\\\\operatorname{Gal}(L/K)$ is...\\n\\nIn fact, the notion of a Galois group can be defined for profinite field extensions. Given a separable and normal profinite field extension $L/K$, say that\\n$L = \\\\varinjlim_i L_i$ where $L_i/K$ are finite extensions. Its Galois group $\\\\operatorname{Gal}(L/K)$\\n', 'tags': [HTMLTagWithIndices(tag=<b definition=\"Galois group of a separable and normal finite field extension\">Galois group</b>, start=64, end=76), HTMLTagWithIndices(tag=<span notation=\"\">$\\operatorname{Gal}(L/K)$</span>, start=77, end=102), HTMLTagWithIndices(tag=<b definition=\"\">Galois group</b>, start=330, end=342), HTMLTagWithIndices(tag=<span notation=\"\">$\\operatorname{Gal}(L/K)$</span>, start=343, end=368)]}\n"
     ]
    }
   ],
   "source": [
    "html_data = html_data_from_note(mf, vault=None, note_name=None)\n",
    "print(html_data)\n",
    "\n",
    "assert html_data['note_name'] is None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the following example, the note has an HTML tag already with extra data (attributes other than `'definition'` or `'notation'`). We assert that the extra data is preserved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is the text of the mocked note: \n",
      "\n",
      " Let $X$ be a topological space and let $U \\subseteq X$ be an subspace. The <b definition=\"Closure of a subspace of a topological space\" typo=\"dosure of $U$\">closure of $U$</b> is defined as...\n",
      "\n",
      "\n",
      "{'note_name': \"Note's name\", 'raw_text': 'Let $X$ be a topological space and let $U \\\\subseteq X$ be an subspace. The closure of $U$ is defined as...', 'tags': [HTMLTagWithIndices(tag=<b definition=\"Closure of a subspace of a topological space\" typo=\"dosure of $U$\">closure of $U$</b>, start=75, end=89)]}\n"
     ]
    }
   ],
   "source": [
    "with (mock.patch('__main__.VaultNote') as mock_VaultNote,\n",
    "      mock.patch('__main__.MarkdownFile.from_vault_note') as mock_from_vault_note,\n",
    "      mock.patch('__main__.isinstance') as mock_isinstance):\n",
    "    mock_VaultNote.exists.return_value = True\n",
    "    mock_VaultNote.name = \"Note's name\"\n",
    "    mock_isinstance.return_value = True\n",
    "\n",
    "    text = r'Let $X$ be a topological space and let $U \\subseteq X$ be an subspace. The <b definition=\"Closure of a subspace of a topological space\" typo=\"dosure of $U$\">closure of $U$</b> is defined as...'\n",
    "    mf = MarkdownFile.from_string(text)\n",
    "    mock_from_vault_note.return_value = mf\n",
    "    print(f\"The following is the text of the mocked note: \\n\\n {text}\\n\\n\")\n",
    "\n",
    "    html_data = html_data_from_note(mock_VaultNote, None)\n",
    "    print(html_data)\n",
    "    assert html_data['tags'][0][0].has_attr('typo')\n",
    "    test_eq(html_data['tags'][0][0].attrs['typo'], 'dosure of $U$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following example, the (mocked) note has the `#_auto/def_and_notats_identified` tag to indicate that its definition and notation markings were auto-generated by a model (trained with data processed by the `tokenize_html_data` function) using the `auto_mark_def_and_notats` function. In this case, the `html_data_from_note` function returns `None` to prevent gathering data that is unverified and auto-generated by a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is the text of the mocked note: \n",
      "\n",
      "---\n",
      "tags: [_auto/def_and_notat_identified]\n",
      "---\n",
      "Let $X$ be a topological space and let $U \\subseteq X$ be an subspace. The <b definition=\"Closure of a subspace of a topological space\" typo=\"dosure of $U$\">closure of $U$</b> is defined as...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# with (mock.patch('__main__.VaultNote') as mock_VaultNote,\n",
    "#       mock.patch('__main__.MarkdownFile.from_vault_note') as mock_from_vault_note):\n",
    "#     mock_VaultNote.exists.return_value = True\n",
    "#     mock_VaultNote.name = \"Note's name\"\n",
    "text = r'''---\n",
    "tags: [_auto/def_and_notat_identified]\n",
    "---\n",
    "Let $X$ be a topological space and let $U \\subseteq X$ be an subspace. The <b definition=\"Closure of a subspace of a topological space\" typo=\"dosure of $U$\">closure of $U$</b> is defined as...'''\n",
    "\n",
    "mf = MarkdownFile.from_string(text)\n",
    "mock_from_vault_note.return_value = mf\n",
    "print(f\"The following is the text of the mocked note: \\n\\n{text}\\n\\n\")\n",
    "\n",
    "html_data = html_data_from_note(note_or_mf=mf)\n",
    "assert(html_data is None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def tokenize_html_data(\n",
    "        html_locus: HTMLData, # An output of `html_data_from_note`\n",
    "        tokenizer: Union[PreTrainedTokenizer, PreTrainedTokenizerFast],\n",
    "        max_length: int, # Max length for each sequence of tokens\n",
    "        ner_tag_from_html_tag: Callable[[bs4.element.Tag], Union[str, 'None']], # takes in a bs4.element.Tag and outputs the ner_tag (as a string or `None`)\n",
    "        label2id: dict[str, int], # The keys are ner_tag's of the form f\"I-{output}\" or f\"B-{output}\" where `output` is an output of `ner_tag_from_html_tag`.\n",
    "        default_label: str = \"O\", # The default label for the NER tagging.\n",
    "        ) -> tuple[list[list[str]], list[list[int]]]: # The first list consists of the tokens and the second list consists of the named entity recognition tags.\n",
    "    \"\"\"Actually tokenize the html data outputted by `html_data_from_note`.\n",
    "\n",
    "    To account for the possibility that the raw text is long,\n",
    "    this function uses the `tokenizer.batch_encode_plus` function\n",
    "    to tokenize the text into sequences. \n",
    "    \"\"\"\n",
    "    tokenized = tokenizer.batch_encode_plus(\n",
    "        [html_locus[\"raw_text\"]], max_length=max_length, return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True, truncation=True)\n",
    "\n",
    "    default_id = label2id[default_label]        \n",
    "    ner_ids = [[default_id for _ in seq_input_ids]\n",
    "               for seq_input_ids in tokenized['input_ids']]\n",
    "    for tag, start, end in html_locus['tags']:\n",
    "        ner_tag = ner_tag_from_html_tag(tag)\n",
    "        if ner_tag is None:\n",
    "            continue  # `ner_tag` is not of relevant data.\n",
    "        tuppy = _start_end_seqs_indices_for_html_tag(tokenized, start, end - 1)\n",
    "        (start_seq, start_index_in_seq), (end_seq, end_index_in_seq) = tuppy\n",
    "        _set_ner_ids_for_tag(\n",
    "            ner_ids, start_seq, start_index_in_seq, end_seq, end_index_in_seq,\n",
    "            label2id, ner_tag)\n",
    "    # return tokenized[\"input_ids\"], ner_ids\n",
    "    tokens = [tokenizer.convert_ids_to_tokens(tokens_for_seq)\n",
    "              for tokens_for_seq in tokenized[\"input_ids\"]]\n",
    "    return tokens, ner_ids\n",
    "\n",
    "\n",
    "def _start_end_seqs_indices_for_html_tag(\n",
    "        tokenized: BatchEncoding,\n",
    "        tag_start_ind: int,\n",
    "        tag_end_ind: int\n",
    "        ) -> tuple[tuple[int, int], tuple[int, int]]: # The first tuple is `(a, b)` where `tokenized['input_ids'][a][b]` is the token corresponding to the start of the HTML tag's (raw) text. The second tuple is `(c, d)` where `tokenized['input_ids'][c][d]` is the token corresponding to the end of the HTML tag's (raw) text.\n",
    "    start_seq = _search_seq_ind_for_char(tokenized['offset_mapping'], tag_start_ind)\n",
    "    # start_index_in_seq = tokenized.char_to_token(batch_or_char_index=start_seq, char_index=tag_start_ind)\n",
    "    start_index_in_seq = _search_within_seq_for_char(tokenized['offset_mapping'][start_seq], tag_start_ind)\n",
    "    end_seq = _search_seq_ind_for_char(tokenized['offset_mapping'], tag_end_ind)\n",
    "    # end_index_in_seq = tokenized.char_to_token(batch_or_char_index=end_seq, char_index=tag_end_ind)\n",
    "    end_index_in_seq = _search_within_seq_for_char(tokenized['offset_mapping'][end_seq], tag_end_ind)\n",
    "    return (start_seq, start_index_in_seq), (end_seq, end_index_in_seq)\n",
    "\n",
    "\n",
    "def _min_max_char_ind_for_seq(\n",
    "        offset_for_seq: list[tuple[int,int]] # An item in tokenized['offset_mapping']\n",
    "        ):\n",
    "    min_char_ind, max_char_ind = 0, 0\n",
    "    for inds in offset_for_seq:\n",
    "        if inds != (0,0):\n",
    "            min_char_ind = inds[0]\n",
    "            break\n",
    "    for inds in reversed(offset_for_seq):\n",
    "        if inds != (0,0):\n",
    "            max_char_ind = inds[1]\n",
    "            break\n",
    "    return min_char_ind, max_char_ind\n",
    "\n",
    "def _char_is_in_seq(\n",
    "        offset_for_seq: list[int], # An item in tokenized['offset_mapping']\n",
    "        char: int # The index of a character in the original raw text\n",
    "        ) -> bool:\n",
    "    min_char_ind, max_char_ind = _min_max_char_ind_for_seq(offset_for_seq)\n",
    "    return min_char_ind <= char and char < max_char_ind\n",
    "\n",
    "def _search_seq_ind_for_char(\n",
    "        offsets: list[tuple[int, int]], # tokenized['offset_mapping']\n",
    "        char: int # The index of a character in the original raw text\n",
    "        ) -> int:\n",
    "    \"\"\"\n",
    "    Binary search the index of the sequence containing the token at the \n",
    "    location of the index `char` within the original (raw) text.\n",
    "\n",
    "    Based on pseudocode from https://pseudoeditor.com/guides/binary-search\n",
    "    \"\"\"\n",
    "    left = 0\n",
    "    right = len(offsets) - 1\n",
    "    while left <= right:\n",
    "        mid = (left + right) // 2\n",
    "        min_char_ind, max_char_ind = _min_max_char_ind_for_seq(offsets[mid])\n",
    "        if min_char_ind <= char and char < max_char_ind:\n",
    "            return mid\n",
    "        elif max_char_ind <= char:\n",
    "            left = mid + 1\n",
    "        else:\n",
    "            right = mid - 1\n",
    "    return -1  # This should not be returned under normal use.\n",
    "\n",
    "\n",
    "def _search_within_seq_for_char(\n",
    "        seq_offset: list[tuple[int, int]],\n",
    "        char: int\n",
    "    ) -> int:\n",
    "    \"\"\"\n",
    "    Binary search for the index within the sequence corresponding\n",
    "    to the token at the location of the index `char` within the\n",
    "    original (raw) text.\n",
    "\n",
    "    Based on pseudocode from https://pseudoeditor.com/guides/binary-search\n",
    "    \"\"\"\n",
    "    left = 0\n",
    "    right = len(seq_offset) - 1\n",
    "    while left <= right:\n",
    "        mid = (left + right) // 2\n",
    "        min_char_ind, max_char_ind = seq_offset[mid] \n",
    "        if min_char_ind <= char and char < max_char_ind:\n",
    "            return mid\n",
    "        elif max_char_ind <= char:\n",
    "            left = mid + 1\n",
    "        else:\n",
    "            right = mid - 1\n",
    "    return -1  # This should not be returned under normal use.\n",
    "\n",
    "\n",
    "def _set_ner_ids_for_tag(\n",
    "        ner_ids: list[list[int]],\n",
    "        start_seq: int, \n",
    "        start_index_in_seq: int,\n",
    "        end_seq: int,\n",
    "        end_index_in_seq: int,\n",
    "        label2id: dict[str, int],\n",
    "        ner_tag: str\n",
    "        ) -> None:\n",
    "    \"\"\"\n",
    "    After the locations of the tokens corresponding to a HTML tag have been found, \n",
    "    mark within `ner_ids` the appropriate NER tags at the locations corresponding\n",
    "    to the tokens' locations.\n",
    "    \"\"\"\n",
    "    ner_ids[start_seq][start_index_in_seq] = label2id[f\"B-{ner_tag}\"]\n",
    "    i_ner_id = label2id[f\"I-{ner_tag}\"]\n",
    "    seq, ind = start_seq, start_index_in_seq + 1\n",
    "    while seq < end_seq or ind <= end_index_in_seq:\n",
    "        if len(ner_ids[seq]) <= ind:\n",
    "            seq += 1\n",
    "            ind = 0\n",
    "        else:\n",
    "            ner_ids[seq][ind] = i_ner_id \n",
    "            ind += 1\n",
    "    \n",
    "\n",
    "\n",
    "def def_or_notat_from_html_tag(\n",
    "        tag: bs4.element.Tag\n",
    "        ) -> Union[str, None]:\n",
    "    \"\"\"\n",
    "    Can be passed as the `ner_tag_from_html_tag` argument in `tokenize_html_data`\n",
    "    for the purposes of compiling a dataset for definition and notation\n",
    "    identification.\n",
    "\n",
    "    The strings f\"I-{output}\" and f\"B-{output}\" are valid ner_tags. To use for \n",
    "    \"\"\"\n",
    "    if \"definition\" in tag.attrs:\n",
    "        return \"definition\"\n",
    "    elif \"notation\" in tag.attrs:\n",
    "        return \"notation\"\n",
    "    return None  # If the HTML tag carries neither definition nor notation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_eq(_min_max_char_ind_for_seq([(0,0), (1,3), (3,4), (4,7), (7,15), (0,0)]), (1,15))\n",
    "\n",
    "offsets = [[(0,0), (0,3), (4,5), (5,6), (6,7), (7,8), (8,9),],\n",
    "           [(10,12), (13,14), (15,18), (18,24)],\n",
    "           [(25,28), (29,35), (36,42), ]]\n",
    "test_eq(_search_seq_ind_for_char(offsets, 0), 0)\n",
    "test_eq(_search_seq_ind_for_char(offsets, 1), 0)\n",
    "test_eq(_search_seq_ind_for_char(offsets, 5), 0)\n",
    "test_eq(_search_seq_ind_for_char(offsets, 8), 0)\n",
    "# I don't think that character index 9 is something that I need to worry about.\n",
    "test_eq(_search_seq_ind_for_char(offsets, 10), 1)\n",
    "test_eq(_search_seq_ind_for_char(offsets, 23), 1)\n",
    "test_eq(_search_seq_ind_for_char(offsets, 25), 2)\n",
    "test_eq(_search_seq_ind_for_char(offsets, 41), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We continue with an example using the HTML data from the example for the `html_data_from_note` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'note_name': None, 'raw_text': 'Let $L/K$ be a separable and normal finite field extension. Its Galois group $\\\\operatorname{Gal}(L/K)$ is...\\n\\nIn fact, the notion of a Galois group can be defined for profinite field extensions. Given a separable and normal profinite field extension $L/K$, say that\\n$L = \\\\varinjlim_i L_i$ where $L_i/K$ are finite extensions. Its Galois group $\\\\operatorname{Gal}(L/K)$\\n', 'tags': [HTMLTagWithIndices(tag=<b definition=\"Galois group of a separable and normal finite field extension\">Galois group</b>, start=64, end=76), HTMLTagWithIndices(tag=<span notation=\"\">$\\operatorname{Gal}(L/K)$</span>, start=77, end=102), HTMLTagWithIndices(tag=<b definition=\"\">Galois group</b>, start=330, end=342), HTMLTagWithIndices(tag=<span notation=\"\">$\\operatorname{Gal}(L/K)$</span>, start=343, end=368)]}\n"
     ]
    }
   ],
   "source": [
    "mf = MarkdownFile.from_string(\n",
    "    r\"\"\"---\n",
    "aliases: []\n",
    "tags: []\n",
    "---\n",
    "# Galois group of a separable and normal finite field extension\n",
    "\n",
    "Let $L/K$ be a separable and normal finite field extension. Its <b definition=\"Galois group of a separable and normal finite field extension\">Galois group</b> <span notation=\"\">$\\operatorname{Gal}(L/K)$</span> is...\n",
    "\n",
    "# Galois group of a separable and normal profinite field extension\n",
    "\n",
    "In fact, the notion of a Galois group can be defined for profinite field extensions. Given a separable and normal profinite field extension $L/K$, say that\n",
    "$L = \\varinjlim_i L_i$ where $L_i/K$ are finite extensions. Its **Galois group** **$\\operatorname{Gal}(L/K)$**\n",
    "\n",
    "# See Also\n",
    "# Meta\n",
    "## References and Citations\n",
    "\"\"\")\n",
    "\n",
    "html_data = html_data_from_note(mf, vault=None, note_name=None)\n",
    "print(html_data)\n",
    "\n",
    "assert html_data['note_name'] is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HTMLTagWithIndices(tag=<b definition=\"Galois group of a separable and normal finite field extension\">Galois group</b>, start=64, end=76),\n",
       " HTMLTagWithIndices(tag=<span notation=\"\">$\\operatorname{Gal}(L/K)$</span>, start=77, end=102),\n",
       " HTMLTagWithIndices(tag=<b definition=\"\">Galois group</b>, start=330, end=342),\n",
       " HTMLTagWithIndices(tag=<span notation=\"\">$\\operatorname{Gal}(L/K)$</span>, start=343, end=368)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html_data['raw_text']\n",
    "html_data[\"tags\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {\n",
    "    \"O\": 0,\n",
    "    \"B-definition\": 1,\n",
    "    \"I-definition\": 2,\n",
    "    \"B-notation\": 3,\n",
    "    \"I-notation\": 4\n",
    "}\n",
    "tokens, ner_tag_ids = tokenize_html_data(html_data, tokenizer, 510, def_or_notat_from_html_tag, label2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, `max_length` is set to 510 (tokens). The string (\"Raw text\") is not very long, so only one sequence should be present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(len(tokens), 1)\n",
    "test_eq(len(ner_tag_ids), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us see what has been tagged:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'O',\n",
       " 1: 'B-definition',\n",
       " 2: 'I-definition',\n",
       " 3: 'B-notation',\n",
       " 4: 'I-notation'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label = {value: key for key, value in label2id.items()}\n",
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gal\t\tB-definition\n",
      "##ois\t\tI-definition\n",
      "group\t\tI-definition\n",
      "$\t\tB-notation\n",
      "\\\t\tI-notation\n",
      "operator\t\tI-notation\n",
      "##name\t\tI-notation\n",
      "{\t\tI-notation\n",
      "gal\t\tI-notation\n",
      "}\t\tI-notation\n",
      "(\t\tI-notation\n",
      "l\t\tI-notation\n",
      "/\t\tI-notation\n",
      "k\t\tI-notation\n",
      ")\t\tI-notation\n",
      "$\t\tI-notation\n",
      "gal\t\tB-definition\n",
      "##ois\t\tI-definition\n",
      "group\t\tI-definition\n",
      "$\t\tB-notation\n",
      "\\\t\tI-notation\n",
      "operator\t\tI-notation\n",
      "##name\t\tI-notation\n",
      "{\t\tI-notation\n",
      "gal\t\tI-notation\n",
      "}\t\tI-notation\n",
      "(\t\tI-notation\n",
      "l\t\tI-notation\n",
      "/\t\tI-notation\n",
      "k\t\tI-notation\n",
      ")\t\tI-notation\n",
      "$\t\tI-notation\n"
     ]
    }
   ],
   "source": [
    "for token, ner_tag in zip(tokens[0], ner_tag_ids[0]):\n",
    "    if ner_tag != 0:\n",
    "        print(f\"{token}\\t\\t{id2label[ner_tag]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us set `max_length` to be shorter to observe an example of a tokenization of a single text across multiple sequences (Of course, in practice, the max token length would be set to be longer, say around 512 or 1024.):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids, ner_tag_ids = tokenize_html_data(html_data, tokenizer, 20, def_or_notat_from_html_tag, label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "print(len(token_ids))\n",
    "print(len(ner_tag_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2],\n",
       " [2, 2, 2, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 3, 4, 4],\n",
       " [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0]]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_tag_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Data from standalone text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, data can be gathered from a str with definition and notations marked with some general formatting, as long as a parser for those markings is specified. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _calculate_clean_indices_and_create_tags(\n",
    "        original_text: str,\n",
    "        markings: list[tuple[str, int, int, dict]]\n",
    "        ) -> tuple[str, list[HTMLTagWithIndices]]:\n",
    "    \"\"\"Helper: returns (clean_text, list_of_tags).\"\"\"\n",
    "    results = []\n",
    "    clean_text_parts = []\n",
    "    current_raw_idx = 0\n",
    "    clean_text_len = 0\n",
    "    soup = bs4.BeautifulSoup(\"\", 'html.parser')\n",
    "\n",
    "    for content, raw_start, raw_end, attrs in markings:\n",
    "        # Append text BEFORE the mark\n",
    "        pre_text = original_text[current_raw_idx:raw_start]\n",
    "        clean_text_parts.append(pre_text)\n",
    "        \n",
    "        pre_text_len = len(pre_text)\n",
    "        clean_text_len += pre_text_len\n",
    "        \n",
    "        # Calculate indices\n",
    "        tag_start = clean_text_len\n",
    "        tag_end = tag_start + len(content)\n",
    "        \n",
    "        # Create Tag\n",
    "        tag_name = \"b\" if \"definition\" in attrs else \"span\"\n",
    "        tag = soup.new_tag(tag_name, **attrs)\n",
    "        tag.string = content\n",
    "        results.append(HTMLTagWithIndices(tag, tag_start, tag_end))\n",
    "        \n",
    "        # Append the CONTENT of the mark (clean text)\n",
    "        clean_text_parts.append(content)\n",
    "        \n",
    "        clean_text_len += len(content)\n",
    "        current_raw_idx = raw_end\n",
    "\n",
    "    # Append remaining text\n",
    "    clean_text_parts.append(original_text[current_raw_idx:])\n",
    "    \n",
    "    return \"\".join(clean_text_parts), results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def extract_html_tag_indices_from_marked_text(\n",
    "        text: str, # The text containing custom markings (e.g. \"Let [NOT:G] be a [DEF:group]\").\n",
    "        marker_parser: Callable[[str], list[tuple[str, int, int, dict]]] # A function that parses the text and returns a list of tuples. Each tuple should contain: (1) The *inner content* of the marked section, (2) The *start index* of the marking in `text`, (3) The *end index* of the marking in `text`, and (4) A dictionary of *attributes* for the HTML tag.\n",
    "        ) -> list[HTMLTagWithIndices]: # A list of `HTMLTagWithIndices` objects. The start/end indices corresponds to the *inner content's* location in a \"clean\" version of the text (where markings are removed).\n",
    "    \"\"\"\n",
    "    Extracts a list of HTML tags and their indices from marked text.\n",
    "    \"\"\"\n",
    "    markings = marker_parser(text)\n",
    "    markings.sort(key=lambda x: x[1])\n",
    "    _, tags = _calculate_clean_indices_and_create_tags(text, markings)\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "html_tag_data_from_marked_text (Galois example) passed!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def bracket_parser(text: str) -> list[tuple[str, int, int, dict]]:\n",
    "    \"\"\"\n",
    "    Parses marks like [DEF:content] and [NOT:content].\n",
    "    Returns list of (content, start, end, attrs).\n",
    "    \"\"\"\n",
    "    # Regex to find [TYPE:content]\n",
    "    # Note: Using non-greedy match (.*?) to handle multiple brackets correctly\n",
    "    pattern = re.compile(r'\\[(DEF|NOT):(.*?)]')\n",
    "    \n",
    "    results = []\n",
    "    for match in pattern.finditer(text):\n",
    "        tag_type = match.group(1)\n",
    "        content = match.group(2)\n",
    "        start, end = match.span()\n",
    "        \n",
    "        attrs = {}\n",
    "        if tag_type == 'DEF':\n",
    "            attrs['definition'] = ''\n",
    "        else:\n",
    "            attrs['notation'] = ''\n",
    "            \n",
    "        results.append((content, start, end, attrs))\n",
    "    return results\n",
    "\n",
    "# --- Test Setup ---\n",
    "# Marked Text: \"The [DEF:Galois group] [NOT:$\\operatorname{Gal}(L/K)$] is...\"\n",
    "marked_text = r\"The [DEF:Galois group] [NOT:$\\operatorname{Gal}(L/K)$] is...\"\n",
    "\n",
    "# \"Clean\" Text (what indices refer to): \n",
    "# \"The Galois group $\\operatorname{Gal}(L/K)$ is...\"\n",
    "# Indices Breakdown:\n",
    "# \"The \" -> 0-4\n",
    "# \"Galois group\" -> 4-16 (Length 12)\n",
    "# \" \" -> 16-17\n",
    "# \"$\\operatorname{Gal}(L/K)$\" -> 17-42 (Length 25)\n",
    "# \" is...\" -> 42...\n",
    "\n",
    "tag_data = extract_html_tag_indices_from_marked_text(marked_text, bracket_parser)\n",
    "\n",
    "# --- Assertions ---\n",
    "assert len(tag_data) == 2, f\"Expected 2 tags, got {len(tag_data)}\"\n",
    "\n",
    "# 1. Check Definition Tag (\"Galois group\")\n",
    "defn = tag_data[0]\n",
    "assert defn.tag.string == \"Galois group\"\n",
    "assert \"definition\" in defn.tag.attrs\n",
    "assert defn.start == 4\n",
    "assert defn.end == 16, f\"Expected end 16, got {defn.end}\"\n",
    "\n",
    "# 2. Check Notation Tag (\"$\\operatorname{Gal}(L/K)$\")\n",
    "notat = tag_data[1]\n",
    "expected_math = r\"$\\operatorname{Gal}(L/K)$\"\n",
    "assert notat.tag.string == expected_math\n",
    "assert \"notation\" in notat.tag.attrs\n",
    "assert notat.start == 17, f\"Expected start 17, got {notat.start}\"\n",
    "assert notat.end == 42, f\"Expected end 42, got {notat.end}\"\n",
    "\n",
    "print(\"html_tag_data_from_marked_text (Galois example) passed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def html_data_from_marked_text(\n",
    "        text: str, # The text containing custom markings (e.g. \"Let [NOT:G] be a [DEF:group]\").\n",
    "        marker_parser: Callable[[str], list[tuple[str, int, int, dict]]] # A function that parses the text and returns a list of tuples. Each tuple should contain: (1) The *inner content* of the marked section, (2) The *start index* of the marking in `text`, (3) The *end index* of the marking in `text`, and (4) A dictionary of *attributes* for the HTML tag.\n",
    "        ) -> StrAndHTMLTagsWithIndices: # An object containing the \"clean\" text (markings removed) and the list of HTML tags with their indices in that clean text.\n",
    "    \"\"\"\n",
    "    Creates an `StrAndHTMLTagsWithIndices` object from text with custom markings.\n",
    "    \"\"\"\n",
    "    markings = marker_parser(text)\n",
    "    markings.sort(key=lambda x: x[1])\n",
    "    clean_text, tags = _calculate_clean_indices_and_create_tags(text, markings)\n",
    "    \n",
    "    return StrAndHTMLTagsWithIndices(clean_text, tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StrAndHTMLTagsWithIndices(raw_text='The Galois group $\\\\operatorname{Gal}(L/K)$ is...', tags=[HTMLTagWithIndices(tag=<b definition=\"\">Galois group</b>, start=4, end=16), HTMLTagWithIndices(tag=<span notation=\"\">$\\operatorname{Gal}(L/K)$</span>, start=17, end=42)])\n",
      "html_data_from_marked_text (Galois example) passed!\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "import re\n",
    "\n",
    "def bracket_parser(text: str) -> list[tuple[str, int, int, dict]]:\n",
    "    \"\"\"Parses [DEF:content] and [NOT:content].\"\"\"\n",
    "    pattern = re.compile(r'\\[(DEF|NOT):(.*?)]')\n",
    "    results = []\n",
    "    for match in pattern.finditer(text):\n",
    "        tag_type = match.group(1)\n",
    "        content = match.group(2)\n",
    "        start, end = match.span()\n",
    "        attrs = {'definition' if tag_type == 'DEF' else 'notation': ''}\n",
    "        results.append((content, start, end, attrs))\n",
    "    return results\n",
    "\n",
    "# --- Test Setup ---\n",
    "# Marked Text: \"The [DEF:Galois group] [NOT:$\\operatorname{Gal}(L/K)$] is...\"\n",
    "marked_text = r\"The [DEF:Galois group] [NOT:$\\operatorname{Gal}(L/K)$] is...\"\n",
    "\n",
    "# Expected \"Clean\" Text (markings stripped, content kept)\n",
    "expected_clean_text = r\"The Galois group $\\operatorname{Gal}(L/K)$ is...\"\n",
    "\n",
    "# --- Run Function ---\n",
    "data_obj = html_data_from_marked_text(marked_text, bracket_parser)\n",
    "\n",
    "print(data_obj)\n",
    "# --- Assertions ---\n",
    "\n",
    "# 1. Check Clean Text\n",
    "assert data_obj.raw_text == expected_clean_text, \\\n",
    "    f\"Clean text mismatch.\\nExpected: '{expected_clean_text}'\\nGot:      '{data_obj.raw_text}'\"\n",
    "\n",
    "# 2. Check Tag Count\n",
    "assert len(data_obj.tags) == 2, f\"Expected 2 tags, got {len(data_obj.tags)}\"\n",
    "\n",
    "# 3. Check Definition Tag (\"Galois group\")\n",
    "# Indices in clean text: \"The \" (4) -> start 4\n",
    "# \"Galois group\" (length 12) -> end 16\n",
    "defn = data_obj.tags[0]\n",
    "assert defn.tag.string == \"Galois group\"\n",
    "assert \"definition\" in defn.tag.attrs\n",
    "assert defn.start == 4\n",
    "assert defn.end == 16\n",
    "\n",
    "# 4. Check Notation Tag (\"$\\operatorname{Gal}(L/K)$\")\n",
    "# Indices in clean text:\n",
    "# \"The Galois group \" (17 chars: 4 + 12 + 1 space) -> start 17?\n",
    "# Wait: \"The \" (4) + \"Galois group\" (12) + \" \" (1) = 17.\n",
    "# So Notation starts at 17.\n",
    "# Content: \"$\\operatorname{Gal}(L/K)$\" (Length 25)\n",
    "# End: 17 + 25 = 42.\n",
    "notat = data_obj.tags[1]\n",
    "expected_math = r\"$\\operatorname{Gal}(L/K)$\"\n",
    "assert notat.tag.string == expected_math\n",
    "assert \"notation\" in notat.tag.attrs\n",
    "assert notat.start == 17\n",
    "assert notat.end == 42\n",
    "\n",
    "print(\"html_data_from_marked_text (Galois example) passed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Miscellaneous parse formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is intended to parse latex code of the author's writing for definition and notation markings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def latex_highlight_parser(text: str) -> list[tuple[str, int, int, dict]]:\n",
    "    \"\"\"\n",
    "    Parses LaTeX highlighting commands (\\\\hldef, \\\\hl, \\\\hlin, \\\\hlalign) to identify\n",
    "    definitions and notations.\n",
    "    \n",
    "    This function is designed to be passed as the `marker_parser` argument to\n",
    "    `html_data_from_marked_text`.\n",
    "\n",
    "    It handles nested braces using recursive regex and strips surrounding whitespace \n",
    "    from the inner content (e.g., stripping padding newlines in `\\\\hlalign{\\\\n ... \\\\n}`).\n",
    "    It also implements special logic for `\\\\hlin` to expand the notation range to \n",
    "    include surrounding `$$` delimiters if present.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[tuple[str, int, int, dict]]\n",
    "        A list of tuples representing the found markings. Each tuple contains:\n",
    "        1. **Content** (`str`): The inner text of the marking (stripped of padding whitespace).\n",
    "           This is the text that will remain in the \"clean\" output and be wrapped by the tag.\n",
    "        2. **Start Index** (`int`): The start index of the *entire marking wrapper* \n",
    "           (e.g., the index of `\\\\`) in the original `text`.\n",
    "        3. **End Index** (`int`): The end index of the *entire marking wrapper* \n",
    "           (e.g., the index after `}`) in the original `text`.\n",
    "        4. **Attributes** (`dict`): A dictionary of HTML attributes, e.g. \n",
    "           `{'definition': ''}` or `{'notation': ''}`.\n",
    "    \"\"\"\n",
    "    pattern = r'\\\\(hldef|hlalign|hlin|hl)\\s*(\\{(?:[^{}]++|(?2))*\\})'\n",
    "    results = []\n",
    "    \n",
    "    for match in regex.finditer(pattern, text):\n",
    "        cmd_type = match.group(1)\n",
    "        full_brace_group = match.group(2)\n",
    "        raw_inner = full_brace_group[1:-1]\n",
    "        match_start, match_end = match.span()\n",
    "        \n",
    "        # Strip padding whitespace from content\n",
    "        lstripped = raw_inner.lstrip()\n",
    "        stripped = lstripped.rstrip()\n",
    "        content = stripped\n",
    "        \n",
    "        start = match_start\n",
    "        end = match_end\n",
    "        attrs = {'definition' if cmd_type == 'hldef' else 'notation': ''}\n",
    "        \n",
    "        if cmd_type == 'hlin':\n",
    "            # 1. Scan Backwards for start $$\n",
    "            p = start - 1\n",
    "            while p >= 0 and text[p].isspace(): p -= 1\n",
    "            \n",
    "            # Check if we hit $$ immediately (ignoring space)\n",
    "            if p >= 1 and text[p] == '$' and text[p-1] == '$':\n",
    "                found_start_dollars = True\n",
    "                new_start = p - 1\n",
    "                # BUG WAS HERE: prefix = \"\" \n",
    "                # FIX: Capture the text between $$ (p+1) and \\hlin (start)\n",
    "                prefix = text[p+1 : start] \n",
    "            else:\n",
    "                found_start_dollars = False\n",
    "                prefix = \"\" # Initialize for safety\n",
    "\n",
    "            \n",
    "            # 2. Scan Forwards for end $$\n",
    "            # We want to allow punctuation like \".\" or \",\" between } and $$\n",
    "            # E.g. $$ \\hlin{x}. $$\n",
    "            \n",
    "            q = end\n",
    "            suffix = \"\"\n",
    "            found_end_dollars = False\n",
    "            \n",
    "            # Heuristic: Scan forward for a limited distance or until $$\n",
    "            # We capture everything between } and $$ into 'suffix'\n",
    "            # Stop if we hit a newline (safeguard)\n",
    "            temp_q = q\n",
    "            while temp_q < len(text) - 1:\n",
    "                if text[temp_q] == '\\n': break\n",
    "                \n",
    "                if text[temp_q] == '$' and text[temp_q+1] == '$':\n",
    "                    found_end_dollars = True\n",
    "                    new_end = temp_q + 2\n",
    "                    # The text between original end and $$ is the suffix\n",
    "                    suffix = text[end:temp_q]\n",
    "                    # Clean the suffix? Usually we just want to include it.\n",
    "                    # e.g. suffix might be \". \" (period and space)\n",
    "                    # We usually trim the space before the $$, but keeping it is safer for fidelity.\n",
    "                    break\n",
    "                temp_q += 1\n",
    "            \n",
    "            # Only apply expansion if we found BOTH start and end $$\n",
    "            # AND (optionally) if we found the start $$ directly. \n",
    "            # (Does it make sense to have content BEFORE \\hlin? e.g. $$ x = \\hlin{y} $$?\n",
    "            #  If so, we should scan backwards for $$ similarly to how we scanned forwards).\n",
    "            \n",
    "            # Let's implement symmetric scanning for robustness.\n",
    "            \n",
    "            # RE-TRY Backward Scan with content capture\n",
    "            if not found_start_dollars:\n",
    "                 temp_p = start - 1\n",
    "                 while temp_p >= 1:\n",
    "                     if text[temp_p] == '\\n': break\n",
    "                     if text[temp_p] == '$' and text[temp_p-1] == '$':\n",
    "                         found_start_dollars = True\n",
    "                         new_start = temp_p - 1\n",
    "                         prefix = text[temp_p+1 : start] # Content between $$ and \\hlin\n",
    "                         break\n",
    "                     temp_p -= 1\n",
    "\n",
    "            if found_start_dollars and found_end_dollars:\n",
    "                # We found $$ ... \\hlin{...} ... $$\n",
    "                # We want the tag to cover the WHOLE thing: \"$$ prefix content suffix $$\"\n",
    "                # And we want to remove the WHOLE thing from the text.\n",
    "                \n",
    "                # Careful: The 'prefix' and 'suffix' currently contain the raw characters \n",
    "                # from the marked text (including potentially ignored spaces).\n",
    "                # We should probably strip excessive padding next to the $$ inside the tag?\n",
    "                # Standard convention: $$ content $$ -> Tag content \"$$ content $$\"\n",
    "                \n",
    "                # Let's just assemble it raw to preserve user's punctuation/spacing logic,\n",
    "                # then maybe strip outer edges if needed.\n",
    "                \n",
    "                # Current 'content' is stripped inner content of \\hlin.\n",
    "                # 'prefix' is text between $$ and \\hlin.\n",
    "                # 'suffix' is text between \\hlin and $$.\n",
    "                \n",
    "                # Example: $$ \\hlin{K}. $$\n",
    "                # prefix = \" \"\n",
    "                # content = \"K\"\n",
    "                # suffix = \". \"\n",
    "                # Result: \"$$ K. $$\"\n",
    "                \n",
    "                full_content = f\"$${prefix}{content}{suffix}$$\"\n",
    "                \n",
    "                content = full_content\n",
    "                start = new_start\n",
    "                end = new_end\n",
    "\n",
    "        results.append((content, start, end, attrs))\n",
    "        \n",
    "    return results\n",
    "    # pattern = r'\\\\(hldef|hlalign|hlin|hl)\\s*(\\{(?:[^{}]++|(?2))*\\})'\n",
    "    \n",
    "    # results = []\n",
    "    \n",
    "    # for match in regex.finditer(pattern, text):\n",
    "    #     cmd_type = match.group(1)\n",
    "    #     full_brace_group = match.group(2)\n",
    "        \n",
    "    #     # Raw content inside braces (e.g. \" \\n Content \\n \")\n",
    "    #     raw_inner = full_brace_group[1:-1]\n",
    "        \n",
    "    #     # Calculate indices relative to the 'text' string\n",
    "    #     # Match span covers `\\hl{...}`\n",
    "    #     match_start, match_end = match.span()\n",
    "        \n",
    "    #     # We want to identify where the \"real\" content starts/ends inside the match\n",
    "    #     # Start of raw_inner is: match_end - 1 (closing brace) - len(raw_inner)\n",
    "    #     # Actually easier: find start of {\n",
    "    #     brace_start_idx = match.start(2) # Index of {\n",
    "    #     inner_start_idx = brace_start_idx + 1\n",
    "        \n",
    "    #     # Find leading/trailing whitespace length\n",
    "    #     lstripped = raw_inner.lstrip()\n",
    "    #     leading_ws_len = len(raw_inner) - len(lstripped)\n",
    "        \n",
    "    #     stripped = lstripped.rstrip()\n",
    "    #     trailing_ws_len = len(lstripped) - len(stripped)\n",
    "        \n",
    "    #     content = stripped\n",
    "    #     attrs = {'definition' if cmd_type == 'hldef' else 'notation': ''}\n",
    "        \n",
    "    #     # 1. Base Range: The command wrapper `\\cmd{` and `}`.\n",
    "    #     # We want to effectively say:\n",
    "    #     # \"Remove `\\cmd{ \\n`\", keep `Content`, \"Remove `\\n }`\".\n",
    "    #     # But our interface only supports \"Remove `Range`, Insert `Content`\".\n",
    "        \n",
    "    #     # If we return `start=match_start`, `end=match_end`, `content=stripped`:\n",
    "    #     # \"Remove `\\hl{ \\n Content \\n }`. Insert `Content`.\"\n",
    "    #     # Result Clean Text: `Content`. (Newlines LOST).\n",
    "        \n",
    "    #     # If the user WANTS those newlines preserved in the clean text (outside the tag),\n",
    "    #     # we have a problem. The current architecture assumes \"Marked Region\" -> \"Tag\".\n",
    "    #     # It doesn't support \"Marked Region\" -> \"Prefix + Tag + Suffix\".\n",
    "        \n",
    "    #     # DECISION: \n",
    "    #     # For `\\hlalign`, users usually write:\n",
    "    #     # \\hlalign{\n",
    "    #     # \\begin{align}\n",
    "    #     # ...\n",
    "    #     # \\end{align}\n",
    "    #     # }\n",
    "    #     # They EXPECT the clean text to contain the newlines so the align renders correctly?\n",
    "    #     # Actually, LaTeX doesn't care about the surrounding newlines much.\n",
    "    #     # `\\begin{align}...\\end{align}` is valid without extra newlines.\n",
    "    #     # So losing the newlines inside the braces is probably ACCEPTABLE and cleaner.\n",
    "        \n",
    "    #     # HOWEVER, if `$$ \\hlin{ x } $$` -> `$$x$$`. Losing spaces is fine.\n",
    "        \n",
    "    #     # Special Logic for \\hlin ($$) remains...\n",
    "        \n",
    "    #     # Let's apply the stripping logic:\n",
    "        \n",
    "    #     start = match_start\n",
    "    #     end = match_end\n",
    "        \n",
    "    #     if cmd_type == 'hlin':\n",
    "    #         # ... (Logic to expand to $$ remains same, but apply to `stripped` content) ...\n",
    "    #         # Re-implementing simplified logic for clarity in this snippet:\n",
    "    #         p = start - 1\n",
    "    #         while p >= 0 and text[p].isspace(): p -= 1\n",
    "    #         if p >= 1 and text[p] == '$' and text[p-1] == '$':\n",
    "    #             new_start = p - 1\n",
    "    #             q = end\n",
    "    #             while q < len(text) and text[q].isspace(): q += 1\n",
    "    #             if q < len(text) - 1 and text[q] == '$' and text[q+1] == '$':\n",
    "    #                 new_end = q + 2\n",
    "    #                 content = f\"$${content}$$\"\n",
    "    #                 start = new_start\n",
    "    #                 end = new_end\n",
    "\n",
    "    #     results.append((content, start, end, attrs))\n",
    "        \n",
    "    # return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latex_highlight_parser test passed!\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# --- Setup ---\n",
    "# Input Text with mixed highlighting\n",
    "# 1. Definition: \\hldef{Galois group}\n",
    "# 2. Inline Notation: \\hl{$\\operatorname{Gal}(L/K)$}\n",
    "# 3. Display Notation: $$\\hlin{x^2}$$\n",
    "marked_text = r\"The \\hldef{Galois group} \\hl{$\\operatorname{Gal}(L/K)$} is defined. Display: $$\\hlin{x^2}$$\"\n",
    "\n",
    "# Expected Clean Text\n",
    "expected_clean = r\"The Galois group $\\operatorname{Gal}(L/K)$ is defined. Display: $$x^2$$\"\n",
    "\n",
    "# --- Run ---\n",
    "data_obj = html_data_from_marked_text(marked_text, latex_highlight_parser)\n",
    "\n",
    "# --- Assertions ---\n",
    "\n",
    "# 1. Check Clean Text\n",
    "assert data_obj.raw_text == expected_clean, \\\n",
    "    f\"Clean text mismatch.\\nExpected: {expected_clean}\\nGot:      {data_obj.raw_text}\"\n",
    "\n",
    "# 2. Check Tag Count (3 tags)\n",
    "assert len(data_obj.tags) == 3\n",
    "\n",
    "# 3. Check Definition\n",
    "defn = data_obj.tags[0]\n",
    "assert defn.tag.string == \"Galois group\"\n",
    "assert \"definition\" in defn.tag.attrs\n",
    "# \"The \" (4) -> start 4\n",
    "assert defn.start == 4\n",
    "\n",
    "# 4. Check Inline Notation\n",
    "notat_inline = data_obj.tags[1]\n",
    "assert notat_inline.tag.string == r\"$\\operatorname{Gal}(L/K)$\"\n",
    "assert \"notation\" in notat_inline.tag.attrs\n",
    "# \"The Galois group \" (17) -> start 17\n",
    "assert notat_inline.start == 17\n",
    "\n",
    "# 5. Check Display Notation\n",
    "# Clean text segment: \" is defined. Display: $$\"\n",
    "# \"The Galois group $\\operatorname{Gal}(L/K)$\" (length 17 + 25 = 42)\n",
    "# \" is defined. Display: $$\" (length 22)\n",
    "# Start index should be 42 + 22 = 64\n",
    "notat_display = data_obj.tags[2]\n",
    "assert notat_display.tag.string == \"$$x^2$$\"\n",
    "assert \"notation\" in notat_display.tag.attrs\n",
    "\n",
    "print(\"latex_highlight_parser test passed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Galois group $\\operatorname{Gal}(L/K)$ is defined. Display: $$x^2$$\n",
      "[HTMLTagWithIndices(tag=<b definition=\"\">Galois group</b>, start=4, end=16), HTMLTagWithIndices(tag=<span notation=\"\">$\\operatorname{Gal}(L/K)$</span>, start=17, end=42), HTMLTagWithIndices(tag=<span notation=\"\">$$x^2$$</span>, start=64, end=71)]\n"
     ]
    }
   ],
   "source": [
    "print(data_obj.raw_text)\n",
    "print(data_obj.tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robust parser tests passed!\n",
      "StrAndHTMLTagsWithIndices(raw_text='Set $\\\\{x\\\\}$. Display: $$x^2$$ Align: \\\\begin{align}a&=b\\\\end{align}', tags=[HTMLTagWithIndices(tag=<span notation=\"\">$\\{x\\}$</span>, start=4, end=11), HTMLTagWithIndices(tag=<span notation=\"\">$$x^2$$</span>, start=22, end=29), HTMLTagWithIndices(tag=<span notation=\"\">\\begin{align}a&amp;=b\\end{align}</span>, start=37, end=65)])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Setup ---\n",
    "# 1. Nested Braces: \\hl{$\\{x\\}$}\n",
    "# 2. Display Math: $$\\hlin{x^2}$$\n",
    "# 3. Align: \\hlalign{\\begin{align}a&=b\\end{align}}\n",
    "\n",
    "marked_text = (\n",
    "    r\"Set \\hl{$\\{x\\}$}. \"\n",
    "    r\"Display: $$\\hlin{x^2}$$ \"\n",
    "    r\"Align: \\hlalign{\\begin{align}a&=b\\end{align}}\"\n",
    ")\n",
    "\n",
    "# Expected Clean Text\n",
    "# 1. $\\begin{Bmatrix}x\\end{Bmatrix}$ (restored)\n",
    "# 2. $$x^2$$ (restored, tag wraps entire thing)\n",
    "# 3. \\begin{align}a&=b\\end{align} (restored)\n",
    "expected_clean = (\n",
    "    r\"Set $\\{x\\}$. \"\n",
    "    r\"Display: $$x^2$$ \"\n",
    "    r\"Align: \\begin{align}a&=b\\end{align}\"\n",
    ")\n",
    "\n",
    "# --- Run ---\n",
    "data_obj = html_data_from_marked_text(marked_text, latex_highlight_parser)\n",
    "\n",
    "# --- Assertions ---\n",
    "# 1. Clean Text\n",
    "assert data_obj.raw_text == expected_clean, \\\n",
    "    f\"Clean text mismatch.\\nExpected: '{expected_clean}'\\nGot:      '{data_obj.raw_text}'\"\n",
    "\n",
    "# 2. Nested Brace Tag\n",
    "tag_nested = data_obj.tags[0]\n",
    "assert tag_nested.tag.string == r\"$\\{x\\}$\"\n",
    "assert \"notation\" in tag_nested.tag.attrs\n",
    "\n",
    "# 3. Display Math Tag\n",
    "# Should wrap \"$$x^2$$\" because parser expanded to include $$\n",
    "tag_display = data_obj.tags[1]\n",
    "assert tag_display.tag.string == r\"$$x^2$$\"\n",
    "assert \"notation\" in tag_display.tag.attrs\n",
    "\n",
    "# 4. Align Tag\n",
    "tag_align = data_obj.tags[2]\n",
    "assert tag_align.tag.string == r\"\\begin{align}a&=b\\end{align}\"\n",
    "assert \"notation\" in tag_align.tag.attrs\n",
    "\n",
    "print(\"Robust parser tests passed!\")\n",
    "print(data_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiline hlalign padding test passed!\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "#| notest\n",
    "\n",
    "# --- Setup ---\n",
    "# Input has padding newlines inside the braces:\n",
    "# {\n",
    "#   \\begin{align} ... \\end{align}\n",
    "# }\n",
    "marked_text = r\"\"\"\n",
    "Here is some text.\n",
    "\\hlalign{\n",
    "\\begin{align}\n",
    "asdf\n",
    "\\end{align}\n",
    "}\n",
    "End of text.\n",
    "\"\"\"\n",
    "\n",
    "# Expected Clean Text:\n",
    "# The `\\hlalign{` and `}` are removed.\n",
    "# Crucially, the padding newlines inside the braces are ALSO removed by the strip() logic.\n",
    "# So we expect NO newline between \"Here is some text.\" and \"\\begin{align}\",\n",
    "# unless there was one outside the command (which there is: line 2 has a newline).\n",
    "#\n",
    "# Original:\n",
    "# Line 1: \"Here is some text.\"\n",
    "# Line 2: \"\\hlalign{\" -> Replaced by \"\\begin{align}\" (start of content)\n",
    "# ... content ...\n",
    "# Line 6: \"}\" -> Replaced by \"\" (end of content)\n",
    "# Line 7: \"End of text.\"\n",
    "\n",
    "# So the clean text will effectively collapse the structure slightly.\n",
    "# \"Here is some text.\\n\\begin{align}\\nasdf\\n\\end{align}\\nEnd of text.\"\n",
    "expected_clean = (\n",
    "    \"Here is some text.\\n\"\n",
    "    r\"\\begin{align}\" + \"\\n\"\n",
    "    r\"asdf\" + \"\\n\"\n",
    "    r\"\\end{align}\" + \"\\n\"\n",
    "    \"End of text.\"\n",
    ")\n",
    "\n",
    "# --- Run ---\n",
    "data_obj = html_data_from_marked_text(marked_text, latex_highlight_parser)\n",
    "\n",
    "# --- Assertions ---\n",
    "\n",
    "# 1. Check Clean Text\n",
    "# If the parser failed to strip padding, we would see extra newlines (e.g. \\n\\n\\begin...)\n",
    "assert data_obj.raw_text.strip() == expected_clean.strip(), \\\n",
    "    f\"Clean text mismatch.\\nExpected:\\n{repr(expected_clean)}\\nGot:\\n{repr(data_obj.raw_text)}\"\n",
    "\n",
    "# 2. Check Tag Content\n",
    "tag_align = data_obj.tags[0]\n",
    "tag_content = tag_align.tag.string\n",
    "\n",
    "# Verify NO leading/trailing newlines in the tag content itself\n",
    "# (It should start immediately with \\begin and end with \\end)\n",
    "assert tag_content.startswith(r\"\\begin{align}\"), f\"Tag content has leading garbage: {repr(tag_content[:20])}\"\n",
    "assert tag_content.endswith(r\"\\end{align}\"), f\"Tag content has trailing garbage: {repr(tag_content[-20:])}\"\n",
    "\n",
    "# Verify INTERNAL newlines are preserved (between begin and asdf)\n",
    "assert \"\\nasdf\\n\" in tag_content, \"Internal newlines were incorrectly stripped!\"\n",
    "\n",
    "print(\"Multiline hlalign padding test passed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some examples of `latex_highlight_parser` in the context of `html_data_from_marked_text`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1 Passed\n"
     ]
    }
   ],
   "source": [
    "# from trouver.machinelearning.tokenize.def_and_notat_token_classification import (\n",
    "#     html_data_from_marked_text, \n",
    "#     latex_highlight_parser\n",
    "# )\n",
    "\n",
    "# Input text with marking commands\n",
    "marked_text = r\"The \\hldef{Galois group} \\hl{$\\operatorname{Gal}(L/K)$} is defined.\"\n",
    "\n",
    "# Process the text\n",
    "data = html_data_from_marked_text(marked_text, latex_highlight_parser)\n",
    "\n",
    "# 1. Clean Text Verification\n",
    "# The commands \\hldef{...} and \\hl{...} are stripped, leaving only the content.\n",
    "expected_clean = r\"The Galois group $\\operatorname{Gal}(L/K)$ is defined.\"\n",
    "assert data.raw_text == expected_clean\n",
    "\n",
    "# 2. Tag Verification\n",
    "# Tag 1: Definition\n",
    "def_tag = data.tags[0]\n",
    "assert def_tag.tag.string == \"Galois group\"\n",
    "assert \"definition\" in def_tag.tag.attrs\n",
    "# \"The \" is 4 chars long, so start is 4.\n",
    "assert def_tag.start == 4 \n",
    "assert def_tag.end == 4 + len(\"Galois group\")\n",
    "\n",
    "# Tag 2: Notation\n",
    "not_tag = data.tags[1]\n",
    "assert not_tag.tag.string == r\"$\\operatorname{Gal}(L/K)$\"\n",
    "assert \"notation\" in not_tag.tag.attrs\n",
    "# Starts after \"The Galois group \" (17 chars)\n",
    "assert not_tag.start == 17\n",
    "\n",
    "print(\"Example 1 Passed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 2 Passed\n"
     ]
    }
   ],
   "source": [
    "# Input: Display math where \\hlin wraps the inner equation\n",
    "# Note: The parser handles the spaces around the `\\hlin` command inside the $$\n",
    "marked_text = r\"The kernel is trivial: $$ \\hlin{ K = \\{e\\} } $$\"\n",
    "\n",
    "data = html_data_from_marked_text(marked_text, latex_highlight_parser)\n",
    "\n",
    "# 1. Clean Text Verification\n",
    "# The result should look like standard LaTeX display math.\n",
    "# The spaces inside the braces are stripped by the parser, but the spaces\n",
    "# between $$ and the content depend on how we reconstruct it.\n",
    "# Our parser returns content \"$$K = \\{e\\}$$\" (with $$ added).\n",
    "# And it consumes the original \"$$ \\hlin{...} $$\".\n",
    "# So the clean text is just the content inserted at that spot.\n",
    "expected_clean = r\"The kernel is trivial: $$ K = \\{e\\} $$\"\n",
    "assert data.raw_text == expected_clean\n",
    "\n",
    "# 2. Tag Verification\n",
    "tag = data.tags[0]\n",
    "# The tag should wrap the WHOLE math block, including $$\n",
    "assert tag.tag.string == r\"$$ K = \\{e\\} $$\"\n",
    "assert \"notation\" in tag.tag.attrs\n",
    "\n",
    "print(\"Example 2 Passed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Typo/Punctuation inside $$ test passed!\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "\n",
    "# --- Setup ---\n",
    "# Case: Punctuation inside $$ but outside \\hlin\n",
    "marked_text = r\"The kernel is trivial: $$ \\hlin{ K = \\{e\\} }. $$\"\n",
    "\n",
    "data = html_data_from_marked_text(marked_text, latex_highlight_parser)\n",
    "\n",
    "# 1. Clean Text Verification\n",
    "# Original text logic would normally produce:\n",
    "# \"The kernel is trivial: $$ K = \\{e\\} . $$\" (spaces preserved from prefix/suffix)\n",
    "#\n",
    "# Let's trace the parser:\n",
    "# prefix = \" \" (between $$ and \\hlin)\n",
    "# content = \"K = \\{e\\}\"\n",
    "# suffix = \". \" (between } and $$)\n",
    "# result = \"$$ K = \\{e\\}. $$\"\n",
    "expected_clean = r\"The kernel is trivial: $$ K = \\{e\\}. $$\"\n",
    "\n",
    "assert data.raw_text == expected_clean, \\\n",
    "    f\"Clean Text Mismatch.\\nExp: '{expected_clean}'\\nGot: '{data.raw_text}'\"\n",
    "\n",
    "# 2. Tag Verification\n",
    "tag = data.tags[0]\n",
    "tag_str = tag.tag.string\n",
    "\n",
    "# Ensure the tag wraps the whole $$ block\n",
    "assert tag_str.startswith(\"$$\")\n",
    "assert tag_str.endswith(\"$$\")\n",
    "assert r\"K = \\{e\\}\" in tag_str\n",
    "# Ensure the period is INSIDE the tag\n",
    "assert \".\" in tag_str\n",
    "# Ensure the tag covers the full range\n",
    "assert tag.start == 23 # Index of $$ start in \"The kernel is trivial: \"\n",
    "\n",
    "print(\"Typo/Punctuation inside $$ test passed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 3 Passed\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "# Input: Multi-line align block with padding newlines inside the command\n",
    "marked_text = r\"\"\"Consider the sequence:\n",
    "\\hlalign{\n",
    "\\begin{align}\n",
    "0 \\to A \\to B \\to 0\n",
    "\\end{align}\n",
    "}\n",
    "It is exact.\"\"\"\n",
    "\n",
    "data = html_data_from_marked_text(marked_text, latex_highlight_parser)\n",
    "\n",
    "# 1. Clean Text Verification\n",
    "# Padding newlines inside \\hlalign{...} are stripped.\n",
    "# The newline AFTER \"sequence:\" remains.\n",
    "# The newline BEFORE \"It is exact\" remains.\n",
    "expected_clean = r\"\"\"Consider the sequence:\n",
    "\\begin{align}\n",
    "0 \\to A \\to B \\to 0\n",
    "\\end{align}\n",
    "It is exact.\"\"\"\n",
    "\n",
    "# Strip checks to ignore potential leading/trailing whitespace difference in the file itself\n",
    "assert data.raw_text.strip() == expected_clean.strip()\n",
    "\n",
    "# 2. Tag Verification\n",
    "tag = data.tags[0]\n",
    "content = tag.tag.string\n",
    "\n",
    "# Ensure the tag content preserves the internal structure\n",
    "assert r\"\\begin{align}\" in content\n",
    "assert r\"0 \\to A\" in content\n",
    "assert r\"\\end{align}\" in content\n",
    "assert content.startswith(r\"\\begin{align}\")\n",
    "assert content.endswith(r\"\\end{align}\")\n",
    "# Ensure internal newlines are preserved\n",
    "assert \"\\n\" in content\n",
    "# Ensure tag is marked as notation\n",
    "assert \"notation\" in tag.tag.attrs\n",
    "\n",
    "print(\"Example 3 Passed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmenting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _split_text_by_html_data_parts(\n",
    "        # text_tags_and_locations = StrAndHTMLTagsWithIndices\n",
    "        datapoint: HTMLData\n",
    "        ) -> list[tuple[str, Union[bs4.element.Tag, None]]]:\n",
    "    r\"\"\"\n",
    "    Helper function to `augment_html_data`.\n",
    "    \"\"\"\n",
    "    to_return: list[tuple[str, Union[bs4.element.Tag, None]]] = []\n",
    "    split_points: list[int] = []\n",
    "    for tag_ind in datapoint['tags']:\n",
    "        split_points.append(tag_ind.start)\n",
    "        split_points.append(tag_ind.end)\n",
    "    pieces: list[str] = split_string_at_indices(datapoint['raw_text'], split_points)\n",
    "    for i, piece in enumerate(pieces):\n",
    "        if i % 2 == 0:\n",
    "            to_return.append((piece, None))\n",
    "        else:\n",
    "            to_return.append((piece, datapoint['tags'][int(i/2)].tag))\n",
    "    return to_return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('# Galois group of a separable and normal finite field extension\\n\\nLet $L/K$ be a separable and normal finite field extension. Its ',\n",
       "  None),\n",
       " ('Galois group',\n",
       "  <b definition=\"Galois group of a separable and normal finite field extension\">Galois group</b>),\n",
       " (' ', None),\n",
       " ('$\\\\operatorname{Gal}(L/K)$',\n",
       "  <span notation=\"\">$\\operatorname{Gal}(L/K)$</span>),\n",
       " (' is...\\n\\n# See Also\\n# Meta\\n## References and Citations\\n', None)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|hide\n",
    "sample_text = r\"\"\"# Galois group of a separable and normal finite field extension\n",
    "\n",
    "Let $L/K$ be a separable and normal finite field extension. Its <b definition=\"Galois group of a separable and normal finite field extension\">Galois group</b> <span notation=\"\">$\\operatorname{Gal}(L/K)$</span> is...\n",
    "\n",
    "# See Also\n",
    "# Meta\n",
    "## References and Citations\n",
    "\"\"\"\n",
    "\n",
    "sample_str_and_html_tags_with_indices: StrAndHTMLTagsWithIndices = remove_html_tags_in_text(sample_text)\n",
    "sample_html_data = HTMLData(note_name=None, raw_text=sample_str_and_html_tags_with_indices.raw_text, tags=sample_str_and_html_tags_with_indices.tags)\n",
    "output = _split_text_by_html_data_parts(sample_html_data)\n",
    "assert isinstance(output[1][1], bs4.element.Tag)\n",
    "output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', None),\n",
       " ('Hello', <b definition=\"\">Hello</b>),\n",
       " (' asdf ', None),\n",
       " (' $\\\\operatorname{Gal}$', <b notation=\"\"> $\\operatorname{Gal}$</b>),\n",
       " ('', None)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "sample_text = r\"\"\"<b definition=\"\">Hello</b> asdf <b notation=\"\"> $\\operatorname{Gal}$</b>\"\"\"\n",
    "sample_str_and_html_tags_with_indices: StrAndHTMLTagsWithIndices = remove_html_tags_in_text(sample_text)\n",
    "sample_html_data = HTMLData(note_name=None, raw_text=sample_str_and_html_tags_with_indices.raw_text, tags=sample_str_and_html_tags_with_indices.tags)\n",
    "output = _split_text_by_html_data_parts(sample_html_data)\n",
    "assert isinstance(output[1][1], bs4.element.Tag)\n",
    "assert isinstance(output[3][1], bs4.element.Tag)\n",
    "test_is(output[4][1], None)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HTMLTagWithIndices(tag=<b definition=\"\">Hello</b>, start=0, end=5), HTMLTagWithIndices(tag=<b notation=\"\"> $\\operatorname{Gal}$</b>, start=11, end=32)]\n"
     ]
    }
   ],
   "source": [
    "print(sample_str_and_html_tags_with_indices.tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def augment_html_data(\n",
    "        datapoint: HTMLData,\n",
    "        num_augmentation_sets: int = 1, # Each augmentation set consists of an augmentation with low, medium, and high probability modifications.\n",
    "        seed: Optional[int] = None\n",
    "        ) -> list[HTMLData]:\n",
    "    r\"\"\"Augment a given datapoint for HTML tagging.\n",
    "\n",
    "    \"\"\"\n",
    "    augmented_datapoints: list[HTMLData] = []\n",
    "    pieces: list[tuple[str, Union[bs4.element.Tag, None]]] = _split_text_by_html_data_parts(\n",
    "        datapoint)\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "    for _ in range(num_augmentation_sets):\n",
    "        augmented_datapoints.append(\n",
    "            _augment_html_data_once(pieces, 'low', datapoint['note_name']))\n",
    "        augmented_datapoints.append(\n",
    "            _augment_html_data_once(pieces, 'mid', datapoint['note_name']))\n",
    "        augmented_datapoints.append(\n",
    "            _augment_html_data_once(pieces, 'hi', datapoint['note_name']))\n",
    "        # augmented_datapoints.append(_augment_html_data_once(pieces, 'high'))\n",
    "    return augmented_datapoints\n",
    "\n",
    "\n",
    "def _augment_html_data_once(\n",
    "        pieces: list[tuple[str, Union[bs4.element.Tag, None]]],\n",
    "        modification: Literal['low', 'mid', 'high'],\n",
    "        note_name: str,\n",
    "        ) -> HTMLData:\n",
    "    methods = [\n",
    "        # (push_dollar_signs,0.2),\n",
    "        (remove_font_styles_at_random, 0.1), (change_font_styles_at_random, 0.2), (change_greek_letters_at_random, 0.1), \n",
    "        (remove_math_keywords,0.1), (random_latex_command_removal,0.2),\n",
    "        (random_word_removal,0.1), (dollar_sign_manipulation,0.05),\n",
    "        (random_char_modification,0.001)]\n",
    "    if modification == 'low':\n",
    "        method_inclusion_chance = 0.3\n",
    "        scale = 0.5\n",
    "    elif modification == 'mid':\n",
    "        method_inclusion_chance = 0.5\n",
    "        scale = 1.0\n",
    "    else:\n",
    "        method_inclusion_chance = 0.8\n",
    "        scale = 1.5\n",
    "    \n",
    "    random_methods = []\n",
    "    def create_method(method, p, scale):\n",
    "        return lambda x: method(x, p=p*scale)\n",
    "    for method, p in methods:\n",
    "        if random.random() < method_inclusion_chance:\n",
    "            random_methods.append(create_method(method, p, scale))\n",
    "    # random_methods = [\n",
    "    #     lambda x: method(x, p=p*scale) for method, p in methods if random.random() < method_inclusion_chance]\n",
    "    augmented_pieces = [\n",
    "        (augment_text(text, random_methods), copy.copy(tag))\n",
    "        for text, tag in pieces]\n",
    "    accumulated_len: int = 0\n",
    "    accumulated_text: str = \"\"\n",
    "    tags_with_indices: list[HTMLTagWithIndices] = []\n",
    "    for text, tag in augmented_pieces:\n",
    "        accumulated_text = f'{accumulated_text}{text}'\n",
    "        if tag:\n",
    "            tag.string = text\n",
    "            tags_with_indices.append(HTMLTagWithIndices(\n",
    "                tag, accumulated_len, accumulated_len + len(text)))\n",
    "        accumulated_len += len(text)\n",
    "    return HTMLData(note_name=note_name, raw_text=accumulated_text, tags=tags_with_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `augment_html_data` can be used to augment definition and notation token classification data gathered via `html_data_from_note`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'note_name': None,\n",
       "  'raw_text': 'Let $L/K$ be a separable and normal finite field extension. Its Galois group $\\\\operatorname{Gal}(L/K)$ is...\\n\\nIn fact, the notion of a Galois group can be defined for profinite field extensions. Given a separable and normal profinite field extension $L/K$, say that\\n$L = \\\\varinjlim_i L_i$ where $L_i/K$ are finite extensions. Its Galois group $\\\\operatorname{Gal}(L/K)$\\n',\n",
       "  'tags': [HTMLTagWithIndices(tag=<b definition=\"Galois group of a separable and normal finite field extension\">Galois group</b>, start=64, end=76),\n",
       "   HTMLTagWithIndices(tag=<span notation=\"\">$\\operatorname{Gal}(L/K)$</span>, start=77, end=102),\n",
       "   HTMLTagWithIndices(tag=<b definition=\"\">Galois group</b>, start=330, end=342),\n",
       "   HTMLTagWithIndices(tag=<span notation=\"\">$\\operatorname{Gal}(L/K)$</span>, start=343, end=368)]},\n",
       " [{'note_name': None,\n",
       "   'raw_text': 'Let $L/K$ be a separable and normal finite field extension. Its Galois group $\\\\operatorname{Gal}(L/K)$ is...\\n\\nIn fact, the notion  a Galois group can be defined for profinite field extensions. Given a separable and normal profinite field extension $L/K$, say that\\n$L = \\\\varinjlim_i L_i$ where $L_i/K$ are finite extensions. Its Galois group $\\\\operatorname{Gal}(L/K)$\\n',\n",
       "   'tags': [HTMLTagWithIndices(tag=<b definition=\"Galois group of a separable and normal finite field extension\">Galois group</b>, start=64, end=76),\n",
       "    HTMLTagWithIndices(tag=<span notation=\"\">$\\operatorname{Gal}(L/K)$</span>, start=77, end=102),\n",
       "    HTMLTagWithIndices(tag=<b definition=\"\">Galois group</b>, start=328, end=340),\n",
       "    HTMLTagWithIndices(tag=<span notation=\"\">$\\operatorname{Gal}(L/K)$</span>, start=341, end=366)]},\n",
       "  {'note_name': None,\n",
       "   'raw_text': 'Let $L/K$ be a separable and normal finite field extension. Its Galois group $\\\\operatorname{Gal}(L/K)$ is...\\n\\nIn fact, the notion of a Galois group can be defined for profinite field extensions. Given a separable and normal profinite field extension $L/K$, say that\\n$L = \\\\varinjlim_i L_i$ where $L_i/K$ are finite extensions. Its Galois group $\\\\operatorname{Gal}(L/K)$\\n',\n",
       "   'tags': [HTMLTagWithIndices(tag=<b definition=\"Galois group of a separable and normal finite field extension\">Galois group</b>, start=64, end=76),\n",
       "    HTMLTagWithIndices(tag=<span notation=\"\">$\\operatorname{Gal}(L/K)$</span>, start=77, end=102),\n",
       "    HTMLTagWithIndices(tag=<b definition=\"\">Galois group</b>, start=330, end=342),\n",
       "    HTMLTagWithIndices(tag=<span notation=\"\">$\\operatorname{Gal}(L/K)$</span>, start=343, end=368)]},\n",
       "  {'note_name': None,\n",
       "   'raw_text': 'Let $L/K$ be a separable and normal finite field extension. Its Galois group $(L/K)$ is...\\n\\nIn fact, the notion of a Galois group can be defined for profinite field extensions. Given a separable and normal profinite field extension $L/K$~ say that\\n$L = _i L_i$ where $L_i/K$ are finite extensions. Its Galois group $\\\\mathbb{Gal}(L/K)$\\n',\n",
       "   'tags': [HTMLTagWithIndices(tag=<b definition=\"Galois group of a separable and normal finite field extension\">Galois group</b>, start=64, end=76),\n",
       "    HTMLTagWithIndices(tag=<span notation=\"\">$(L/K)$</span>, start=77, end=84),\n",
       "    HTMLTagWithIndices(tag=<b definition=\"\">Galois group</b>, start=302, end=314),\n",
       "    HTMLTagWithIndices(tag=<span notation=\"\">$\\mathbb{Gal}(L/K)$</span>, start=315, end=334)]}])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mf = MarkdownFile.from_string(\n",
    "    r\"\"\"---\n",
    "aliases: []\n",
    "tags: []\n",
    "---\n",
    "# Galois group of a separable and normal finite field extension\n",
    "\n",
    "Let $L/K$ be a separable and normal finite field extension. Its <b definition=\"Galois group of a separable and normal finite field extension\">Galois group</b> <span notation=\"\">$\\operatorname{Gal}(L/K)$</span> is...\n",
    "\n",
    "# Galois group of a separable and normal profinite field extension\n",
    "\n",
    "In fact, the notion of a Galois group can be defined for profinite field extensions. Given a separable and normal profinite field extension $L/K$, say that\n",
    "$L = \\varinjlim_i L_i$ where $L_i/K$ are finite extensions. Its **Galois group** **$\\operatorname{Gal}(L/K)$**\n",
    "\n",
    "# See Also\n",
    "# Meta\n",
    "## References and Citations\n",
    "\"\"\")\n",
    "\n",
    "html_data: HTMLData = html_data_from_note(mf)\n",
    "output = augment_html_data(html_data,seed=None)\n",
    "html_data, output\n",
    "# html_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is sample code to then gather data for definition/notation identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "\n",
    "# TODO: test\n",
    "\n",
    "notes = [] # Replace with actual notes\n",
    "vault = '' # Replace with actual vault\n",
    "\n",
    "# vault = 'C:' # Replace with actual vault\n",
    "# notes = [] # Replace with actual notes\n",
    "\n",
    "html_data = [html_data_from_note(note, vault) for note in notes]\n",
    "max_length = 1022\n",
    "\n",
    "tokenized_html_data = [tokenize_html_data(html_locus, tokenizer, max_length, def_or_notat_from_html_tag, label2id) for html_locus in html_data]\n",
    "token_id_data = [token_ids for token_ids, _ in tokenized_html_data]\n",
    "ner_tag_data = [ner_tag_ids for _, ner_tag_ids in tokenized_html_data]\n",
    "token_seqs = [token_seq for token_seq in token_ids for token_ids in token_id_data]\n",
    "ner_tag_seqs = [ner_tag_seq for ner_tag_seq in ner_tag_ids for ner_tag_ids in ner_tag_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "max_length = 1022\n",
    "label2id = {\n",
    "    \"O\": 0,\n",
    "    \"B-definition\": 1,\n",
    "    \"I-definition\": 2,\n",
    "    \"B-notation\": 3,\n",
    "    \"I-notation\": 4\n",
    "} \n",
    "id2label = {value: key for key, value in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "note_names, token_seqs, ner_tag_seqs = [], [], []\n",
    "for html_locus, (token_ids, ner_tag_ids) in zip(html_data, tokenized_html_data):\n",
    "    note_names.extend([html_locus[\"Note name\"]] * len(token_ids))\n",
    "    token_seqs.extend(token_ids)\n",
    "    ner_tag_seqs.extend(ner_tag_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "# ner_tags = ClassLabel(names=list(label2id))\n",
    "\n",
    "# ds = Dataset.from_dict(\n",
    "#         {\"note_name\": note_names,\n",
    "#         \"tokens\": token_ids,\n",
    "#         \"ner_tags\": ner_tag_ids},\n",
    "#         features=Features(\n",
    "#             {\n",
    "#              \"note_name\": Value(dtype='string'),\n",
    "#              \"tokens\": Sequence(Value(dtype='string')),\n",
    "#              \"ner_tags\": Sequence(ner_tags)}\n",
    "#         ))\n",
    "\n",
    "# ds.save_to_disk(\".\")\n",
    "\n",
    "# ds.load_from_disk(\".\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See https://huggingface.co/docs/transformers/tasks/token_classification for training a token classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<b definition=\"\" style=\"border-width:1px;border-style:solid;padding:3px\">hi</b>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = bs4.BeautifulSoup('', 'html.parser')\n",
    "tag = soup.new_tag('b', style=\"border-width:1px;border-style:solid;padding:3px\", definition=\"\")\n",
    "tag.string = 'hi'\n",
    "tag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _make_tag(\n",
    "        text: str,\n",
    "        entity_type: str # 'definition' or 'notation'\n",
    "        ) -> bs4.element.Tag:\n",
    "    \"\"\"\n",
    "    Helper function to `_html_tag_data_from_part` and `_consolidate_token_preds`.\n",
    "    \"\"\"\n",
    "    soup = bs4.BeautifulSoup('', 'html.parser')\n",
    "    if entity_type == 'definition':\n",
    "        tag = soup.new_tag(\n",
    "            'b',\n",
    "            style=\"border-width:1px;border-style:solid;padding:3px\",\n",
    "            definition=\"\")\n",
    "    else:\n",
    "        tag = soup.new_tag(\n",
    "            'span',\n",
    "            style=\"border-width:1px;border-style:solid;padding:3px\",\n",
    "            notation=\"\")\n",
    "    tag.string = text\n",
    "    return tag\n",
    "\n",
    "\n",
    "def _html_tag_data_from_part(\n",
    "        main_text: str,\n",
    "        # part: list[dict[str]]) -> tuple[bs4.element.Tag, int, int]:\n",
    "        part: list[dict[str]] # An item of an output of `_divide_token_preds_into_parts`. Each dict likely contains keys such as `'entity'`, `'score'`, `'index'`, `'word'`, `'start'`, and `'end'`, depending on the model used.\n",
    "        ) -> tuple[HTMLTagWithIndices]:\n",
    "    \"\"\"\n",
    "    Helper function to `_html_tags_from_token_preds`\n",
    "    \"\"\"\n",
    "    start_token: dict[str] = part[0]\n",
    "    end_token: dict[str] = part[-1]\n",
    "    start_char_pos: int = start_token['start']\n",
    "    end_char_pos: int = end_token['end']\n",
    "\n",
    "    # Depending on the tokenizer starting spaces might be included in a given token.\n",
    "    # We exclude such a starting space.\n",
    "    while main_text[start_char_pos].isspace():\n",
    "        start_char_pos += 1\n",
    "    while main_text[end_char_pos-1].isspace():\n",
    "        end_char_pos -= 1\n",
    "\n",
    "    # the `'entity'` is either 'I-definition', 'B-definition', 'I-notation',\n",
    "    # or 'B-notation'\n",
    "    entity_type = start_token['entity'][2:]\n",
    "    html_text = main_text[start_char_pos:end_char_pos]\n",
    "    \n",
    "    # return (_make_tag(html_text, entity_type), start_char, end_char)\n",
    "    return HTMLTagWithIndices(_make_tag(html_text, entity_type), start_char_pos, end_char_pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "main_text = r\"Let $I \\subset A$ be an ideal. Define its radical by $\\sqrt{I}$\"\n",
    "\n",
    "sample_output_1 = _html_tag_data_from_part(\n",
    "    main_text, [{\n",
    "        'entity': 'B-definition',\n",
    "        'score': 0.37319255,\n",
    "        'index': 25,  # This is moot for the purposes of this test.\n",
    "        'word': 'radical',\n",
    "        'start': 42,\n",
    "        'end': 49\n",
    "    }])\n",
    "test_eq(str(sample_output_1.tag), '<b definition=\"\" style=\"border-width:1px;border-style:solid;padding:3px\">radical</b>')\n",
    "\n",
    "sample_output_2 = _html_tag_data_from_part(\n",
    "    main_text, [{\n",
    "        'entity': 'B-notation',\n",
    "        'score': 0.67021805,\n",
    "        'index': 27,  # This is moot for the purposes of this test.\n",
    "        'word': '$',\n",
    "        'start': 53,\n",
    "        'end': 54},\n",
    "    {\n",
    "        'entity': 'I-notation',\n",
    "        'score': 0.9748327,\n",
    "        'index': 28,  # This is moot for the purposes of this test.\n",
    "        'word': '\\\\',\n",
    "        'start': 54,\n",
    "        'end': 55},\n",
    "    {\n",
    "        'entity': 'I-notation',\n",
    "        'score': 0.9754836,\n",
    "        'index': 29,  # This is moot for the purposes of this test.\n",
    "        'word': 'sq',\n",
    "        'start': 55,\n",
    "        'end': 57},\n",
    "    {\n",
    "        'entity': 'I-notation',\n",
    "        'score': 0.9750675,\n",
    "        'index': 30,  # This is moot for the purposes of this test.\n",
    "        'word': '##rt',\n",
    "        'start': 57,\n",
    "        'end': 59},\n",
    "    {\n",
    "        'entity': 'I-notation',\n",
    "        'score': 0.97785944,\n",
    "        'index': 31,  # This is moot for the purposes of this test.\n",
    "        'word': '{',\n",
    "        'start': 59,\n",
    "        'end': 60},\n",
    "    {\n",
    "        'entity': 'I-notation',\n",
    "        'score': 0.97785944,\n",
    "        'index': 32,  # This is moot for the purposes of this test.\n",
    "        'word': 'i',\n",
    "        'start': 60,\n",
    "        'end': 61},\n",
    "    {\n",
    "        'entity': 'I-notation',\n",
    "        'score': 0.97785944,\n",
    "        'index': 33,  # This is moot for the purposes of this test.\n",
    "        'word': '}',\n",
    "        'start': 61,\n",
    "        'end': 62},\n",
    "    {\n",
    "        'entity': 'I-notation',\n",
    "        'score': 0.97785944,\n",
    "        'index': 34,  # This is moot for the purposes of this test.\n",
    "        'word': '$',\n",
    "        'start': 62,\n",
    "        'end': 63},\n",
    "    ])\n",
    "test_eq(str(sample_output_2.tag), r'<span notation=\"\" style=\"border-width:1px;border-style:solid;padding:3px\">$\\sqrt{I}$</span>')\n",
    "# main_text.find('radical')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "# With modernbert, the tokenizer tends to capture spaces, within tokens, which creates some subtle issues with spacing.\n",
    "text = r'Definition 1.1. - We call $\\mathscr{U}$-topos, or simply topos if there is no fear of confusion, a category $\\mathrm{E}$ such that there is a site $\\mathrm{C} \\in \\mathscr{U}$ such that $\\mathrm{E}$ is equivalent to the category $\\tilde{\\mathrm{C}}$ of $\\mathrm{U}$-sheaves of sets over $\\mathrm{C}$ .'\n",
    "\n",
    "part_1 = [\n",
    "        {'entity': 'B-definition',\n",
    "        'score': 0.9991014,\n",
    "        'index': 9,\n",
    "        'word': '$\\\\',\n",
    "        'start': 25,\n",
    "        'end': 28},\n",
    "        {'entity': 'I-definition',\n",
    "        'score': 0.99974686,\n",
    "        'index': 10,\n",
    "        'word': 'mathscr',\n",
    "        'start': 28,\n",
    "        'end': 35},\n",
    "        {'entity': 'I-definition',\n",
    "        'score': 0.999652,\n",
    "        'index': 11,\n",
    "        'word': '{',\n",
    "        'start': 35,\n",
    "        'end': 36},\n",
    "        {'entity': 'I-definition',\n",
    "        'score': 0.99985695,\n",
    "        'index': 12,\n",
    "        'word': 'U',\n",
    "        'start': 36,\n",
    "        'end': 37},\n",
    "        {'entity': 'I-definition',\n",
    "        'score': 0.9997553,\n",
    "        'index': 13,\n",
    "        'word': '}$-',\n",
    "        'start': 37,\n",
    "        'end': 40},\n",
    "        {'entity': 'I-definition',\n",
    "        'score': 0.9997918,\n",
    "        'index': 14,\n",
    "        'word': 'top',\n",
    "        'start': 40,\n",
    "        'end': 43},\n",
    "        {'entity': 'I-definition',\n",
    "        'score': 0.9997335,\n",
    "        'index': 15,\n",
    "        'word': 'os',\n",
    "        'start': 43,\n",
    "        'end': 45},]\n",
    "\n",
    "part_2 = [\n",
    "        {'entity': 'B-definition',\n",
    "        'score': 0.594063,\n",
    "        'index': 19,\n",
    "        'word': 'to',\n",
    "        'start': 56,\n",
    "        'end': 59},\n",
    "        {'entity': 'I-definition',\n",
    "        'score': 0.9056522,\n",
    "        'index': 20,\n",
    "        'word': 'pos',\n",
    "        'start': 59,\n",
    "        'end': 62}]\n",
    "\n",
    "sample_output_1: HTMLTagWithIndices = _html_tag_data_from_part(text, part_1)\n",
    "assert not sample_output_1.tag.text.startswith(' ')\n",
    "assert sample_output_1.tag.text == '$\\\\mathscr{U}$-topos'\n",
    "\n",
    "sample_output_2: HTMLTagWithIndices = _html_tag_data_from_part(text, part_1)\n",
    "assert not sample_output_2.tag.text.startswith(' ')\n",
    "assert sample_output_2.tag.text == '$\\\\mathscr{U}$-topos'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _current_token_continues_the_previous_token(\n",
    "        current_token: dict,\n",
    "        previous_token: dict,\n",
    "        note: Optional[VaultNote] = None,\n",
    "        ) -> bool:\n",
    "    \"\"\"\n",
    "    Helper function to `_divide_token_preds_into_parts`.\n",
    "    \"\"\"\n",
    "    if current_token['entity'].startswith('I-'):\n",
    "        if current_token['entity'][2:] == previous_token['entity'][2:]:\n",
    "            return True\n",
    "        elif note:\n",
    "            warnings.warn(rf\"\"\"\n",
    "                In the note {note.name} at {note.path()},\n",
    "                The token '{previous_token['word']}' is marked as '{previous_token['entity']}'\n",
    "                and the subsequent token '{current_token['word']}' is marked as '{current_token['entity']}',\n",
    "                which is unusual because the two consecutive tokens seem to be of different\n",
    "                entities, and yet the latter token does not start with a 'B-'.\n",
    "\n",
    "                The latter token will be treated like the beginning of a new entity.\"\"\"\n",
    "                    )\n",
    "            return False\n",
    "    else:\n",
    "        return False\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "previous_token_1 = {\n",
    "        'entity': 'I-notation',\n",
    "        'score': 0.97785944,\n",
    "        'index': 33,  # This is moot for the purposes of this test.\n",
    "        'word': '}',\n",
    "        'start': 61,\n",
    "        'end': 62}\n",
    "current_token_1 = {\n",
    "        'entity': 'I-notation',\n",
    "        'score': 0.97785944,\n",
    "        'index': 34,  # This is moot for the purposes of this test.\n",
    "        'word': '$',\n",
    "        'start': 62,\n",
    "        'end': 63}\n",
    "assert _current_token_continues_the_previous_token(current_token_1, previous_token_1, note=VaultNote('', rel_path='hi'))\n",
    "\n",
    "# Something like below should hopefully not happen, but it should still give a warning message\n",
    "previous_token_2 = {\n",
    "        'entity': 'I-definition',\n",
    "        'score': 0.97785944,\n",
    "        'index': 33,  # This is moot for the purposes of this test.\n",
    "        'word': '}',\n",
    "        'start': 61,\n",
    "        'end': 62}\n",
    "current_token_2 = {\n",
    "        'entity': 'I-notation',\n",
    "        'score': 0.97785944,\n",
    "        'index': 34,  # This is moot for the purposes of this test.\n",
    "        'word': '$',\n",
    "        'start': 62,\n",
    "        'end': 63}\n",
    "\n",
    "with warnings.catch_warnings(record=True) as w:\n",
    "    sample_output = _current_token_continues_the_previous_token(current_token_2, previous_token_2, note=VaultNote('', rel_path='hi'))\n",
    "    assert w\n",
    "    assert not sample_output\n",
    "\n",
    "previous_token_3 = {\n",
    "        'entity': 'I-definition',\n",
    "        'score': 0.97785944,\n",
    "        'index': 33,  # This is moot for the purposes of this test.\n",
    "        'word': '##tion',\n",
    "        'start': 58,\n",
    "        'end': 62}\n",
    "current_token_3 = {\n",
    "        'entity': 'B-notation',\n",
    "        'score': 0.97785944,\n",
    "        'index': 34,  # This is moot for the purposes of this test.\n",
    "        'word': '$',\n",
    "        'start': 62,\n",
    "        'end': 63}\n",
    "\n",
    "assert not _current_token_continues_the_previous_token(current_token_3, previous_token_3, note=VaultNote('', rel_path='hi'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _divide_token_preds_into_parts(\n",
    "        token_preds: list[dict[str]],\n",
    "        excessive_space_threshold: int,\n",
    "        note: Optional[VaultNote] = None\n",
    "        ) -> list[list[dict[str]]]:\n",
    "    \"\"\"\n",
    "    Divide `token_preds` into parts so that each part\n",
    "    represents a single definition/notation marking.\n",
    "\n",
    "    Helper function to `_html_tags_from_token_preds`.\n",
    "    \"\"\"\n",
    "    token_preds_parts = []\n",
    "    for current_token in token_preds:\n",
    "        if not token_preds_parts:\n",
    "            token_preds_parts.append([current_token])\n",
    "            continue\n",
    "        prev_token = token_preds_parts[-1][-1]\n",
    "        if _current_token_continues_the_previous_token(\n",
    "                current_token, prev_token, note):\n",
    "            prev_token_end = prev_token['end']\n",
    "            cur_token_start = current_token['start']\n",
    "            if prev_token_end + excessive_space_threshold >= cur_token_start and note:\n",
    "                Warning(rf\"\"\"\n",
    "                    In the note {note.name} at {note.path()},\n",
    "                    There seems to be excessive space between the token\n",
    "                    {prev_token['word']} and {current_token['word']}, which\n",
    "                    seem to be part of the same entity\"\"\"\n",
    "                        )\n",
    "            token_preds_parts[-1].append(current_token)\n",
    "        else:\n",
    "            token_preds_parts.append([current_token])\n",
    "    return token_preds_parts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "main_text = r\"Let $I \\subset A$ be an ideal. Define its radical by $\\sqrt{I}$\"\n",
    "\n",
    "preds = [\n",
    "    {\n",
    "        'entity': 'B-definition',\n",
    "        'score': 0.37319255,\n",
    "        'index': 25,  # This is moot for the purposes of this test.\n",
    "        'word': 'radical',\n",
    "        'start': 42,\n",
    "        'end': 49\n",
    "    },\n",
    "    {\n",
    "        'entity': 'B-notation',\n",
    "        'score': 0.67021805,\n",
    "        'index': 27,  # This is moot for the purposes of this test.\n",
    "        'word': '$',\n",
    "        'start': 53,\n",
    "        'end': 54},\n",
    "    {\n",
    "        'entity': 'I-notation',\n",
    "        'score': 0.9748327,\n",
    "        'index': 28,  # This is moot for the purposes of this test.\n",
    "        'word': '\\\\',\n",
    "        'start': 54,\n",
    "        'end': 55},\n",
    "    {\n",
    "        'entity': 'I-notation',\n",
    "        'score': 0.9754836,\n",
    "        'index': 29,  # This is moot for the purposes of this test.\n",
    "        'word': 'sq',\n",
    "        'start': 55,\n",
    "        'end': 57},\n",
    "    {\n",
    "        'entity': 'I-notation',\n",
    "        'score': 0.9750675,\n",
    "        'index': 30,  # This is moot for the purposes of this test.\n",
    "        'word': '##rt',\n",
    "        'start': 57,\n",
    "        'end': 59},\n",
    "    {\n",
    "        'entity': 'I-notation',\n",
    "        'score': 0.97785944,\n",
    "        'index': 31,  # This is moot for the purposes of this test.\n",
    "        'word': '{',\n",
    "        'start': 59,\n",
    "        'end': 60},\n",
    "    {\n",
    "        'entity': 'I-notation',\n",
    "        'score': 0.97785944,\n",
    "        'index': 32,  # This is moot for the purposes of this test.\n",
    "        'word': 'i',\n",
    "        'start': 60,\n",
    "        'end': 61},\n",
    "    {\n",
    "        'entity': 'I-notation',\n",
    "        'score': 0.97785944,\n",
    "        'index': 33,  # This is moot for the purposes of this test.\n",
    "        'word': '}',\n",
    "        'start': 61,\n",
    "        'end': 62},\n",
    "    {\n",
    "        'entity': 'I-notation',\n",
    "        'score': 0.97785944,\n",
    "        'index': 34,  # This is moot for the purposes of this test.\n",
    "        'word': '$',\n",
    "        'start': 62,\n",
    "        'end': 63},\n",
    "    ]\n",
    "\n",
    "output = _divide_token_preds_into_parts(\n",
    "    preds,  excessive_space_threshold=2, note=VaultNote('', rel_path='hi')\n",
    ")\n",
    "\n",
    "# Test that the list finds two parts, one for the definition, and the other for the notation.\n",
    "test_eq(len(output), 2)\n",
    "test_eq(len(output[0]), 1)\n",
    "test_eq(len(output[1]), len(preds) - 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _ranges_overlap(\n",
    "        current_1: HTMLTagWithIndices,\n",
    "        current_2: HTMLTagWithIndices\n",
    "        # current_1: tuple[bs4.element.Tag, int, int],\n",
    "        # current_2: tuple[bs4.element.Tag, int, int]\n",
    "        ) -> bool:\n",
    "    \"\"\"\n",
    "    Based on https://stackoverflow.com/a/64745177\n",
    "\n",
    "    Helper function to `_collate_html_tags`, `_consolidate_token_preds`.\n",
    "    \"\"\"\n",
    "    return max(current_1.start, current_2.start) < min(current_1.end, current_2.end)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# In actuality, there should be bs4.element.Tag objects in place of ''.\n",
    "assert _ranges_overlap(HTMLTagWithIndices('', 3, 8), HTMLTagWithIndices('', 6, 12))\n",
    "assert _ranges_overlap(HTMLTagWithIndices('', 3, 8), HTMLTagWithIndices('', 3, 4))\n",
    "assert not _ranges_overlap(HTMLTagWithIndices('', 3, 8), HTMLTagWithIndices('', 8, 9))\n",
    "assert _ranges_overlap(HTMLTagWithIndices('', 6, 12), HTMLTagWithIndices('', 3, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# If the ML model predicts \n",
    "# predictions made around \n",
    "latex_commands_to_avoid = [\n",
    "    # Document structure\n",
    "    \"\\\\documentclass\", \"\\\\usepackage\", \"\\\\begin\", \"\\\\end\",\n",
    "    # Sectioning\n",
    "    \"\\\\part\", \"\\\\chapter\", \"\\\\section\", \"\\\\subsection\", \"\\\\subsubsection\", \"\\\\paragraph\", \"\\\\subparagraph\",\n",
    "    # Referencing and citation\n",
    "    \"\\\\ref\", \"\\\\pageref\", \"\\\\cite\", \"\\\\bibitem\", \"\\\\bibliography\", \"\\\\bibliographystyle\",\n",
    "    \n",
    "    # Lists\n",
    "    \"\\\\enumerate\", \"\\\\itemize\", \"\\\\item\", \"\\\\description\",\n",
    "    \n",
    "    # Footnotes\n",
    "    \"\\\\footnote\", \"\\\\footnotemark\", \"\\\\footnotetext\",\n",
    "    \n",
    "    # Floats and captions\n",
    "    # \"\\\\figure\", \"\\\\table\", \"\\\\caption\", \"\\\\centering\",\n",
    "    \n",
    "    # Page formatting\n",
    "    \"\\\\newpage\", \"\\\\clearpage\", \"\\\\cleardoublepage\", \"\\\\pagebreak\", \"\\\\newline\", \"\\\\linebreak\",\n",
    "    \n",
    "    # Definitions and counters\n",
    "    \"\\\\newcommand\", \"\\\\renewcommand\", \"\\\\def\", \"\\\\let\", \"\\\\setcounter\", \"\\\\addtocounter\", \"\\\\stepcounter\", \"\\\\refstepcounter\",\n",
    "    \n",
    "    # Index and glossary\n",
    "    \"\\\\index\", \"\\\\glossary\", \"\\\\printindex\", \"\\\\printglossary\",\n",
    "    \n",
    "    # Hyperlinks (if using hyperref package)\n",
    "    \"\\\\href\", \"\\\\url\", \"\\\\hypersetup\", \"\\\\hyperlink\", \"\\\\hypertarget\",\n",
    "    \n",
    "    # Spacing\n",
    "    \"\\\\hspace\", \"\\\\vspace\", \"\\\\phantom\", \"\\\\vphantom\", \"\\\\hphantom\",\n",
    "    \n",
    "    # Alignment environments\n",
    "    \"\\\\center\", \"\\\\flushleft\", \"\\\\flushright\",\n",
    "    \n",
    "    # Verbatim and code\n",
    "    \"\\\\verb\", \"\\\\verbatim\", \"\\\\lstlisting\",\n",
    "    \n",
    "    # Theorems and proofs\n",
    "    \"\\\\theorem\", \"\\\\lemma\", \"\\\\proof\", \"\\\\corollary\",\n",
    "    \n",
    "    # Cross-referencing\n",
    "    \"\\\\cref\", \"\\\\Cref\", \"\\\\vref\", \"\\\\Vref\",  # from cleveref package\n",
    "    \n",
    "    # Table of contents related\n",
    "    \"\\\\tableofcontents\", \"\\\\listoffigures\", \"\\\\listoftables\",\n",
    "    \n",
    "    # Appendix\n",
    "    \"\\\\appendix\",\n",
    "    \n",
    "    # Author and title information\n",
    "    \"\\\\title\", \"\\\\author\", \"\\\\date\", \"\\\\maketitle\",\n",
    "    \n",
    "    # Abstract\n",
    "    \"\\\\abstract\", \"\\\\keywords\",\n",
    "    \n",
    "    # Including external files\n",
    "    \"\\\\input\", \"\\\\include\",\n",
    "    \n",
    "    # Page numbering\n",
    "    \"\\\\pagenumbering\", \"\\\\thepage\",\n",
    "    \n",
    "    # Margin adjustments\n",
    "    \"\\\\marginpar\", \"\\\\marginparsep\", \"\\\\marginparwidth\",\n",
    "    \n",
    "    # Columns\n",
    "    \"\\\\twocolumn\", \"\\\\onecolumn\", \"\\\\multicolumn\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _str_contains_latex_command_to_avoid(text):\n",
    "    \"\"\"\n",
    "    Helper function to `_consolidate_token_preds`\n",
    "    \"\"\"\n",
    "    # Check if any command from the list is in the text\n",
    "    for command in latex_commands_to_avoid:\n",
    "        if command.lower() in text:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String 1: Does not contain a LaTeX command to avoid.\n",
      "String 2: Contains a LaTeX command to avoid.\n",
      "String 3: Contains a LaTeX command to avoid.\n",
      "String 4: Contains a LaTeX command to avoid.\n",
      "String 5: Contains a LaTeX command to avoid.\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "test_strings = [\n",
    "    r\"This is a normal sentence.\",\n",
    "    r\"This sentence contains \\section{A Section}.\",\n",
    "    r\"Here's a \\footnote{This is a footnote.}\",\n",
    "    r\"Let's \\begin{enumerate} some items.\",\n",
    "    r\"\\href{https://example.com}{This is a hyperlink}\",\n",
    "]\n",
    "\n",
    "for i, string in enumerate(test_strings, 1):\n",
    "    result = _str_contains_latex_command_to_avoid(string)\n",
    "    print(f\"String {i}: {'Contains' if result else 'Does not contain'} a LaTeX command to avoid.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _consolidate_token_preds(\n",
    "        main_text: str,\n",
    "        tag_data: list[HTMLTagWithIndices]\n",
    "        ) -> list[HTMLTagWithIndices]:\n",
    "    \"\"\"\n",
    "    Since the model's predictions can yield some odd results\n",
    "    (e.g. notations not being marked for an entire LaTeX string\n",
    "    $<span notation=\"\">$S_k := ...</span>$$), this function tries\n",
    "    to consolidate some oddities.\n",
    "    \n",
    "    \"\"\"\n",
    "    latex_inds = latex_indices(main_text)\n",
    "    extended_tag_data = _extend_tag_data_ranges(main_text, latex_inds, tag_data)\n",
    "    # extended_tag_data = _extend_tag_data_ranges_to_encompass_latex(main_text, latex_inds, tag_data)\n",
    "    # extended_tag_data = _extend_tag_data_ranges_to_border_spaces(main_text, extended_tag_data)\n",
    "    tag_data_notats_chopped = _cutoff_notation_tag_data(main_text, extended_tag_data)\n",
    "    # Go through the extended tag data to throw out overlapping ones.\n",
    "    ultimate_tag_data: list[HTMLTagWithIndices] = []\n",
    "    for tag_point in tag_data_notats_chopped:\n",
    "        if (_no_overlap_with_previous_tag_data(ultimate_tag_data, tag_point)\n",
    "                and not _str_contains_latex_command_to_avoid(tag_point.tag.text)):\n",
    "            ultimate_tag_data.append(tag_point)\n",
    "    return ultimate_tag_data\n",
    "\n",
    "\n",
    "def _extend_tag_data_ranges(\n",
    "        main_text: str,\n",
    "        latex_inds: list[tuple[int, int]],\n",
    "        tag_data: list[HTMLTagWithIndices],\n",
    "        ) -> list[HTMLTagWithIndices]:\n",
    "    \"\"\"\n",
    "    Extend tag data so that\n",
    "    1. the tag data does not start or end within any latex math mode string.\n",
    "    2. the tag data is immediately preceded by whitespace (or the start/end of line)\n",
    "       and followed by whitespace or punctuation\n",
    "    3. the tag data does not start/end in the middle of the arguments of a latex command.\n",
    "\n",
    "    Helper function to `_consolidate_token_preds`.\n",
    "    \"\"\"\n",
    "    # TODO: make sure that tag has balanced curly braces \n",
    "    extended_tag_data = []\n",
    "    for tag_tuple in tag_data:\n",
    "        tag_tuple_before_extension = tag_tuple\n",
    "        while True:\n",
    "            tag_tuple_after_extension = _extend_tag_data_range_for_math_mode(\n",
    "                tag_tuple_before_extension, main_text, latex_inds)\n",
    "            tag_tuple_after_extension = _extend_tag_data_range_to_border_space_or_punc(\n",
    "                tag_tuple_after_extension, main_text)\n",
    "            tag_tuple_after_extension = _extend_tag_data_ranges_to_balance_curly_braces(\n",
    "                tag_tuple_after_extension, main_text)\n",
    "            tag_tuple_before_extension = tag_tuple_after_extension \n",
    "            if (tag_tuple_before_extension[1] == tag_tuple_after_extension[1]\n",
    "                    and tag_tuple_before_extension[2] == tag_tuple_after_extension[2]):\n",
    "                break\n",
    "        extended_tag_data.append(tag_tuple_after_extension)\n",
    "    return extended_tag_data\n",
    "\n",
    "\n",
    "def _update_tag_data(\n",
    "        tag_tuple: HTMLTagWithIndices,\n",
    "        main_text: str,\n",
    "        new_start: int,\n",
    "        new_end: int) -> HTMLTagWithIndices:\n",
    "    new_text = main_text[new_start:new_end]\n",
    "    tag_type = 'definition' if 'definition' in tag_tuple.tag.attrs else 'notation'\n",
    "    new_tag = _make_tag(new_text, tag_type)\n",
    "    return HTMLTagWithIndices(new_tag, new_start, new_end)\n",
    "\n",
    "\n",
    "def _extend_tag_data_range_for_math_mode(\n",
    "        tag_tuple: HTMLTagWithIndices,\n",
    "        main_text: str,\n",
    "        latex_inds: list[tuple[int, int]],\n",
    "        ) -> HTMLTagWithIndices:\n",
    "    \"\"\"\n",
    "    Extend tag data so that the tag data does not start or end within latex math mode string.\n",
    "    \"\"\"\n",
    "    extended_range = [tag_tuple.start, tag_tuple.end]\n",
    "    for tex_range in latex_inds:\n",
    "        if not _ranges_overlap(HTMLTagWithIndices(0, tex_range[0], tex_range[1]), tag_tuple):\n",
    "            continue\n",
    "        extended_range = (min(extended_range[0], tex_range[0]), max(extended_range[1], tex_range[1]))\n",
    "    return _update_tag_data(tag_tuple, main_text, extended_range[0], extended_range[1])\n",
    "\n",
    "\n",
    "def _extend_tag_data_range_to_border_space_or_punc(\n",
    "        tag_tuple: HTMLTagWithIndices,\n",
    "        main_text: str,\n",
    "        ) -> HTMLTagWithIndices:\n",
    "    \"\"\"\n",
    "    Extend tag data so that the tag data borders spaces or punctuations.\n",
    "\n",
    "    Helper function to `_consolidate_token_preds`.\n",
    "    \"\"\"\n",
    "    combined_range = [tag_tuple.start, tag_tuple.end]\n",
    "    while combined_range[0] != 0 and not main_text[combined_range[0]-1].isspace():\n",
    "        combined_range[0] -= 1\n",
    "    # while combined_range[1] != len(main_text) and not main_text[combined_range[1]].isspace():\n",
    "    while combined_range[1] != len(main_text) and is_not_space_and_not_punc(main_text[combined_range[1]]):\n",
    "        combined_range[1] += 1\n",
    "    return _update_tag_data(tag_tuple, main_text, combined_range[0], combined_range[1])\n",
    "\n",
    "\n",
    "def _extend_tag_data_ranges_to_balance_curly_braces(\n",
    "        tag_tuple: HTMLTagWithIndices,\n",
    "        main_text: str\n",
    "        ) -> list[HTMLTagWithIndices]:\n",
    "    \"\"\"\n",
    "    Extend tag data to balance curly braces\n",
    "    \"\"\"\n",
    "    combined_range = [tag_tuple.start, tag_tuple.end]\n",
    "    while not _is_balanced_braces(main_text[combined_range[0]:combined_range[1]]):\n",
    "        changed = False\n",
    "        while combined_range[0] != 0 and _first_curly_bracket(main_text[combined_range[0]:combined_range[1]]) == r'}':\n",
    "            combined_range[0] -= 1\n",
    "            changed = True\n",
    "        while combined_range[1] != len(main_text) and _last_curly_bracket(main_text[combined_range[0]:combined_range[1]]) == r'{':\n",
    "            combined_range[1] += 1\n",
    "            changed = True\n",
    "        if not changed:\n",
    "            break\n",
    "    return _update_tag_data(tag_tuple, main_text, combined_range[0], combined_range[1])\n",
    "\n",
    "\n",
    "def _cutoff_notation_tag_data(\n",
    "        main_text: str,\n",
    "        tag_data: list[HTMLTagWithIndices],\n",
    "        ) -> list[HTMLTagWithIndices]:\n",
    "    \"\"\"\n",
    "    Helper function to `_consolidate_token_preds`.\n",
    "\n",
    "    Guarantees that a notation tag is a pure math mode latex string\n",
    "    by cutting only the pure math mode string\n",
    "    that occurs within it. Assumes that `_extend_tag_data_ranges_to_encompass_latex`\n",
    "    works as intended.\n",
    "    \"\"\"\n",
    "    cutout_notation_tag_data: list[HTMLTagWithIndices] = []\n",
    "    for tag, start, end in tag_data:\n",
    "        if not 'notation' in tag.attrs:\n",
    "            cutout_notation_tag_data.append(HTMLTagWithIndices(tag, start, end))\n",
    "            continue\n",
    "        tag_text = main_text[start:end]\n",
    "        tex_inds_in_tagged = latex_indices(tag_text)\n",
    "        for sub_start, sub_end in tex_inds_in_tagged:\n",
    "            tex_str = main_text[start+sub_start:start+sub_end]\n",
    "            cutout_tag = _make_tag(tex_str, 'notation')\n",
    "            cutout_notation_tag_data.append(\n",
    "                HTMLTagWithIndices(cutout_tag, start+sub_start, start+sub_end))\n",
    "    return cutout_notation_tag_data\n",
    "\n",
    "\n",
    "\n",
    "def _no_overlap_with_previous_tag_data(\n",
    "        ultimate_tag_data: list[HTMLTagWithIndices],\n",
    "        current_tag_data: HTMLTagWithIndices  # Current tag data\n",
    "        ) -> bool:\n",
    "    \"\"\"\n",
    "    Return `True`, if the `current_tag_data` does not overlap with\n",
    "    any tag data that will be ultimately added. \n",
    "\n",
    "    Helper function for `_consolidate_token_preds`.\n",
    "    \"\"\"\n",
    "    for prev in reversed(ultimate_tag_data):\n",
    "        if _ranges_overlap(prev, current_tag_data):\n",
    "            return False\n",
    "    return True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "main_text = 'Hi. This is some text. Here is a notation $$M_k :=... $$ and here is some more $$G_k :=...$$'\n",
    "\n",
    "soup = bs4.BeautifulSoup('', 'html.parser')\n",
    "# In actuality, the tag data will have more information, but the following\n",
    "# is good enough\n",
    "# for the purposes of this test\n",
    "tag_1 = soup.new_tag('span', notation='')\n",
    "tag_2 = soup.new_tag('span', notation='')\n",
    "# The following tries to test an erroneous short/incomplete marking of the very first \n",
    "# dollar sign `$` as well as the subtext 'G_K ' of the second math mode text.\n",
    "\n",
    "tag_data = [HTMLTagWithIndices(tag_1, 42, 43), HTMLTagWithIndices(tag_2, 81, 85)]\n",
    "output = _consolidate_token_preds(main_text, tag_data)\n",
    "test_eq(main_text[output[0][1]: output[0][2]], '$$M_k :=... $$')\n",
    "test_eq(main_text[output[1][1]: output[1][2]], '$$G_k :=...$$')\n",
    "\n",
    "\n",
    "# In the following example, the notation is a priori\n",
    "# found to be 'zeta(s)$ as $$\\zeta'. \n",
    "# So first, the tagged data is extended to encompass\n",
    "# '$\\zeta(s)$ as $$\\zeta(s) = ...$$' and then pure latex\n",
    "# math mode str are extracted\n",
    "main_text = r'Define $\\zeta(s)$ as $$\\zeta(s) = ...$$  '\n",
    "\n",
    "soup = bs4.BeautifulSoup('', 'html.parser')\n",
    "tag_1 = soup.new_tag('span', notation='')\n",
    "tag_data = [HTMLTagWithIndices(tag_1,9,27)]\n",
    "output = _consolidate_token_preds(main_text, tag_data)\n",
    "# main_text.find('zeta')\n",
    "test_eq(main_text[output[0][1]:output[0][2]], r'$\\zeta(s)$')\n",
    "test_eq(main_text[output[1][1]:output[1][2]], r'$$\\zeta(s) = ...$$')\n",
    "\n",
    "\n",
    "\n",
    "# In the following example, we have erroneous\n",
    "# notation and definition markings which start/end within the same \n",
    "# math mode string.\n",
    "# In this case, the preceding marking takes precedence and \n",
    "# the latter overlapping marking is discarded;\n",
    "# this is an unfortunate feature that must be implemented to get\n",
    "# around shortcomings of the model.\n",
    "main_text = r'The Riemann zeta function $\\zeta(s)$ is defined as...'\n",
    "soup = bs4.BeautifulSoup('', 'html.parser')\n",
    "tag_1 = soup.new_tag('b', definition='')\n",
    "tag_2 = soup.new_tag('span', notation='')\n",
    "tag_data = [HTMLTagWithIndices(tag_1,4,27), HTMLTagWithIndices(tag_2,26,30)] # the tag predictions overlap at $ \\zeta(s)$.\n",
    "output = _consolidate_token_preds(main_text, tag_data)\n",
    "test_eq(main_text[output[0][1]:output[0][2]], r'Riemann zeta function $\\zeta(s)$')\n",
    "\n",
    "# In the following example, the definition tag prediction ends within the argument\n",
    "# of the \\emph command. \n",
    "main_text = r'The \\emph{arc complex}'\n",
    "soup = bs4.BeautifulSoup('', 'html.parser')\n",
    "tag = soup.new_tag('b', definition=\"\")\n",
    "tag_data = [HTMLTagWithIndices(tag,4,21)]\n",
    "output = _consolidate_token_preds(main_text, tag_data)\n",
    "test_eq(main_text[output[0][1]:output[0][2]], r'\\emph{arc complex}')\n",
    "\n",
    "# In the following example, the definition tag prediction ends amidst of the \\emph command. \n",
    "main_text = r'The \\emph{arc complex}'\n",
    "soup = bs4.BeautifulSoup('', 'html.parser')\n",
    "tag = soup.new_tag('b', definition=\"\")\n",
    "tag_data = [HTMLTagWithIndices(tag,6,8)]\n",
    "output = _consolidate_token_preds(main_text, tag_data)\n",
    "test_eq(main_text[output[0][1]:output[0][2]], r'\\emph{arc complex}')\n",
    "\n",
    "# In the following example, a bad command gets predicted. \n",
    "# The consolidation should thus leave out this prediction.\n",
    "main_text = r'\\section{lalalala} We define a homotopy'\n",
    "soup = bs4.BeautifulSoup('', 'html.parser')\n",
    "tag = soup.new_tag('b', definition=\"\")\n",
    "tag_data = [HTMLTagWithIndices(tag,0,2)]\n",
    "output = _consolidate_token_preds(main_text, tag_data)\n",
    "test_eq(len(output), 0)\n",
    "# print(output)\n",
    "\n",
    "# In the following example, the definition tag prediction ends with a punctuation mark\n",
    "# (a period in this case). \n",
    "main_text = r'This is called a homotopy. There exist...'\n",
    "soup = bs4.BeautifulSoup('', 'html.parser')\n",
    "tag = soup.new_tag('b', definition=\"\")\n",
    "tag_data = [HTMLTagWithIndices(tag,17,24)]\n",
    "output = _consolidate_token_preds(main_text, tag_data)\n",
    "test_eq(main_text[output[0][1]:output[0][2]], r'homotopy')\n",
    "\n",
    "# The `_extend_tag_data_range_to_border_space_or_punc` function should not try to\n",
    "# encapsulate the period as part of the definition tag.\n",
    "tag_data = [HTMLTagWithIndices(tag,18,23)]\n",
    "output = _consolidate_token_preds(main_text, tag_data)\n",
    "test_eq(main_text[output[0][1]:output[0][2]], r'homotopy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _html_tags_from_token_preds(\n",
    "        main_text: str,\n",
    "        token_preds: list[dict[str]], # An output of `pipeline(text)`; Each dict likely contains keys such as `'entity'`, `'score'`, `'index'`, `'word'`, `'start'`, and `'end'`, depending on the model used.\n",
    "        excessive_space_threshold: int,\n",
    "        note: Optional[VaultNote] = None,\n",
    "        ) -> list[HTMLTagWithIndices]:  # Tag element, start, end, where main_text[start:end] needs to be replaced by the tag element.\n",
    "        # ) -> list[tuple[bs4.element.Tag, int, int]]:  # Tag element, start, end, where main_text[start:end] needs to be replaced by the tag element.\n",
    "    \"\"\"\n",
    "    Return HTML tags for definition and notation classification.\n",
    "\n",
    "    Helper function to `auto_mark_def_and_notats`.\n",
    "    \"\"\"\n",
    "    parts: list[list[dict[str]]] = _divide_token_preds_into_parts(\n",
    "        token_preds, excessive_space_threshold, note)\n",
    "    return [_html_tag_data_from_part(main_text, part) for part in parts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# TODO: test _html_tags_from_token_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _collate_html_tags(\n",
    "        tag_data_1: list[HTMLTagWithIndices],\n",
    "        tag_data_2: list[HTMLTagWithIndices],\n",
    "    ) -> list[tuple[bs4.element.Tag], int, int]:\n",
    "    \"\"\"\n",
    "    Collates the lists of HTML tags and the indices within a certain text\n",
    "    (which is not-needed for this function and hence not included)\n",
    "    that the HTML tags need to replace.\n",
    "\n",
    "    If there are entries in `tag_data_1` and `tag_data_2` with overlapping\n",
    "    ranges, then the entry from `tag_data_1` is prioritized and the entry\n",
    "    from `tag_data_2` is discarded.\n",
    "\n",
    "    Helper function to `auto_mark_def_and_notats`\n",
    "    \"\"\"\n",
    "    collated_list: list[HTMLTagWithIndices] = []\n",
    "    i, j = 0, 0\n",
    "    while i < len(tag_data_1) and j < len(tag_data_2):\n",
    "        current_1 = tag_data_1[i]\n",
    "        current_2 = tag_data_2[j]\n",
    "        if _ranges_overlap(current_1, current_2): # Ignore current_2\n",
    "            j += 1\n",
    "            continue\n",
    "        if current_1[1] > current_2[1]:\n",
    "            collated_list.append(current_2)\n",
    "            j += 1\n",
    "        else:\n",
    "            collated_list.append(current_1)\n",
    "            i += 1\n",
    "    while i < len(tag_data_1):\n",
    "        collated_list.append(tag_data_1[i])\n",
    "        i += 1\n",
    "    while j < len(tag_data_2):\n",
    "        collated_list.append(tag_data_2[j])\n",
    "        j += 1\n",
    "    return collated_list\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "\n",
    "tag_data_1 = [\n",
    "    HTMLTagWithIndices('', 0, 1),\n",
    "    HTMLTagWithIndices('', 9, 12),\n",
    "    HTMLTagWithIndices('', 20, 21)\n",
    "]\n",
    "\n",
    "tag_data_2 = [\n",
    "    HTMLTagWithIndices('', 2, 4),\n",
    "    HTMLTagWithIndices('', 6, 7),\n",
    "    HTMLTagWithIndices('', 8, 10), # This should be discarded\n",
    "    HTMLTagWithIndices('', 10, 13), # This should be discarded\n",
    "    HTMLTagWithIndices('', 17, 20),\n",
    "    HTMLTagWithIndices('', 21, 24)\n",
    "]\n",
    "output = _collate_html_tags(tag_data_1, tag_data_2)\n",
    "test_eq(output, [\n",
    "    HTMLTagWithIndices('', 0, 1), HTMLTagWithIndices('', 2, 4), HTMLTagWithIndices('', 6, 7),\n",
    "    HTMLTagWithIndices('', 9, 12), HTMLTagWithIndices('', 17, 20), HTMLTagWithIndices('', 20, 21), \n",
    "    HTMLTagWithIndices('', 21, 24)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _add_nice_boxing_attrs_to_def_and_notat_tags(\n",
    "        html_tag_data: list[HTMLTagWithIndices]\n",
    "        ) -> list[HTMLTagWithIndices]:\n",
    "    \"\"\"\n",
    "    Add HTML tag attributes to draw boxes around notation data\n",
    "\n",
    "    Helper function to `auto_mark_def_and_notats`.\n",
    "    \"\"\"\n",
    "    listy: list[HTMLTagWithIndices] = []\n",
    "    for tag, start, end in html_tag_data:\n",
    "        if ('notation' in tag.attrs or 'definition' in tag.attrs) and 'style' not in tag.attrs:\n",
    "            tag.attrs['style'] = \"border-width:1px;border-style:solid;padding:3px\"\n",
    "        listy.append(HTMLTagWithIndices(tag, start, end)) \n",
    "    return listy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "soup = bs4.BeautifulSoup('', \"html.parser\")\n",
    "tag = soup.new_tag(\"span\", notation=\"\")\n",
    "tag.string = 'hi'\n",
    "tag_data = [\n",
    "    HTMLTagWithIndices(tag, 0, 2),\n",
    "]\n",
    "output = _add_nice_boxing_attrs_to_def_and_notat_tags(tag_data)\n",
    "assert \"style\" in output[0][0].attrs\n",
    "\n",
    "tag = soup.new_tag(\"span\", definition=\"\")\n",
    "tag.string = 'hi'\n",
    "tag_data = [\n",
    "    HTMLTagWithIndices(tag, 0, 2),\n",
    "]\n",
    "output = _add_nice_boxing_attrs_to_def_and_notat_tags(tag_data)\n",
    "assert \"style\" in output[0][0].attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def def_and_notat_preds_by_model(\n",
    "        text: str,  \n",
    "        pipeline # The pipeline object created using the token classification model and its tokenizer\n",
    "        ) -> list[HTMLTagWithIndices]: # HTMLTAgWithIndices consists of an HTML tag carrying the data of the prediction and ints marking where in `text` the definition or notation is at.\n",
    "    \"\"\"\n",
    "    Predict where definitions and notations occur in `text`\n",
    "\n",
    "    This function uses some of the same helper functions as\n",
    "    `auto_mark_def_and_notats`, but does not raise warning messages as\n",
    "    in `auto_mark_def_and_notats`.\n",
    "    \"\"\"\n",
    "    tag_data = _html_tags_from_token_preds(text, pipeline(text), 2, None)\n",
    "    return tag_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# def _process_mf(\n",
    "#         mf: MarkdownFile) -> None:\n",
    "#     \"\"\"\n",
    "#     Merge display math mode as needed\n",
    "\n",
    "#     Helper function to `auto_mark_def_and_notats`\n",
    "#     \"\"\"\n",
    "#     # mf.merge_display_math_mode()\n",
    "#     # mf.merge_display_math_mode_into_preceding_text()\n",
    "#     mf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _get_main_text_lines(\n",
    "        mf: MarkdownFile) -> tuple[int, int]:\n",
    "    \"\"\"Helper function to `auto_mark_def_and_notats`\"\"\"\n",
    "    tuppy = mf.metadata_lines()\n",
    "    if tuppy is not None:\n",
    "        first_non_metadata_line = tuppy[1] + 1\n",
    "    else:\n",
    "        first_non_metadata_line = 0 \n",
    "    see_also_line = mf.get_line_number_of_heading('See Also')\n",
    "    return first_non_metadata_line, see_also_line\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _append_to_pieces_start_and_end(\n",
    "        pieces_start_and_end: list[tuple[int, int]],\n",
    "        start_chunk: tuple[str, int, int],\n",
    "        end_chunk: tuple[str, int, int]\n",
    "        ) -> None:\n",
    "    \"\"\"Helper function to `_find_places_to_divide_from_chunks`\"\"\"\n",
    "    start_char_index = start_chunk[1]\n",
    "    end_char_index = end_chunk[1] + len(end_chunk[0])\n",
    "    pieces_start_and_end.append([start_char_index, end_char_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _find_places_to_divide_from_chunks(\n",
    "        chunks: list[tuple[str, int, int]], # The str is a chunk of text, the first int is the index in `main_text` that the chunk starts at, and the second int is the approximate token length of the text. Appending all the chunks of text as they are should result back in the original text.\n",
    "        pipeline: pipelines.token_classification.TokenClassificationPipeline, # The token classification pipeline that is used to predict whether tokens are part of definitions or notations introduced in the text. Here, the tokenizer of this pipeline is used to estimate how many tokens a piece of subtext will have.\n",
    "        ) -> list[tuple[int, int]]: # Each tuple is a start and end range for pieces of `main_text` to be considered for predictions\n",
    "    \"\"\"Identify appropriate indices in `main_text` where (overlapping)\n",
    "    pieces in `main_text` should start/end for predictions with `pipeline`.\n",
    "    \n",
    "    Helper function to `_divide_main_text`.\n",
    "\n",
    "    We describe how this function is implemented: starting at the first chunk\n",
    "    (chunks are non-overlapping), start to consider consecutive chunks to\n",
    "    make up a piece. So maybe we have chunks\n",
    "\n",
    "        A B C D E F ....\n",
    "\n",
    "    We build a piece chunk-by-chunk, considering the total token length of the\n",
    "    built sub-piece along the way. The first chunk within a sub-piece \n",
    "    that makes the sub-piece of token-length greater than half the max\n",
    "    token length with respect to `pipeline.tokenizer` will become the start of the\n",
    "    next piece, unless the very first chunk in the piece is already longer than half the max\n",
    "    token length with respect to the tokenizer (this is to ensure that the\n",
    "    piece-building process does not keep starting at the same chunk).\n",
    "    Moreover, a piece will stop building as soon as its token-length exceeds\n",
    "    the max length of the tokenizer.\n",
    "\n",
    "    For instance, maybe the max length (`tokenizer.model_max_length`)\n",
    "    for the tokenizer is 512, and the chunks\n",
    "    are of the following length:\n",
    "\n",
    "        A   B    C  D   E    F     ...\n",
    "        76  130  70 13  150  140   ...\n",
    "\n",
    "    We first build the piece starting at A:\n",
    "\n",
    "        A\n",
    "        76\n",
    "\n",
    "    We continue building the piece by \"appending\" B:\n",
    "\n",
    "        A   B\n",
    "        76  130\n",
    "\n",
    "    Once we append C as well, the piece's length is now 276 and hence over half of 512,\n",
    "    so the next piece will start at C: \n",
    "\n",
    "        A   B    C\n",
    "        76  130  70\n",
    "\n",
    "    Subsequently, we continue building the piece. Only once F is appended does the \n",
    "    length of the entire piece exceed 512 (the length is 579):\n",
    "\n",
    "        A   B    C  D   E    F\n",
    "        76  130  70 13  150  140\n",
    "    \n",
    "    And then we begin building the next piece from C.\n",
    "\n",
    "    Also, consider an example where the first chunk's length exceeds half the max length\n",
    "    of the tokenizer:\n",
    "\n",
    "        A   B    C   ...\n",
    "        300 200  100 ...\n",
    "\n",
    "    Here, the first piece will consist of the chunks A, B, and C because\n",
    "    the length of the piece exceeds the max length of 512 only after appending C.\n",
    "    To guarantee that the next piece does not start with the chunk A again, B is \n",
    "    used as the first chunk in the next piece:\n",
    "\n",
    "        B    C   ...\n",
    "        200  100 ...\n",
    "\n",
    "    If any chunk's token length exceeds the tokenizer's max_model_length, then\n",
    "    the pipeline/model can only predict on the starting tokens in the chunk. \n",
    "    As such, the chunks must not be \"too long\" for best results on the model's predictions.\n",
    "    \"\"\"\n",
    "    tokenizer = pipeline.tokenizer\n",
    "    start_chunk_index, next_piece_start_chunk_index = 0, 0\n",
    "    current_piece_token_len = 0\n",
    "    pieces_start_and_end = []\n",
    "    i = 0\n",
    "    while i < len(chunks):\n",
    "        chunk = chunks[i]\n",
    "        current_piece_token_len = current_piece_token_len + chunk[2]\n",
    "        if (current_piece_token_len > tokenizer.model_max_length / 2\n",
    "                and start_chunk_index == next_piece_start_chunk_index):\n",
    "            # Mark where the next piece should start\n",
    "            next_piece_start_chunk_index = i if start_chunk_index != i else i+1\n",
    "        if (current_piece_token_len > tokenizer.model_max_length):\n",
    "            _append_to_pieces_start_and_end(\n",
    "                pieces_start_and_end, chunks[start_chunk_index], chunk)\n",
    "            i, start_chunk_index = (\n",
    "                next_piece_start_chunk_index, next_piece_start_chunk_index)\n",
    "            current_piece_token_len = 0\n",
    "            continue\n",
    "        i += 1\n",
    "    # Add the last chunk at the end\n",
    "    _append_to_pieces_start_and_end(\n",
    "        pieces_start_and_end, chunks[start_chunk_index], chunks[-1])\n",
    "    return pieces_start_and_end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _divide_main_text(\n",
    "        main_text: str,\n",
    "        pipeline: pipelines.token_classification.TokenClassificationPipeline, # The token classification pipeline that is used to predict whether tokens are part of definitions or notations introduced in the text. Here, the tokenizer of this pipeline is used to estimate how many tokens a piece of subtext will have.\n",
    "        # ) -> list[tuple[str, int, int]]:  # The str is a chunk of text, the first int is the index in `main_text` that the chunk starts at, and the second int is the approximate token length of the text. Appending all the chunks of text as they are should result back in the original text.\n",
    "        ) -> list[tuple[int, int]]:  # Each tuple is a start and end range for pieces of `main_text` to be considered for predictions\n",
    "    \"\"\"Divides `main_text` so that predictions can be made on smaller chunks of text.\n",
    "    \n",
    "    Assumes that dividing `main_text` along newline characters `\\n` will result in\n",
    "    pieces that are \"not too long\".\n",
    "\n",
    "    Helper function to `_format_main_text_and_add_html_tag_data`.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    main_text.split('\\n')\n",
    "    tokenizer = pipeline.tokenizer\n",
    "    newline_indices = [i for i, char in enumerate(main_text) if char == '\\n']\n",
    "    newline_indices.insert(0, 0)\n",
    "    chunks = []  # list[tuple[str, int, int]]  # The str is a chunk of text, the first int is the index in `main_text` that the chunk starts at, and the second int is the approximate token length of the text. Appending all the chunks of text as they are should result back in the original text.\n",
    "    for start, end in pairwise(newline_indices):\n",
    "        chunk = main_text[start:end]\n",
    "        chunks.append((chunk, start, len(tokenizer(chunk)['input_ids'])))\n",
    "    last_chunk = main_text[newline_indices[-1]:]\n",
    "    chunks.append((last_chunk, newline_indices[-1], len(tokenizer(last_chunk)['input_ids'])))\n",
    "    return _find_places_to_divide_from_chunks(chunks, pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _get_token_preds_by_dividing_main_text(\n",
    "        main_text: str,\n",
    "        pipeline: pipelines.token_classification.TokenClassificationPipeline, # The token classification pipeline that is used to predict whether tokens are part of definitions or notations introduced in the text. Here, the tokenizer of this pipeline is used to estimate how many tokens a piece of subtext will have.\n",
    "        excessive_space_threshold: int, \n",
    "        note: Optional[VaultNote] = None,\n",
    "        # ) -> list[tuple[bs4.element.Tag, int, int]]:  # Tag element, start, end, where main_text[start:end] needs to be replaced by the tag element.\n",
    "        ) -> list[HTMLTagWithIndices]:  # Tag element, start, end, where main_text[start:end] needs to be replaced by the tag element.\n",
    "    \"\"\"\n",
    "    Divide the `main_text` into not-too-long pieces to return HTML tag predictions\n",
    "\n",
    "    Helper function for `_format_main_text_and_add_html_tag_data`.\n",
    "    \"\"\"\n",
    "    pieces_start_and_end = _divide_main_text(main_text, pipeline)\n",
    "    cumulative_html_tags_in_main = []\n",
    "    for start_of_piece, end_of_piece in pieces_start_and_end:\n",
    "        # text = main_text[start_of_piece:end_of_piece]\n",
    "        text = main_text[start_of_piece:]\n",
    "        html_tags_in_piece: list[HTMLTagWithIndices] = _html_tags_from_token_preds(\n",
    "            text, pipeline(text), excessive_space_threshold, note)\n",
    "        html_tags_in_piece = _consolidate_token_preds(\n",
    "            text, html_tags_in_piece)\n",
    "        # start and end indices need to be re-adjusted with respect to their places in `main_text`\n",
    "        html_tags_for_piece_in_main_text: list[HTMLTagWithIndices] = [\n",
    "            HTMLTagWithIndices(tag, start_of_piece + start, start_of_piece + end)\n",
    "            for tag, start, end in html_tags_in_piece]\n",
    "        cumulative_html_tags_in_main = _collate_html_tags(\n",
    "            cumulative_html_tags_in_main, html_tags_for_piece_in_main_text)\n",
    "    return cumulative_html_tags_in_main\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_def_and_notat_predictions(\n",
    "        main_text: str,\n",
    "        pipeline: pipelines.token_classification.TokenClassificationPipeline,\n",
    "        excessive_space_threshold: int,\n",
    "        note: Optional[VaultNote] = None\n",
    "        ) -> list[HTMLTagWithIndices]:\n",
    "    \"\"\"\n",
    "    Identifies definitions and notations in the text using the ML pipeline.\n",
    "\n",
    "    Returns a list of `HTMLTagWithIndices` containing the predicted tags \n",
    "    and their start/end indices in `main_text`.\n",
    "    \"\"\"\n",
    "    # 1. Divide text (if needed) and get raw token predictions\n",
    "    html_tags_to_add = _get_token_preds_by_dividing_main_text(\n",
    "        main_text, pipeline, excessive_space_threshold, note)\n",
    "    \n",
    "    # 2. Return the structured data directly\n",
    "    # Note: _get_token_preds_by_dividing_main_text already returns \n",
    "    # list[HTMLTagWithIndices] after calling `consolidate_token_preds`.\n",
    "    return html_tags_to_add\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#| export\n",
    "def mark_def_and_notat_predictions(\n",
    "        main_text: str, # The original text.\n",
    "        predictions: list[HTMLTagWithIndices], # The list of predictions (ranges and metadata) returned by `get_def_and_notat_predictions`. Assumes these are sorted and non-overlapping (which `get_def_and_notat_predictions` ensures).\n",
    "        formatter: Callable[[str, HTMLTagWithIndices], str] # A function that takes the *extracted substring* (the text being marked) and the prediction object, and returns the *formatted string* to replace it with.\n",
    "        ) -> str: # The modified text with predictions marked.\n",
    "    \"\"\"\n",
    "    Applies formatting to the `main_text` based on the definition and notation predictions.\n",
    "    \"\"\"\n",
    "    # Iterate backwards so that index changes don't affect subsequent replacements\n",
    "    # (HTMLTagWithIndices are typically sorted by start index, so reversing is safe)\n",
    "    sorted_preds = sorted(predictions, key=lambda x: x.start, reverse=True)\n",
    "    \n",
    "    formatted_text_list = list(main_text)\n",
    "    \n",
    "    for pred in sorted_preds:\n",
    "        start, end = pred.start, pred.end\n",
    "        original_substring = main_text[start:end]\n",
    "        \n",
    "        # Apply the user-defined formatting rule\n",
    "        replacement_text = formatter(original_substring, pred)\n",
    "        \n",
    "        # Replace the slice in the text\n",
    "        # (Using list slicing for efficiency vs repeated string concatenation)\n",
    "        formatted_text_list[start:end] = list(replacement_text)\n",
    "        \n",
    "    return \"\".join(formatted_text_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1 Passed: The [DEF:Galois group] [NOT:$\\operatorname{Gal}(L/K)$] is...\n",
      "Test 2 Passed: The <b definition=\"\">Galois group</b> <span notation=\"\">$\\operatorname{Gal}(L/K)$</span> is...\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "# String: \"The Galois group $\\operatorname{Gal}(L/K)$ is...\"\n",
    "# Indices breakdown:\n",
    "# \"The Galois group \" -> len 17 (indices 0-16)\n",
    "# \"$\" -> index 17\n",
    "# \"\\operatorname{Gal}(L/K)\" -> len 23\n",
    "# \"$\" -> index 17 + 1 + 23 = 41\n",
    "# String slice [17:42] covers \"$\\operatorname{Gal}(L/K)$\"\n",
    "text = r\"The Galois group $\\operatorname{Gal}(L/K)$ is...\"\n",
    "soup = BeautifulSoup(\"\", 'html.parser')\n",
    "\n",
    "# Pred 1: Definition \"Galois group\" (indices 4-16)\n",
    "tag_def = soup.new_tag(\"b\", definition=\"\")\n",
    "pred_def = HTMLTagWithIndices(tag_def, 4, 16)\n",
    "\n",
    "# Pred 2: Notation \"$\\operatorname{Gal}(L/K)$\"\n",
    "# We want to wrap the WHOLE math string including $.\n",
    "# Start: 17\n",
    "# End: 42 (17 + 25 chars)\n",
    "tag_notat = soup.new_tag(\"span\", notation=\"\")\n",
    "pred_notat = HTMLTagWithIndices(tag_notat, 17, 42)\n",
    "\n",
    "predictions = [pred_def, pred_notat]\n",
    "\n",
    "def bracket_formatter(subtext, pred):\n",
    "    label = \"DEF\" if 'definition' in pred.tag.attrs else \"NOT\"\n",
    "    return f\"[{label}:{subtext}]\"\n",
    "\n",
    "result = mark_def_and_notat_predictions(text, predictions, bracket_formatter)\n",
    "\n",
    "# Expected: \"The [DEF:Galois group] [NOT:$\\operatorname{Gal}(L/K)$] is...\"\n",
    "expected_str = r\"The [DEF:Galois group] [NOT:$\\operatorname{Gal}(L/K)$] is...\"\n",
    "\n",
    "assert result == expected_str, f\"Test 1 Failed.\\nExpected: {expected_str}\\nGot:      {result}\"\n",
    "print(\"Test 1 Passed:\", result)\n",
    "\n",
    "# --- Test 2: HTML Formatter ---\n",
    "# Checks that we can regenerate standard HTML tags\n",
    "def simple_html_formatter(subtext, pred):\n",
    "    # Important: Clone the tag if you want to be pure, \n",
    "    # but modifying pred.tag is usually fine for one-pass formatting.\n",
    "    pred.tag.string = subtext\n",
    "    return str(pred.tag)\n",
    "\n",
    "result_html = mark_def_and_notat_predictions(text, predictions, simple_html_formatter)\n",
    "\n",
    "# Expected: \"The <b definition=\"\">Galois group</b> <span notation=\"\">$\\operatorname{Gal}(L/K)$</span> is...\"\n",
    "expected_html_fragment_1 = r'<b definition=\"\">Galois group</b>'\n",
    "expected_html_fragment_2 = r'<span notation=\"\">$\\operatorname{Gal}(L/K)$</span>'\n",
    "\n",
    "assert expected_html_fragment_1 in result_html\n",
    "assert expected_html_fragment_2 in result_html\n",
    "print(\"Test 2 Passed:\", result_html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unittest.mock import MagicMock\n",
    "\n",
    "class MockTokenizer:\n",
    "    \"\"\"Mocks a HuggingFace tokenizer.\"\"\"\n",
    "    model_max_length = 512\n",
    "    \n",
    "    def __call__(self, text):\n",
    "        # CORRECTED: Return a dict, not SimpleNamespace\n",
    "        length = len(text) // 5 + 1\n",
    "        return {'input_ids': [0] * length}\n",
    "\n",
    "# Update the pipeline to use the corrected tokenizer\n",
    "class MockPipeline:\n",
    "    \"\"\"Mocks the TokenClassificationPipeline.\"\"\"\n",
    "    def __init__(self, preds):\n",
    "        self.tokenizer = MockTokenizer()\n",
    "        self._preds = preds\n",
    "\n",
    "    def __call__(self, text):\n",
    "        return self._preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#| hide\n",
    "from types import SimpleNamespace\n",
    "\n",
    "# 1. Setup Mock Note\n",
    "mock_note = SimpleNamespace(name=\"Test Note\", path=lambda: \"test/path/Test Note.md\") \n",
    "\n",
    "# 2. Setup Input Text\n",
    "# \"The Galois group $\\operatorname{Gal}(L/K)$ of the extension L/K is...\"\n",
    "# Indices:\n",
    "# T=0, h=1, e=2,  =3\n",
    "# G=4, a=5, l=6, o=7, i=8, s=9,  =10\n",
    "# g=11, r=12, o=13, u=14, p=15\n",
    "#  =16\n",
    "# $=17, \\=18, o=19, p=20 ...\n",
    "text_input = r\"The Galois group $\\operatorname{Gal}(L/K)$ of the extension L/K is...\"\n",
    "\n",
    "# 3. Setup Mock Predictions\n",
    "# We simulate the model finding:\n",
    "# A. 'Galois group' as a definition.\n",
    "# B. 'Gal' (inside the latex) as a notation.\n",
    "mock_preds = [\n",
    "    # Definition: \"Galois group\" (indices 4-16)\n",
    "    {\n",
    "        'entity': 'B-definition', 'score': 0.99, 'index': 1, \n",
    "        'word': 'Galois', 'start': 4, 'end': 10\n",
    "    },\n",
    "    {\n",
    "        'entity': 'I-definition', 'score': 0.99, 'index': 2, \n",
    "        'word': 'group', 'start': 11, 'end': 16\n",
    "    },\n",
    "    # Notation: Model finds 'Gal' inside '$\\operatorname{Gal}(L/K)$'\n",
    "    # '$\\operatorname{Gal}(L/K)$' starts at 17.\n",
    "    # $ = 17\n",
    "    # \\operatorname{ = 18 to 31 (len 13)\n",
    "    # Gal = 31 to 34\n",
    "    {\n",
    "        'entity': 'B-notation', 'score': 0.98, 'index': 3, \n",
    "        'word': 'Gal', 'start': 31, 'end': 34\n",
    "    }\n",
    "]\n",
    "\n",
    "mock_pipeline = MockPipeline(mock_preds)\n",
    "\n",
    "# 4. Run Function\n",
    "preds = get_def_and_notat_predictions(\n",
    "    main_text=text_input,\n",
    "    pipeline=mock_pipeline,\n",
    "    excessive_space_threshold=2,\n",
    "    note=mock_note,\n",
    ")\n",
    "\n",
    "# 5. Assertions\n",
    "\n",
    "# Verify we got 2 items\n",
    "assert len(preds) == 2, f\"Expected 2 predictions, got {len(preds)}\"\n",
    "\n",
    "# --- Check Definition (\"Galois group\") ---\n",
    "# The function might return them in order of appearance. \n",
    "# 'Galois group' starts at 4, '$\\operatorname...$' starts at 17.\n",
    "def_pred = preds[0]\n",
    "assert def_pred.tag.text.strip() == 'Galois group'\n",
    "assert 'definition' in def_pred.tag.attrs\n",
    "assert def_pred.start == 4\n",
    "assert def_pred.end == 16\n",
    "\n",
    "# --- Check Notation (\"$\\operatorname{Gal}(L/K)$\") ---\n",
    "# The model predicted 'Gal' (31-34).\n",
    "# The logic should assume 'Gal' implies the surrounding math mode string is the notation.\n",
    "# The full math string is `$\\operatorname{Gal}(L/K)$`.\n",
    "# Start index: 17 ($)\n",
    "# End index: 17 + len('$\\operatorname{Gal}(L/K)$') = 17 + 25 = 42\n",
    "notat_pred = preds[1]\n",
    "assert 'notation' in notat_pred.tag.attrs\n",
    "assert notat_pred.start == 17\n",
    "assert notat_pred.end == 42 # Check length of string manually if this fails\n",
    "assert notat_pred.tag.text == r'$\\operatorname{Gal}(L/K)$'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def predict_and_mark_def_and_notats(\n",
    "        main_text: str, # The text to run predictions on and format.\n",
    "        pipeline: pipelines.token_classification.TokenClassificationPipeline, # The token classification pipeline.\n",
    "        # note: VaultNote, # The note associated with the text (used for debugging/logging).\n",
    "        formatter: Callable[[str, HTMLTagWithIndices], str], # The function that formats the text based on predictions.\n",
    "        excessive_space_threshold: int # Threshold for detecting excessive spacing in predictions.\n",
    "        ) -> str: # The formatted text.\n",
    "    \"\"\"\n",
    "    Runs definition and notation detection on `main_text` and applies the `formatter`.\n",
    "    \"\"\"\n",
    "    predictions = get_def_and_notat_predictions(\n",
    "        main_text, pipeline, excessive_space_threshold)\n",
    "    return mark_def_and_notat_predictions(main_text, predictions, formatter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1 (HTML) Passed with fastcore!\n",
      "Test 2 (Bracket) Passed!\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from unittest.mock import MagicMock\n",
    "from fastcore.test import test_eq # Import fastcore's test_eq\n",
    "\n",
    "# --- Setup ---\n",
    "text_input = r\"The Galois group $\\operatorname{Gal}(L/K)$ is...\"\n",
    "\n",
    "mock_preds = [\n",
    "    {'entity': 'B-definition', 'score': 0.99, 'index': 1, 'word': 'Galois', 'start': 4, 'end': 10},\n",
    "    {'entity': 'I-definition', 'score': 0.99, 'index': 2, 'word': 'group', 'start': 11, 'end': 16},\n",
    "    {'entity': 'B-notation', 'score': 0.98, 'index': 3, 'word': 'Gal', 'start': 31, 'end': 34}\n",
    "]\n",
    "mock_pipeline = MockPipeline(mock_preds)\n",
    "\n",
    "# mock_note = MagicMock()\n",
    "# mock_note.name = \"Test Note\"\n",
    "# mock_note.path.return_value = \"test_note.md\"\n",
    "\n",
    "def html_formatter(subtext, pred):\n",
    "    pred.tag.string = subtext\n",
    "    return str(pred.tag)\n",
    "\n",
    "# --- Run Function ---\n",
    "result_html = predict_and_mark_def_and_notats(\n",
    "    main_text=text_input,\n",
    "    pipeline=mock_pipeline,\n",
    "    formatter=html_formatter,\n",
    "    excessive_space_threshold=2\n",
    ")\n",
    "\n",
    "# --- Define Expectations ---\n",
    "# Based on the output you saw earlier, we define the exact expected string.\n",
    "# We include the specific style attributes that seem to be generated by default in your environment.\n",
    "expected_html = (\n",
    "    r'The <b definition=\"\" style=\"border-width:1px;border-style:solid;padding:3px\">Galois group</b> '\n",
    "    r'<span notation=\"\" style=\"border-width:1px;border-style:solid;padding:3px\">$\\operatorname{Gal}(L/K)$</span> is...'\n",
    ")\n",
    "\n",
    "# --- Assertion with fastcore ---\n",
    "# test_eq will raise an error if they differ and print:\n",
    "# \"test_eq: expected vs actual\" followed by the two strings.\n",
    "# If they are long, it usually helps spot the difference.\n",
    "test_eq(result_html, expected_html)\n",
    "\n",
    "print(\"Test 1 (HTML) Passed with fastcore!\")\n",
    "\n",
    "\n",
    "# --- Test 2: Bracket Formatter (Debug/Sanity Check) ---\n",
    "def bracket_formatter(subtext, pred):\n",
    "    tag_type = \"DEF\" if 'definition' in pred.tag.attrs else \"NOT\"\n",
    "    return f\"[{tag_type}:{subtext}]\"\n",
    "\n",
    "result_bracket = predict_and_mark_def_and_notats(\n",
    "    main_text=text_input,\n",
    "    pipeline=mock_pipeline,\n",
    "    # note=mock_note,\n",
    "    formatter=bracket_formatter,\n",
    "    excessive_space_threshold=2\n",
    ")\n",
    "\n",
    "expected_bracket = r\"The [DEF:Galois group] [NOT:$\\operatorname{Gal}(L/K)$] is...\"\n",
    "assert result_bracket == expected_bracket, f\"Bracket test failed. Got: {result_bracket}\"\n",
    "print(\"Test 2 (Bracket) Passed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _format_main_text_and_add_html_tag_data(\n",
    "        note: VaultNote,\n",
    "        pipeline: pipelines.token_classification.TokenClassificationPipeline, # The token classification pipeline that is used to predict whether tokens are part of definitions or notations introduced in the text.\n",
    "        add_boxing_attr_to_existing_def_and_notat_markings: bool,\n",
    "        excessive_space_threshold: int,\n",
    "        main_text: str,  # The main text to format and to add HTML tag data to\n",
    "        ) -> str:\n",
    "    \"\"\"Helper function to `auto_mark_def_and_notats`\"\"\"\n",
    "    # 1. Pre-processing\n",
    "    main_text = add_space_to_lt_symbols_without_space(main_text)\n",
    "    main_text = convert_double_asterisks_to_html_tags(main_text)\n",
    "    main_text, existing_html_tag_data = remove_html_tags_in_text(main_text)\n",
    "\n",
    "    # 2. Handle existing tag styling\n",
    "    if add_boxing_attr_to_existing_def_and_notat_markings:\n",
    "        existing_html_tag_data = _add_nice_boxing_attrs_to_def_and_notat_tags(\n",
    "            existing_html_tag_data)\n",
    "\n",
    "    # 3. Get New Predictions\n",
    "    new_tag_data = get_def_and_notat_predictions(\n",
    "        main_text, pipeline, excessive_space_threshold, note)\n",
    "\n",
    "    # 4. Merge old and new tags\n",
    "    all_tags_to_add = _collate_html_tags(\n",
    "        existing_html_tag_data, new_tag_data)\n",
    "\n",
    "    # 5. Apply to text\n",
    "    return add_HTML_tag_data_to_raw_text(main_text, all_tags_to_add)\n",
    "    # html_tags_to_add = _get_token_preds_by_dividing_main_text(\n",
    "    #     main_text, pipeline, note, excessive_space_threshold)\n",
    "\n",
    "    # html_tags_to_add_back = _collate_html_tags(\n",
    "    #     existing_html_tag_data, html_tags_to_add)\n",
    "    # return add_HTML_tag_data_to_raw_text(main_text, html_tags_to_add_back)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hyunj\\Documents\\Development\\Python\\trouver\\trouver\\helper\\html.py:104: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  parsed_soup = BeautifulSoup(text, 'html.parser')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#| hide\n",
    "#| notest\n",
    "from types import SimpleNamespace\n",
    "from unittest.mock import MagicMock\n",
    "\n",
    "# --- Test Setup ---\n",
    "\n",
    "# 1. Setup a Mock Note\n",
    "# Use MagicMock so .path() is callable if needed, or .path works as a property\n",
    "mock_note = MagicMock()\n",
    "mock_note.name = \"Test Note\"\n",
    "mock_note.path.return_value = \"test/path/Test Note.md\" \n",
    "\n",
    "# 2. Setup Input Text\n",
    "# \"The Galois group $\\operatorname{Gal}(L/K)$ of the extension L/K is...\"\n",
    "# Indices:\n",
    "# T=0, h=1, e=2,  =3\n",
    "# G=4, a=5, l=6, o=7, i=8, s=9,  =10 (Galois)\n",
    "# g=11, r=12, o=13, u=14, p=15 (group)\n",
    "#  =16\n",
    "# $=17, \\=18 ... $\\operatorname{Gal}(L/K)$ ends at 42\n",
    "text_input = r\"The Galois group $\\operatorname{Gal}(L/K)$ of the extension L/K is...\"\n",
    "\n",
    "# 3. Setup Mock Predictions\n",
    "# We simulate the model finding:\n",
    "# A. 'Galois group' as a definition.\n",
    "# B. 'Gal' (inside the latex) as a notation.\n",
    "mock_preds = [\n",
    "    # Definition: \"Galois group\" (indices 4-16)\n",
    "    {\n",
    "        'entity': 'B-definition', 'score': 0.99, 'index': 1, \n",
    "        'word': 'Galois', 'start': 4, 'end': 10\n",
    "    },\n",
    "    {\n",
    "        'entity': 'I-definition', 'score': 0.99, 'index': 2, \n",
    "        'word': 'group', 'start': 11, 'end': 16\n",
    "    },\n",
    "    # Notation: Model finds 'Gal' inside '$\\operatorname{Gal}(L/K)$'\n",
    "    # We simulate it finding the substring at index 31-34 (inside the operatorname)\n",
    "    {\n",
    "        'entity': 'B-notation', 'score': 0.98, 'index': 3, \n",
    "        'word': 'Gal', 'start': 31, 'end': 34\n",
    "    }\n",
    "]\n",
    "\n",
    "mock_pipeline = MockPipeline(mock_preds)\n",
    "\n",
    "# --- Run Function ---\n",
    "formatted_text = _format_main_text_and_add_html_tag_data(\n",
    "    note=mock_note,\n",
    "    pipeline=mock_pipeline,\n",
    "    add_boxing_attr_to_existing_def_and_notat_markings=True,\n",
    "    excessive_space_threshold=2,\n",
    "    main_text=text_input\n",
    ")\n",
    "\n",
    "# --- Assertions ---\n",
    "\n",
    "# 1. Check if \"Galois group\" is wrapped in a definition tag\n",
    "# It should be boxed because we set add_boxing...=True\n",
    "# Note: exact tag name (b vs span) depends on your implementation.\n",
    "# We check for the key components.\n",
    "assert 'Galois group' in formatted_text\n",
    "assert 'definition=\"\"' in formatted_text\n",
    "assert 'style=\"border-width:1px;border-style:solid;padding:3px\"' in formatted_text\n",
    "\n",
    "# 2. Check if $\\operatorname{Gal}(L/K)$ is wrapped in a notation tag\n",
    "# The logic should have expanded 'Gal' to the full math string.\n",
    "expected_notation_str = r'$\\operatorname{Gal}(L/K)$'\n",
    "expected_notation_html = f'<span notation=\"\" style=\"border-width:1px;border-style:solid;padding:3px\">{expected_notation_str}</span>'\n",
    "\n",
    "assert expected_notation_html in formatted_text, \\\n",
    "    f\"Notation tag missing or incorrect.\\nExpected: {expected_notation_html}\\nGot: {formatted_text}\"\n",
    "\n",
    "# --- Test Case 2: Handling Existing Tags ---\n",
    "# Text with existing \"Extension field L\" definition\n",
    "text_with_existing = r'The <b definition=\"\">extension field $L$</b> is...'\n",
    "mock_pipeline_empty = MockPipeline([]) \n",
    "\n",
    "formatted_text_2 = _format_main_text_and_add_html_tag_data(\n",
    "    note=mock_note,\n",
    "    pipeline=mock_pipeline_empty,\n",
    "    add_boxing_attr_to_existing_def_and_notat_markings=True, \n",
    "    excessive_space_threshold=2,\n",
    "    main_text=text_with_existing\n",
    ")\n",
    "\n",
    "# Check if the 'style' attribute was added (boxing) to the existing bold tag\n",
    "assert 'style=\"border-width:1px;border-style:solid;padding:3px\"' in formatted_text_2, \\\n",
    "    \"Boxing attributes were not added to existing tag.\"\n",
    "assert '<b definition=\"\" style=' in formatted_text_2\n",
    "\n",
    "# --- Test Case 3: Double Asterisks Conversion ---\n",
    "# The function calls `convert_double_asterisks_to_html_tags` internally.\n",
    "# Example: **$K$**\n",
    "text_asterisks = r\"Let **$K$** be a field.\"\n",
    "formatted_text_3 = _format_main_text_and_add_html_tag_data(\n",
    "    note=mock_note,\n",
    "    pipeline=mock_pipeline_empty,\n",
    "    add_boxing_attr_to_existing_def_and_notat_markings=False,\n",
    "    excessive_space_threshold=2,\n",
    "    main_text=text_asterisks\n",
    ")\n",
    "\n",
    "# Should convert **...** to a tag (definition or notation depending on content/heuristics)\n",
    "# Usually math mode inside ** -> notation.\n",
    "assert '<span notation=\"\">$K$</span>' in formatted_text_3 or '<b definition=\"\">$K$</b>' in formatted_text_3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _write_text_with_html_tag_preds_to_note(\n",
    "        note: VaultNote,\n",
    "        mf: MarkdownFile,\n",
    "        main_text: str,\n",
    "        first_non_metadata_line: int,\n",
    "        see_also_line: int\n",
    "        ) -> None:\n",
    "    \"\"\"\n",
    "    Final step of the marking process; the new contents of the note are written.\n",
    "\n",
    "    Helper function to `auto_mark_def_and_notats`\n",
    "    \"\"\"\n",
    "    mf.remove_lines(first_non_metadata_line, see_also_line)\n",
    "    mf.insert_line(first_non_metadata_line,\n",
    "                   {'type': MarkdownLineEnum.DEFAULT, 'line': main_text})\n",
    "    mf.add_tags('_auto/def_and_notat_identified')\n",
    "    mf.write(note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def auto_mark_def_and_notats(\n",
    "        note: VaultNote,  # The standard information note in which to find the definitions and notations.\n",
    "        pipeline: pipelines.token_classification.TokenClassificationPipeline, # The token classification pipeline that is used to predict whether tokens are part of definitions or notations introduced in the text.\n",
    "        # remove_existing_def_and_notat_markings: bool = False,  # If `True`, remove definition and notation markings (both via surrounding by double asterisks `**` as per the legacy method and via HTML tags)\n",
    "        excessive_space_threshold: int = 2,\n",
    "        add_boxing_attr_to_existing_def_and_notat_markings: bool = True # If `True`, then nice attributes are added to the existing notation HTML tags, if not already present.\n",
    "    ) -> None:\n",
    "    \"\"\"\n",
    "    Predict and mark where definitions and notation occur in a note using\n",
    "    a token classification ML model.\n",
    "\n",
    "    Assumes that the note is a standard information note that does not\n",
    "    have a lot of \"user modifications\", such as footnotes, links,\n",
    "    and HTML tags. If\n",
    "    there are many modifications, then these might be deleted.\n",
    "\n",
    "    Assumes that the paragraphs in the text of the note are \"not too long\".\n",
    "    Currently, this means that the paragraphs in the number of tokens\n",
    "    in the text of the note should (roughly) not exceed \n",
    "    `pipeline.tokenizer.model_max_length`.\n",
    "\n",
    "    Existing markings for definition and notation data (i.e. by\n",
    "    surrounding with double asterisks or by HTML tags) are preserved\n",
    "    (and turned into HTML tags), unless the markings overlap with \n",
    "    predictions, in which case the original is preserved (and still\n",
    "    turned into an HTML tag if possible)\n",
    "\n",
    "    Since the model can make \"invalid\" predictions (mostly those which\n",
    "    start or end within a LaTeX math mode str), the actual markings\n",
    "    are not necessarily direct translates from the model's predictions.\n",
    "    See the helper function `_consolidate_token_preds` for more details\n",
    "    on how this is implemented.\n",
    "    \n",
    "    **Raises**\n",
    "    Warning messages (`UserWarning`) are printed in the following situations:\n",
    "\n",
    "    - There are two consecutive tokens within the `pipeline`'s predictions\n",
    "      of different entity types (e.g. one is predicted to belong within a\n",
    "      definition and the other within a notation), but the latter token's\n",
    "      predicted `'entity'` more specifically begins with `'I-'` (i.e. is\n",
    "      `'I-definition'` or `'I-notation'`) as opposed to `'B-'`.\n",
    "        - `note`'s name, and path are included in the warning message in\n",
    "          this case.\n",
    "    - There are two consecutive tokens within the `pipeline`'s predictions\n",
    "      which the pipeline predicts to belong to the same entity, and yet\n",
    "      there is excessive space (specified by `excessive_space_threshold`)\n",
    "      between the end of the first token and the start of the second.\n",
    "\n",
    "    \"\"\"\n",
    "    mf = MarkdownFile.from_vault_note(note)\n",
    "    mf.cleanup_formatting()\n",
    "    # _process_mf(mf)\n",
    "    first_non_metadata_line, see_also_line = _get_main_text_lines(mf)\n",
    "     \n",
    "    main_text = mf.text_of_lines(first_non_metadata_line, see_also_line)\n",
    "    main_text = _format_main_text_and_add_html_tag_data(\n",
    "        note, pipeline, add_boxing_attr_to_existing_def_and_notat_markings,\n",
    "        excessive_space_threshold, main_text)\n",
    "    _write_text_with_html_tag_preds_to_note(\n",
    "        note, mf, main_text, first_non_metadata_line, see_also_line)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following examples, we mock pipeline objects instead of using actual ones.\n",
    "\n",
    "In the below example, we run the `auto_mark_def_and_notats` function on a note that has double asterisks `**` surrounding parts of the text that introduced definitions or notations. In these cases, appropriate HTML tags replace the double asterisks instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text before:\n",
      "\n",
      "\n",
      "---\n",
      "cssclass: clean-embeds\n",
      "aliases: []\n",
      "tags: [_meta/literature_note, _meta/definition, _meta/notation]\n",
      "---\n",
      "# Ring of integers modulo $n$[^1]\n",
      "\n",
      "Let $n \\geq 1$ be an integer. The **ring of integers modulo $n$**, denoted by **$\\mathbb{Z}/n\\mathbb{Z}$**, is, informally, the ring whose elements are represented by the integers with the understanding that $0$ and $n$ are equal.\n",
      "\n",
      "More precisely, $\\mathbb{Z}/n\\mathbb{Z}$ has the elements $0,1,\\ldots,n-1$.\n",
      "\n",
      "...\n",
      "\n",
      "\n",
      "# See Also\n",
      "- [[reference_with_tag_labels_Exercise 1|reference_with_tag_labels_Z_nZ_is_a_ring]]\n",
      "# Meta\n",
      "## References\n",
      "\n",
      "## Citations and Footnotes\n",
      "[^1]: Kim, Definition 2\n",
      "\n",
      "\n",
      "\n",
      "Text after:\n",
      "\n",
      "---\n",
      "cssclass: clean-embeds\n",
      "aliases: []\n",
      "tags: [_meta/definition, _meta/literature_note, _auto/def_and_notat_identified, _meta/notation]\n",
      "---\n",
      "# Ring of integers modulo $n$[^1]\n",
      "\n",
      "Let $n \\geq 1$ be an integer. The <b definition=\"\" style=\"border-width:1px;border-style:solid;padding:3px\">ring of integers modulo $n$</b>, denoted by <span notation=\"\" style=\"border-width:1px;border-style:solid;padding:3px\">$\\mathbb{Z}/n\\mathbb{Z}$</span>, is, informally, the ring whose elements are represented by the integers with the understanding that $0$ and $n$ are equal.\n",
      "\n",
      "More precisely, $\\mathbb{Z}/n\\mathbb{Z}$ has the elements $0,1,\\ldots,n-1$.\n",
      "\n",
      "...\n",
      "\n",
      "# See Also\n",
      "- [[reference_with_tag_labels_Exercise 1|reference_with_tag_labels_Z_nZ_is_a_ring]]\n",
      "# Meta\n",
      "## References\n",
      "\n",
      "## Citations and Footnotes\n",
      "[^1]: Kim, Definition 2\n"
     ]
    }
   ],
   "source": [
    "with (tempfile.TemporaryDirectory(prefix='temp_dir', dir=os.getcwd()) as temp_dir,\n",
    "      mock.patch('__main__.pipelines.token_classification.TokenClassificationPipeline') as mock_pipeline):\n",
    "    temp_vault = Path(temp_dir) / 'test_vault_6'\n",
    "    shutil.copytree(_test_directory() / 'test_vault_6', temp_vault)\n",
    "\n",
    "    mock_pipeline.tokenizer.model_max_length = 512\n",
    "\n",
    "    vn = VaultNote(temp_vault, name='reference_with_tag_labels_Definition 2')\n",
    "    print(\"Text before:\\n\\n\")\n",
    "    print(vn.text())\n",
    "    print(\"\\n\\n\\nText after:\\n\")\n",
    "    auto_mark_def_and_notats(vn, mock_pipeline)\n",
    "    print(vn.text())\n",
    "    mf = MarkdownFile.from_vault_note(vn)\n",
    "    assert mf.has_tag('_auto/def_and_notat_identified')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: more examples with pipeline mocking actual outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Miscellaneous marking formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is helper code for making predictions on the author's writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def latex_highlight_formatter(text: str, pred: HTMLTagWithIndices) -> str:\n",
    "    r\"\"\"\n",
    "    Formats definitions and notations with LaTeX highlighting commands.\n",
    "    \n",
    "    Rules:\n",
    "    - Definitions: \\hldef{text}\n",
    "    - Notations:\n",
    "        - $...$ -> \\hl{$...$}\n",
    "        - $$...$$ -> $$\\hlin{...}$$\n",
    "        - \\begin{align*}...\\end{align*} -> \\hlalign{\\begin{align*}...\\end{align*}}\n",
    "    \"\"\"\n",
    "    is_definition = 'definition' in pred.tag.attrs\n",
    "    \n",
    "    # --- Case 1: Definition ---\n",
    "    if is_definition:\n",
    "        return f\"\\\\hldef{{{text}}}\"\n",
    "    \n",
    "    # --- Case 2: Notation ---\n",
    "    # We need to detect the type of math environment in `text`\n",
    "    \n",
    "    stripped_text = text.strip()\n",
    "    \n",
    "    # Subcase 2a: Display Math with $$...$$\n",
    "    if stripped_text.startswith('$$') and stripped_text.endswith('$$'):\n",
    "        # Extract content inside $$...$$\n",
    "        # Content length is len(text) - 4 (for the two $$ pairs)\n",
    "        # We need to handle potential whitespace, so we use indices based on the strip\n",
    "        inner_content = stripped_text[2:-2] \n",
    "        return f\"$$\\\\hlin{{{inner_content}}}$$\"\n",
    "    \n",
    "    # Subcase 2b: Align Environment\n",
    "    # Checks for \\begin{align*} or \\begin{align}\n",
    "    if r'\\begin{align' in text:\n",
    "        return f\"\\\\hlalign{{{text}}}\"\n",
    "    \n",
    "    # Subcase 2c: Inline Math $...$ (Default for notations)\n",
    "    # Note: We wrap the *entire* text (including $) in \\hl{...}\n",
    "    # User requirement: \"if $blah$, then \\hl{$blah$}\"\n",
    "    return f\"\\\\hl{{{text}}}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Compiling the model with `torch.compile` and using a `torch.cpu` device is not supported. Falling back to non-compiled mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let $L/K$ be a separable, normal algebraic field extension. The \\hldef{Galois group} \\hl{$\\operatorname{Gal}(L/K)$} is defined as the automorphism group.\n",
      "\n",
      "Consider the sequence: $$ 0 \\to A \\to B \\to C \\to 0 $$\n"
     ]
    }
   ],
   "source": [
    "#| notest\n",
    "from transformers import pipeline, AutoModelForTokenClassification, AutoTokenizer\n",
    "# from trouver.machine_learning.tokenize.def_and_notat_token_classification import (\n",
    "#     predict_and_mark_def_and_notats,\n",
    "#     latex_highlight_formatter\n",
    "# )\n",
    "from types import SimpleNamespace\n",
    "\n",
    "# 1. Load the Model\n",
    "# Replace 'hyunjongkim/math-def-and-notat-token-classification' with your actual model path/ID\n",
    "# We use 'token-classification' task. 'aggregation_strategy=\"simple\"' helps merge B- and I- tags automatically.\n",
    "model = AutoModelForTokenClassification.from_pretrained('hyunjongkimmath/def_and_notat_token_classification_model_modernbert_base')\n",
    "tokenizer = AutoTokenizer.from_pretrained('hyunjongkimmath/def_and_notat_token_classification_model_modernbert_base')\n",
    "def_notat_classifier = pipeline('ner', model=model, tokenizer=tokenizer)\n",
    "\n",
    "# 2. Setup the Inputs\n",
    "# Text with definition (\"Galois group\"), inline notation ($...$), and display notation ($$...$$)\n",
    "text = (\n",
    "    r\"Let $L/K$ be a separable, normal algebraic field extension. The Galois group $\\operatorname{Gal}(L/K)$ is defined as the automorphism group.\"\n",
    "    \"\\n\\n\"\n",
    "    r\"Consider the sequence: $$ 0 \\to A \\to B \\to C \\to 0 $$\"\n",
    ")\n",
    "\n",
    "# A dummy note object (required by the function for logging)\n",
    "# note = SimpleNamespace(name=\"Example Note\", path=\"Example.md\")\n",
    "\n",
    "# 3. Run Prediction and Formatting\n",
    "formatted_text = predict_and_mark_def_and_notats(\n",
    "    main_text=text,\n",
    "    pipeline=def_notat_classifier,\n",
    "    # note=note,\n",
    "    formatter=latex_highlight_formatter,\n",
    "    excessive_space_threshold=2\n",
    ")\n",
    "\n",
    "# 4. View Result\n",
    "print(formatted_text)\n",
    "\n",
    "# Expected Output (conceptual):\n",
    "# The \\hldef{Galois group} \\hl{$\\operatorname{Gal}(L/K)$} is defined by the kernel. \n",
    "# Consider the sequence: $$\\hlin{ 0 \\to A \\to B \\to C \\to 0 }$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All latex_highlight_formatter tests passed!\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# --- Setup ---\n",
    "soup = BeautifulSoup(\"\", 'html.parser')\n",
    "\n",
    "def make_pred(tag_type, text):\n",
    "    # Helper to create a dummy prediction\n",
    "    attrs = {tag_type: \"\"}\n",
    "    tag = soup.new_tag(\"span\", **attrs)\n",
    "    # Indices don't matter for the formatter, only the text passed to it\n",
    "    return HTMLTagWithIndices(tag, 0, 0)\n",
    "\n",
    "# --- Test 1: Definition ---\n",
    "text_def = \"Galois group\"\n",
    "pred_def = make_pred(\"definition\", text_def)\n",
    "assert latex_highlight_formatter(text_def, pred_def) == r\"\\hldef{Galois group}\"\n",
    "\n",
    "# --- Test 2: Inline Notation ---\n",
    "text_inline = \"$x$\"\n",
    "pred_not = make_pred(\"notation\", text_inline)\n",
    "assert latex_highlight_formatter(text_inline, pred_not) == r\"\\hl{$x$}\"\n",
    "\n",
    "# --- Test 3: Display Notation ($$) ---\n",
    "text_display = \"$$x^2$$\"\n",
    "pred_not = make_pred(\"notation\", text_display)\n",
    "# Expect: $$\\hlin{x^2}$$\n",
    "assert latex_highlight_formatter(text_display, pred_not) == r\"$$\\hlin{x^2}$$\"\n",
    "\n",
    "# Test 3b: Display with whitespace\n",
    "text_display_ws = \"$$  y^2  $$\"\n",
    "# Expect: $$\\hlin{  y^2  }$$  (Inner content preserved)\n",
    "assert latex_highlight_formatter(text_display_ws, pred_not) == r\"$$\\hlin{  y^2  }$$\"\n",
    "\n",
    "# --- Test 4: Align Notation ---\n",
    "text_align = r\"\"\"\\begin{align*}\n",
    "x &= y \\\\\n",
    "y &= z\n",
    "\\end{align*}\"\"\"\n",
    "pred_not = make_pred(\"notation\", text_align)\n",
    "\n",
    "expected_align = r\"\\hlalign{\" + text_align + \"}\"\n",
    "assert latex_highlight_formatter(text_align, pred_not) == expected_align\n",
    "\n",
    "print(\"All latex_highlight_formatter tests passed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using an agent to verify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NER model approach above is quite fast and often accurate, even on a personal use computer and CPU. Nevertheless, it does make mistakes, so manual correction is needed. The following is an agentic process that attempts to find if a the NER model has made mistakes when marking definitions and notations on a note and thus the note needs manual correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "DEF_NOTAT_VERIFY_SYSTEM_PROMPT = r\"\"\"\n",
    "You are an expert auditor of mathematical texts. Your task is to validate semantic HTML markings (attributes: \"definition\" or \"notation\") within an excerpt. You must determine if the current markings correctly identify **newly introduced** terms while ignoring **contextual** objects.\n",
    "\n",
    "### I. The Core Binary Classification\n",
    "For every mathematical object or term, apply this binary test.\n",
    "\n",
    "**1. THE TARGET (Must be Marked)**\n",
    "A term is a Target if and only if the excerpt **establishes its meaning for the first time** intended for use beyond the immediate sentence.\n",
    "*   **New Constructions:** \"For any field $K$, let $G_K$ denote...\" (Mark $G_K$).\n",
    "*   **Formal Definitions:** \"We say a sheaf is <b definition>flasque</b> if...\" (Mark \"flasque\").\n",
    "*   **Explicit Assignments:** \"We define the L-series $L(s, \\chi)$ as...\" (Mark \"L-series\" and $L(s, \\chi)$).\n",
    "*   **Self-Contained \"Recalls\":** If the text says \"Recall that X is called Y if [Definition]\", and the excerpt contains the actual definition criteria, Mark Y. The author is establishing the definition for this text.\n",
    "    *   *Correct:* \"Recall that $X$ is <b definition>totally disconnected</b> if connected components are points.\"\n",
    "    *   *Incorrect (Don't mark):* \"Recall the properties of totally disconnected spaces.\"\n",
    "\n",
    "**2. THE CONTEXT (Must NOT be Marked)**\n",
    "Everything else is Context. This includes:\n",
    "*   **Generic Variables:** \"Let $X$ be a scheme...\" ($X$ is a generic placeholder).\n",
    "*   **Specific Instances/Applications:** \"Let $G = \\text{Gal}(L/K)$...\" ($G$ is just a shorthand for this specific proof).\n",
    "*   **Reminders/Recalls:** \"Recall that $H^i$ denotes cohomology...\" (The definition exists outside this excerpt).\n",
    "*   **Process Variables:** \"By induction on $n$...\", \"It suffices to treat...\", \"Let $f: X \\to Y$ be a morphism...\"\n",
    "\n",
    "### II. Auditing Rules\n",
    "\n",
    "1.  **The \"Defined Here\" Rule:** Only mark objects if the text explicitly links the symbol/term to its formal name or construction *in this specific excerpt*. If the text implies the reader should already know it (e.g., \"Consider the trace defined above\"), do not mark it.\n",
    "2.  **Construction vs. Instance:**\n",
    "    *   \"Let $f$ be the map defined in Eq 1\" $\\to$ **Context** (Reference to past).\n",
    "    *   \"Let $f$ denote the map...\" $\\to$ **Target** (Establishing new notation).\n",
    "3.  **OCR Robustness:** Treat malformed LaTeX (e.g., missing `$`) as valid text. If `L(s)` is a Target but lacks delimiters, it still requires a mark.\n",
    "\n",
    "### III. Reference Examples\n",
    "\n",
    "*   **Correct Definition:** \"We call a functor $F$ <b definition>representable</b> if...\"\n",
    "    *   *Reason:* Defines the property \"representable\". $F$ is Context.\n",
    "*   **Correct Notation:** \"Let $\\mathbb{A}^n$ denote affine space.\"\n",
    "    *   *Reason:* Explicit assignment of global notation.\n",
    "*   **False Positive (Do not mark):** \"Let $C$ be a category. If $C$ has limits...\"\n",
    "    *   *Reason:* $C$ is a generic variable used to set the stage.\n",
    "*   **False Positive (Do not mark):** \"The fiber product $X \\times_Y Z$ (see Chapter 1)...\"\n",
    "    *   *Reason:* Reference to a prior definition.\n",
    "\n",
    "### IV. Output format\n",
    "Analyze the excerpt based on these rules. Determine `has_incorrect_markings` (Context marked as Target) and `has_missing_markings` (Target not marked). Return the result as a JSON object matching the requested schema. Provide detailed reasoning distinguishing \"Target\" from \"Context\".\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# DEF_NOTAT_VERIFY_SYSTEM_PROMPT = r\"\"\"\n",
    "# You are an expert auditor of mathematical texts, trained to identify errors in semantic markings of definitions and notations introduced in small parts (\"excerpts\") of mathematical text. Your analysis must be guided by the following principle and rules. The markings are by HTML tags that contain the attribute \"definition\" or \"notation\".\n",
    "\n",
    "# I. The Core Principle: Definitional Intent\n",
    "\n",
    "# The primary goal of the markings is to determine the EXACT location that a definition or notation in an excerpt is defined at. In principle, a reader with knowledge of all definitions and notations across all excerpts should be able to understand any part of the overarching text. YOUR primary goal is to determine whether or not the markings in the provided excerpt succeed at this goal.\n",
    "\n",
    "# You must be able to distinguish between:\n",
    "\n",
    "#     Definitions and notations that are INTRODUCED in the excerpt. This MUST be marked.\n",
    "\n",
    "#     \"Supporting Cast\": Local Variables that should not have any bearing outside the immediate excerpt, ALONG WITH pre-existing or standard definitions and notations that are neither introduced nor defined in the excerept.\n",
    "\n",
    "# A marking is an error if and only if it violates this principle.\n",
    "# II. The Rules of Auditing\n",
    "\n",
    "#     Before all else, read the excerpt and identify only Global Main Subjects: objects or notations intended to persist across multiple sections as standard terminology for the entire work. If a variable is introduced to state a specific Theorem or Lemma (e.g., 'Let G be a group' in the context of Theorem 1), it is Supporting Cast for that specific logic and must not be marked.\n",
    "\n",
    "#         \"In this section, K is a field...\" -> K is explicitly described as a variable needed to understand other parts of the section.\n",
    "\n",
    "#     Classify All Other Objects as Supporting Cast: Any variable or object used in a context of application is Supporting Cast.\n",
    "\n",
    "#         Clues for Supporting Cast (No Mark): 'It suffices to treat...', 'By induction on...', 'We have...', 'Recall that...', 'Note that...'. These phrases indicate the text is using existing tools to perform a proof.\n",
    "\n",
    "#         Clues for Main Subject (Mark): 'We define...', 'We shall denote...', 'Let us call...'. These indicate the initial assignment of a name/symbol to a concept.\"\n",
    "    \n",
    "#         \"Let C be a category...\" -> C is Supporting Cast. The text is using **A** (as opposed to some specific) category, not defining what a category is.\n",
    "\n",
    "#         \"Let g: G -> X be a morphism...\" -> g, G, and X are Supporting Cast (again, the morphism is **A MORPHISM** (as opposed to some specific morphism)). They are props for the discussion and/or the definitions/notations being defined.\n",
    "\n",
    "#     Distinguish Construction from Application: An object is a Main Subject only if the notation itself or the specific name is being assigned for the first time.\n",
    "\n",
    "#         Example: 'We define the L-series L(s,)=...' is a Main Subject.\n",
    "\n",
    "#         Counter-Example: 'Consider the L-series L(s,) defined in Chapter 1' is Supporting Cast.\n",
    "\n",
    "#         Counter-Example: Gal(L/K) in a theorem is an application of a standard construction, and not the introduction of one unless an introduction or reminder is explicit.\n",
    "\n",
    "#     Construction vs. Instance: An object is a Main Subject only if the text is defining a general construction or global notation.\n",
    "\n",
    "#         Example: 'For any field K, let G_K denote its Galois group.'  This is an introduction of notation G_K. (Mark it).\n",
    "\n",
    "#         Counter-Example: 'Let G=Gal(L/K) be the Galois group of the extension.'  This is merely naming a specific instance for the sake of a proof. G is Supporting Cast. (Do not mark).\"\n",
    "\n",
    "#     Distinguishing Definition from Reminder: \n",
    "#         Since you only see an excerpt, use the \"First Invocation\" Heuristic: \n",
    "#         Assume a notation/term is being introduced if it meets two criteria:\n",
    "#         1. It is a Specialized Notation (multicharacter symbols like tr.deg, cd_p, or functional operators) rather than a simple variable (x, y, K).\n",
    "#         2. The text explicitly links the symbol to its formal name (e.g., \"where tr.deg denotes the transcendental degree\"). \n",
    "        \n",
    "#         Even if this appears in a \"where\" clause or a proof, treat it as a Main Subject \"in effect\" for this excerpt.\n",
    "\n",
    "#     Handle Malformed Text (OCR Errors): Treat mathematical notation that is missing its $ delimiters (e.g., L\\left(X\\right)) as if it were properly wrapped. If it represents a Main Subject, it requires a mark.\n",
    "\n",
    "#     However, you are NOT trying to judge whether the given excerpt is well formatted or even coherent. You are trying to only decide whether or not the excerpt introduces definitions and notations and whether it marks those newly introduced definitions and notations\n",
    "\n",
    "# III. Illustrative Guide & Edge Cases\n",
    "\n",
    "# Reference these examples to resolve ambiguity.\n",
    "\n",
    "#     Example A: Clear Definition\n",
    "\n",
    "#         Text: \"We say that a sheaf F is <b style=\"border-width:1px;border-style:solid;padding:3px\" definition=\"\">flasque</b> if for every open set U, the map... is surjective.\"\n",
    "\n",
    "#         Analysis: The Main Subject is the property \"flasque.\" The mark is CORRECT. F and U are Supporting Cast.\n",
    "\n",
    "#     Example B: The Ambiguous \"Let...\" Clause\n",
    "\n",
    "#         Scenario 1 (Supporting Cast):\n",
    "\n",
    "#             Text: \"Let $C$ be a category... We say the inductive limit of $G$ is <b style=\"border-width:1px;border-style:solid;padding:3px\" definition=\"\">universal</b> if...\"\n",
    "\n",
    "#             Analysis: The Main Subject is the property \"universal.\" The object $C$ is merely Supporting Cast, used to provide a context for the definition. Tagging $C$ would be an INCORRECT MARKING. On the other hand Tagging \"universal\" is a CORRECT MARKING.\n",
    "\n",
    "#         Scenario 2 (Supporting Cast): In a Proof or Theorem, 'Let X be...' is almost always Supporting Cast. Rule of Thumb: If an object is defined as a specific instance to prove a point (e.g., 'There exists a subextension L...'), it is Supporting Cast. A Main Subject is typically introduced in a formal 'Definition' block or via the phrase 'We shall call such a [concept] a...'\n",
    "\n",
    "#     Example C: New Construction\n",
    "\n",
    "#         Text: \"Let $g: G \\to X$ and $m: Y \\to X$ be morphisms. We define the fiber product, denoted <span style=\"border-width:1px;border-style:solid;padding:3px\" notation=\"\">$G \\times_X Y$</span>, as the pullback...\"\n",
    "\n",
    "#         Analysis: g, G, X, m, Y are all Supporting Cast. The Main Subjects are the term \"fiber product\" and its notation $G \\times_X Y$. Both require marks. Tagging g or m would be INCORRECT.\n",
    "\n",
    "# IV. Final Instructions\n",
    "\n",
    "#     Analyze: Read the user's text and apply the Core Principle and Rules to identify the definitions and notations that are being introduced and to identify the Supporting Cast.\n",
    "\n",
    "#     Reason: In your reasoning, explicitly use the terms \"Definitions and Notations Introduced\" and \"Supporting Cast\" to justify your findings. State why an object fits into one category or the other.\n",
    "\n",
    "#     Conclude: Based on your analysis, determine the values for has_incorrect_markings and has_missing_markings. An incorrect marking is a tag on a Supporting Cast member. A missing marking is a lack of a tag on a Main Subject.\n",
    "\n",
    "#     Format: Provide your result as a valid JSON object matching the requested schema. Your reasoning must be detailed with at least 4 sentences and follow the steps above.\n",
    "\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# DEF_NOTAT_VERIFY_SYSTEM_PROMPT = r\"\"\"\n",
    "# You are a rigorous Mathematical Text Auditor. Your goal is to identify semantic marking errors in LaTeX/HTML definitions.\n",
    "\n",
    "# ### 1. RECOGNITION RULES (Crucial)\n",
    "# - DEFINED TERMS: If the text says \"An object X is [Term] if...\" or \"We call Y a [Term]...\", the [Term] IS the primary subject and MUST be tagged.\n",
    "# - INSTANTIATED OBJECTS: If a specific object is introduced with \"Let X be...\", \"Define X as...\", or \"Consider the object X...\", then X ITSELF is a notation that MUST be tagged.\n",
    "#     - Example: \"Let $\\mathbb{A}^m_0$ be the affine space...\" -> $\\mathbb{A}^m_0$ IS a notation/definition here.\n",
    "#     - Contrast: \"Let $n$ be an integer...\" -> $n$ is a generic variable (do not tag).\n",
    "# - NOTATION: Symbols like $X_{proet}$, $\\Delta_f$, or specific spaces like $\\mathbb{A}_0^m$ are formal notations. If they are tagged with <b> or <span>, they are CORRECT.\n",
    "# - UNWRAPPED MATH: If a string looks like math (e.g., has `\\left(`, `\\mathscr`, `_0`, `^2`), TREAT IT AS MATH NOTATION, even if it lacks dollar sign $ wrappers.\n",
    "#     - Example: `L\\left(X, t\\right)` -> Treat as `$L(X, t)$` -> **MUST BE TAGGED** if it's the term being defined.\n",
    "# - **EXPLICIT DEFINITIONS**: Any symbol followed by \"defined... by\", \":=\", or \"given by\" is a DEFINED TERM.\n",
    "#     - Text: \"The L-series L(X,t)... defined... by...\" -> Both \"L-series\" AND \"L(X,t)\" MUST be tagged.\n",
    "\n",
    "# ### 2. EVALUATION CRITERIA\n",
    "# - False Positives (has_incorrect_markings): Tags on common english words (e.g., \"sine\"), repeated tags for terms defined previously, or tags on *truly* generic variables like $i, j, n$ used only as indices.\n",
    "# - False Negatives (has_missing_markings):\n",
    "#     - Missing tags on the primary term being introduced (e.g., \"The **Swan Conductor**\").\n",
    "#     - Missing tags on the specific mathematical object being instantiated (e.g., \"Let **$X$** be...\").\n",
    "#     - Missing tags on new notation symbols (e.g., $:=$, $\\mapsto$).\n",
    "# - **False Negatives (has_missing_markings):**\n",
    "#     - Missing tags on definitions (even if English term is tagged, the associated notation MUST also be tagged).\n",
    "#     - **CRITICAL:** Missing tags on \"naked\" LaTeX code that represents the defined term (e.g., `L\\left(...\\right)`).\n",
    "\n",
    "# ### 3. MANDATORY AUDIT STEPS\n",
    "# 1. List what is actually inside <b> and <span> tags.\n",
    "# 2. Check if untagged math symbols are *specific objects* (mark missing) or *generic indices* (ignore).\n",
    "# 3. Only vote TRUE if there is a clear violation.\n",
    "\n",
    "# ### INSTRUCTIONS:\n",
    "# 1. Perform a step-by-step audit of the text.\n",
    "# 2. Explicitly distinguish between \"Generic Variables\" (n, i, k) and \"Specific Instantiated Objects\" (X, G, $\\mathbb{A}^n$).\n",
    "# 3. Output valid JSON.\n",
    "\n",
    "# You are running in 'Structured Output' mode. \n",
    "# You must output a VALID JSON object matching the requested schema.\n",
    "\n",
    "# REASONING MUST BE DETAILED: 4+ sentences minimum.\n",
    "\n",
    "# \"\"\"\n",
    "\n",
    "DEF_NOTAT_VERIFY_USER_PROMPT = r\"\"\"\n",
    "Audit the following excerpt of mathematical text for definition and notation marking errors. Return the result in the specified JSON format.\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AuditResult(BaseModel):\n",
    "    reasoning: str\n",
    "    has_incorrect_markings: bool\n",
    "    has_missing_markings: bool\n",
    "\n",
    "class AuditVoteResult(TypedDict, total=True):\n",
    "    should_remove: bool\n",
    "    should_add: bool\n",
    "    incorrect_tally: int  # Added tally for incorrect markings\n",
    "    missing_tally: int    # Added tally for missing markings\n",
    "    error_tally: int    # New: specifically tracks failed LLM calls\n",
    "    total_votes: int      # Useful for context/percentage\n",
    "    stats: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "INITIAL_ERROR_MESSAGE = \"ERROR during LLM call\"\n",
    "def run_strict_audit(\n",
    "        client: OpenAI,\n",
    "        text_to_check: str,\n",
    "        system_prompt: str = DEF_NOTAT_VERIFY_SYSTEM_PROMPT,\n",
    "        user_prompt: str = DEF_NOTAT_VERIFY_USER_PROMPT,\n",
    "        temperature: float = 0.7,\n",
    "        verbose: bool = False,\n",
    "        ) -> AuditResult:\n",
    "    \"\"\"\n",
    "    Make a client (running on an LLM) audit the definition and notation markings\n",
    "    in `text_to_check` to see if any definitions/notations should have been marked\n",
    "    but were not (false negatives) or were marked but should not have been\n",
    "    (false positives).\n",
    "\n",
    "    Sends text to the model and forces a JSON response matching AuditResult.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"local-model-name\",  # Name often ignored by local servers, but required\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": f\"{user_prompt}:\\n\\n{text_to_check}\"},\n",
    "            ],\n",
    "            temperature=temperature,  # Low temperature = more deterministic structure\n",
    "            max_tokens=512,\n",
    "            # --- THE FIX: NEW STRUCTURED OUTPUT FORMAT ---\n",
    "            # response_format={\n",
    "            #     \"type\": \"json_schema\",\n",
    "            #     \"json_schema\": {\n",
    "            #         \"name\": \"audit_result\", # Identifying name for the schema\n",
    "            #         \"schema\": AuditResult.model_json_schema(), # Auto-generates the schema\n",
    "            #         \"strict\": True # Forces the model to adhere exactly\n",
    "            #     }\n",
    "            # }\n",
    "            response_format={\n",
    "                \"type\": \"json_schema\",\n",
    "                \"json_schema\": {\n",
    "                    \"name\": \"audit_result\",\n",
    "                    \"strict\": True, # Setting this False may allow the model to be longer with its reasoning at the cost of potentially breaking json \n",
    "                    # \"strict\": False,\n",
    "                    \"schema\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"reasoning\": {\"type\": \"string\"},\n",
    "                            \"has_incorrect_markings\": {\"type\": \"boolean\"},\n",
    "                            \"has_missing_markings\": {\"type\": \"boolean\"}\n",
    "                        },\n",
    "                        \"required\": [\"reasoning\", \"has_incorrect_markings\", \"has_missing_markings\"],\n",
    "                        \"additionalProperties\": False\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # 1. Get the raw string (The server GUARANTEES this is valid JSON matching schema)\n",
    "        raw_content = response.choices[0].message.content\n",
    "        \n",
    "        # 2. Parse directly into Pydantic\n",
    "        result = AuditResult.model_validate_json(raw_content)\n",
    "        \n",
    "        return result\n",
    "\n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"Audit Failed: {e}\")\n",
    "        return AuditResult(\n",
    "            reasoning=f\"{INITIAL_ERROR_MESSAGE}: {str(e)}\",\n",
    "            has_incorrect_markings=False,\n",
    "            has_missing_markings=False\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "ALL_AUDITS_FAILED_STRING = \"All audit attempts failed due to system errors.\"\n",
    "def run_audit_voting(\n",
    "        client: OpenAI,\n",
    "        text_to_check: str,\n",
    "        iterations: int = 3,\n",
    "        guarantee_success_count: bool = False, # New optional argument\n",
    "        max_attempts: int = 10,                 # Safety break for the guaranteed mode\n",
    "        verbose: bool = True,\n",
    "        system_prompt: str = DEF_NOTAT_VERIFY_SYSTEM_PROMPT,\n",
    "        user_prompt: str = DEF_NOTAT_VERIFY_USER_PROMPT,\n",
    "        temperature: float = 0.7,\n",
    "        ) -> AuditVoteResult:\n",
    "    \"\"\"\n",
    "    Run `run_strict_audit` multiple times to let the LLM within `client`     \n",
    "    vote against itself concerning whether a text has \n",
    "    false negatives or false positives for the marked definitions and notations\n",
    "    \"\"\"\n",
    "    audits: list[AuditResult] = []\n",
    "    total_attempts = 0\n",
    "    successful_count = 0\n",
    "\n",
    "    while True:\n",
    "        audit = run_strict_audit(\n",
    "            client, text_to_check, system_prompt, user_prompt, temperature, verbose)\n",
    "        total_attempts += 1\n",
    "\n",
    "        # Check if this specific audit was a success (didn't return the error message)\n",
    "        is_error = audit.reasoning.startswith(INITIAL_ERROR_MESSAGE)\n",
    "        if not is_error:\n",
    "            successful_count += 1\n",
    "\n",
    "        audits.append(audit)\n",
    "\n",
    "        if verbose:\n",
    "            status = \"Error\" if is_error else \"Success\"\n",
    "            print(f\"Attempt {total_attempts} ({status})\")\n",
    "            print(audit, '\\n\\n')\n",
    "        # if verbose:\n",
    "        #     print(f'Vote {i}')\n",
    "        #     print(audit, '\\n\\n')\n",
    "        # Termination conditions\n",
    "        if guarantee_success_count:\n",
    "            # Mode 2: Stop when we have enough successes or hit the safety wall\n",
    "            if successful_count >= iterations or total_attempts >= max_attempts:\n",
    "                break\n",
    "        else:\n",
    "            # Mode 1: Stop when we hit the fixed iteration count\n",
    "            if total_attempts >= iterations:\n",
    "\n",
    "                break\n",
    "    # Filtering and Tallying\n",
    "    failures = [a for a in audits if a.reasoning.startswith(INITIAL_ERROR_MESSAGE)]\n",
    "    successes = [a for a in audits if not a.reasoning.startswith(INITIAL_ERROR_MESSAGE)]\n",
    "    error_count = len(failures)\n",
    "    total = len(audits)\n",
    "\n",
    "    # We base the voting threshold only on successful audits\n",
    "    # If using Mode 2, this will be exactly `iterations`.\n",
    "    # If using Mode 1, this will be `iterations - error_count`.\n",
    "    valid_vote_count = len(successes)\n",
    "    \n",
    "    if valid_vote_count == 0:\n",
    "        return AuditVoteResult(\n",
    "            should_remove=False, \n",
    "            should_add=False, \n",
    "            incorrect_tally=0,\n",
    "            missing_tally=0,\n",
    "            error_tally=error_count,\n",
    "            total_votes=total,\n",
    "            stats=ALL_AUDITS_FAILED_STRING\n",
    "        )\n",
    "\n",
    "    threshold = (valid_vote_count // 2) + 1\n",
    "    inc_votes = sum(1 for audit in successes if audit.has_incorrect_markings)\n",
    "    mis_votes = sum(1 for audit in successes if audit.has_missing_markings)\n",
    "\n",
    "    reasonings = [f\"[{i+1}] {audit.reasoning}\" for i, audit in enumerate(audits)]\n",
    "\n",
    "    return AuditVoteResult(\n",
    "        should_remove=inc_votes >= threshold,\n",
    "        should_add=mis_votes >= threshold,\n",
    "        incorrect_tally=inc_votes,\n",
    "        missing_tally=mis_votes,\n",
    "        error_tally=error_count,\n",
    "        total_votes=total,\n",
    "        stats=(\n",
    "            f\"Summary: {inc_votes} inc, {mis_votes} mis, {error_count} failed out of {total} total attempts.\\n\"\n",
    "            f\"Threshold for action (based on {valid_vote_count} successes): {threshold}\\n\\n\"\n",
    "            f\"Reasonings:\\n\" + \"\\n\\n\".join(reasonings)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # error_count = len(failures)\n",
    "    # total = len(audits)\n",
    "    # total = len(audits)\n",
    "    # # failures = [\n",
    "    #     # a for a in audits if a.reasoning.startswith(INITIAL_ERROR_MESSAGE)]\n",
    "    # error_count = len(failures)\n",
    "    # threshold = (len(audits) // 2) + 1\n",
    "    # inc_votes = sum(1 for audit in audits if audit.has_incorrect_markings)\n",
    "    # mis_votes = sum(1 for audit in audits if audit.has_missing_markings)\n",
    "\n",
    "    # # Check if all attempts resulted in an error object\n",
    "    # if all(audit.reasoning.startswith(INITIAL_ERROR_MESSAGE) for audit in audits):\n",
    "    #     return AuditVoteResult(\n",
    "    #         should_remove=False, \n",
    "    #         should_add=False, \n",
    "    #         incorrect_tally=0,\n",
    "    #         missing_tally=0,\n",
    "    #         error_tally = error_count,\n",
    "    #         total_votes=total,\n",
    "    #         stats=ALL_AUDITS_FAILED_STRING\n",
    "    #     )\n",
    "\n",
    "    # reasonings = [f\"[{i+1}] {audit.reasoning}\" for i, audit in enumerate(audits)]\n",
    "    # # reasonings = [audit.reasoning for audit in audits]\n",
    "\n",
    "    # return AuditVoteResult(\n",
    "    #     should_remove=inc_votes >= threshold,\n",
    "    #     should_add=mis_votes >= threshold,\n",
    "    #     incorrect_tally=inc_votes,\n",
    "    #     missing_tally=mis_votes,\n",
    "    #     error_tally=error_count,\n",
    "    #     total_votes=total,\n",
    "    #     stats=(\n",
    "    #         f\"Summary: {inc_votes} inc, {mis_votes} mis, {error_count} failed out of {total}.\\n\"\n",
    "    #         f\"Threshold for action: {threshold}\\n\\n\"\n",
    "    #         f\"Reasonings:\\n\" + \"\\n\\n\".join(reasonings)\n",
    "    #     )\n",
    "    # )\n",
    "    # # return AuditVoteResult(\n",
    "    # #     should_remove = inc_votes >= threshold,\n",
    "    # #     should_add = mis_votes >= threshold,\n",
    "    # #     incorrect_tally=inc_votes,\n",
    "    # #     missing_tally=missing_votes,\n",
    "    # #     total_votes=total,\n",
    "    # #     stats = f\"Inc: {inc_votes}/{len(audits)}, Mis: {mis_votes}/{len(audits)}\\n\\nReasonings:\\n\\n{\"\\n\\n\".join(reasonings)}\"\n",
    "    # # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Audit...\n",
      "Vote 0\n",
      "reasoning='Mock reasoning: no issues detected' has_incorrect_markings=False has_missing_markings=False \n",
      "\n",
      "\n",
      "Vote 1\n",
      "reasoning='Mock reasoning: no issues detected' has_incorrect_markings=False has_missing_markings=False \n",
      "\n",
      "\n",
      "Vote 2\n",
      "reasoning='Mock reasoning: no issues detected' has_incorrect_markings=False has_missing_markings=False \n",
      "\n",
      "\n",
      "\n",
      "SUCCESS:\n",
      "{'should_remove': False, 'should_add': False, 'stats': 'Inc: 0/3, Mis: 0/3\\n\\nReasonings:\\n\\nMock reasoning: no issues detected\\n\\nMock reasoning: no issues detected\\n\\nMock reasoning: no issues detected'}\n"
     ]
    }
   ],
   "source": [
    "from unittest.mock import Mock\n",
    "from openai import OpenAI\n",
    "\n",
    "# Mock client setup - JSON response for Pydantic parsing\n",
    "mock_client = Mock()\n",
    "mock_response = Mock(\n",
    "    model=\"deepseek-r1-distill-qwen-7b\",\n",
    "    choices=[Mock(\n",
    "        message=Mock(\n",
    "            content='{\"reasoning\": \"Mock reasoning: no issues detected\", \"has_incorrect_markings\": false, \"has_missing_markings\": false}'\n",
    "        )\n",
    "    )]\n",
    ")\n",
    "mock_client.chat.completions.create.return_value = mock_response\n",
    "\n",
    "raw_text_example = r\"\"\"\n",
    "Let $L/K$ be a normal separable algebraic field extension. Its Galois group $\\operatorname{Gal}(L/K)$ is defined as ...\n",
    "\"\"\"\n",
    "\n",
    "# PASS THE MOCK CLIENT\n",
    "print(\"Running Audit...\")\n",
    "# with patch('__main__.DEF_NOTAT_VERIFY_SYSTEM_PROMPT', \"Mock audit prompt.\"):\n",
    "    # ... mock_client setup same as above\n",
    "result = run_audit_voting(client=mock_client, text_to_check=raw_text_example)\n",
    "\n",
    "\n",
    "if result:\n",
    "    print(\"\\nSUCCESS:\")\n",
    "    print(result)\n",
    "else:\n",
    "    print(\"\\nFailed to get a result.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "\n",
    "if False:\n",
    "    client = OpenAI(base_url=\"http://127.0.0.1:1234/v1\", api_key=\"lm-studio\")\n",
    "\n",
    "\n",
    "    raw_text_example = r\"\"\"\n",
    "    In [Kal ], Katz examined a more general class of trigonometric sums, for which he was also able to give similar uniform estimate. To explain this result, let\n",
    "\n",
    "    $$  h: X \\longrightarrow \\mathbb{A}_\\mathbb{Z}  $$\n",
    "\n",
    "    be a finitely generated scheme over the affine line $\\mathbb{Z}_{\\mathbb{Z}}$ above $\\mathbb{Z}$. For every prime $p$, every power $q$ of it, and every non-trivial character $\\psi$ of $F_{q}$, one defines the following generalized Trigonometric Sum\n",
    "\n",
    "    $$  \\sum_{x \\in X\\left(\\mathbb{F}_{q}\\right)} \\psi(h(x))  $$\n",
    "\n",
    "    Let $N$ be the supremum of the dimensions of the fibers of the complexified morphism\n",
    "\n",
    "    $$  h \\otimes \\mathbb{C}: X \\otimes \\mathbb{C} \\longrightarrow \\mathbb{A}_{\\mathbb{C}}  $$\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Running Audit...\")\n",
    "    result = run_audit_voting(client, raw_text_example)\n",
    "\n",
    "    if result:\n",
    "        print(\"\\nSUCCESS:\")\n",
    "        print(result)\n",
    "    #     print(f\"Reasoning: {result.reasoning}\")\n",
    "    #     print(f\"Incorrect Markings: {result.has_incorrect_markings}\")\n",
    "    #     print(f\"Missing Markings:   {result.has_missing_markings}\")\n",
    "    else:\n",
    "        print(\"\\nFailed to get a result.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "client = OpenAI(base_url=\"http://127.0.0.1:1234/v1\", api_key=\"lm-studio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Audit...\n",
      "Vote 0\n",
      "reasoning=\"First, I'll examine each part of the text step by step. The excerpt starts with 'Let F be an object of C' which introduces F as a definition introduced in the current scope. Therefore, F should have a mark. Next, it mentions 'C / F' as the category being defined; this is also a newly introduced notation that needs to be marked. Following that, the objects of C / F are pairs consisting of an object X of C and a morphism u from X to F. Here, both X and u are supporting cast because they're used in the context without being defined within this specific excerpt. The morphisms between these objects are defined as g morphisms satisfying a commutative diagram condition. Since 'g' is introduced specifically for this definition, it should be marked as a new notation. There are no instances of missing marks on main subjects, so that aspect remains unchanged.\" has_incorrect_markings=False has_missing_markings=True \n",
      "\n",
      "\n",
      "Vote 1\n",
      "reasoning=\"I will analyze this excerpt step by step according to the principles outlined. First, I identify all newly introduced definitions and notations. The main subjects are 'C/F' as a category, 'objects (X,u)' within it, and morphisms from X to Y. Supporting cast includes C, F, which is an object of C; X, u, Y, v, g, G, etc., as context or props without being defined in the excerpt. The marks on these objects are checked against their roles.\" has_incorrect_markings=False has_missing_markings=False \n",
      "\n",
      "\n",
      "Vote 2\n",
      "reasoning=\"To analyze this excerpt, I first identify all newly introduced definitions and notations. The text begins by introducing an object F as an element of C, which is a Supporting Cast since it's used to define another concept without being defined itself within the excerpt. Next, the category C/F is defined, with its objects being pairs consisting of an object X in C and a morphism u from X to F. This definition introduces two new elements: the category C/F (a Main Subject) and the notation for these objects as (X, u). The term 'morphism' refers to a supporting cast because it's a standard concept already established. Similarly, g is used in the context of morphisms between Y and X, which are also supporting casts. The main subjects here are the category C/F and its objects (X, u), each requiring their respective definitions marked with 'definition'. Notations introduced include the category notation C/F and the object pair notation (X, u). All other elements like F, X, u, Y, v, g, and the morphism condition are part of supporting casts since they're either pre-existing or introduced for local use within this excerpt. Therefore, only the Main Subjectsthe category C/F and its objectsare correctly marked with 'definition' while all supporting cast elements are not marked as they should be.\" has_incorrect_markings=False has_missing_markings=True \n",
      "\n",
      "\n",
      "\n",
      "SUCCESS:\n",
      "{'should_remove': False, 'should_add': True, 'stats': \"Inc: 0/3, Mis: 2/3\\n\\nReasonings:\\n\\nFirst, I'll examine each part of the text step by step. The excerpt starts with 'Let F be an object of C' which introduces F as a definition introduced in the current scope. Therefore, F should have a mark. Next, it mentions 'C / F' as the category being defined; this is also a newly introduced notation that needs to be marked. Following that, the objects of C / F are pairs consisting of an object X of C and a morphism u from X to F. Here, both X and u are supporting cast because they're used in the context without being defined within this specific excerpt. The morphisms between these objects are defined as g morphisms satisfying a commutative diagram condition. Since 'g' is introduced specifically for this definition, it should be marked as a new notation. There are no instances of missing marks on main subjects, so that aspect remains unchanged.\\n\\nI will analyze this excerpt step by step according to the principles outlined. First, I identify all newly introduced definitions and notations. The main subjects are 'C/F' as a category, 'objects (X,u)' within it, and morphisms from X to Y. Supporting cast includes C, F, which is an object of C; X, u, Y, v, g, G, etc., as context or props without being defined in the excerpt. The marks on these objects are checked against their roles.\\n\\nTo analyze this excerpt, I first identify all newly introduced definitions and notations. The text begins by introducing an object F as an element of C, which is a Supporting Cast since it's used to define another concept without being defined itself within the excerpt. Next, the category C/F is defined, with its objects being pairs consisting of an object X in C and a morphism u from X to F. This definition introduces two new elements: the category C/F (a Main Subject) and the notation for these objects as (X, u). The term 'morphism' refers to a supporting cast because it's a standard concept already established. Similarly, g is used in the context of morphisms between Y and X, which are also supporting casts. The main subjects here are the category C/F and its objects (X, u), each requiring their respective definitions marked with 'definition'. Notations introduced include the category notation C/F and the object pair notation (X, u). All other elements like F, X, u, Y, v, g, and the morphism condition are part of supporting casts since they're either pre-existing or introduced for local use within this excerpt. Therefore, only the Main Subjectsthe category C/F and its objectsare correctly marked with 'definition' while all supporting cast elements are not marked as they should be.\"}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# raw_text_example = r\"\"\"\n",
    "\n",
    "# 3.4.0. Let $\\mathrm{F}$ be an object of $\\mathrm{C}$ We denote by $\\mathrm{C} / \\mathrm{F}$ the following category: The objects of $\\mathrm{C} / \\mathrm{F}$ are the pairs formed by an object $\\mathrm{X}$ of $\\mathrm{C}$ and of a morphism $u$ of $\\mathrm{X}$ in $\\mathrm{F}$. Let $(\\mathrm{X}, u)$ and $(\\mathrm{Y}, v)$ two objects. A morphism from $(\\mathrm{X}, u)$ to $(\\mathrm{Y}, v)$ is a $g$ morphism from X to Y such that the following diagram is commutative:\n",
    "\n",
    "# ![[Pasted image 20250209192841.png]]\n",
    "# \"\"\"\n",
    "\n",
    "# print(\"Running Audit...\")\n",
    "# result = run_audit_voting(client, raw_text_example)\n",
    "\n",
    "# if result:\n",
    "#     print(\"\\nSUCCESS:\")\n",
    "#     print(result)\n",
    "# #     print(f\"Reasoning: {result.reasoning}\")\n",
    "# #     print(f\"Incorrect Markings: {result.has_incorrect_markings}\")\n",
    "# #     print(f\"Missing Markings:   {result.has_missing_markings}\")\n",
    "# else:\n",
    "#     print(\"\\nFailed to get a result.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
