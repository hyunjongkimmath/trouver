"""Functions for gathering and processing tokenization data and for using ML models trained with such data."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../../nbs/07_machine_learning_15.tokenize.def_and_notat_token_classification.ipynb.

# %% auto 0
__all__ = ['latex_commands_to_avoid', 'DEF_NOTAT_VERIFY_SYSTEM_PROMPT', 'DEF_NOTAT_VERIFY_USER_PROMPT', 'INITIAL_ERROR_MESSAGE',
           'ALL_AUDITS_FAILED_STRING', 'convert_double_asterisks_to_html_tags',
           'raw_text_with_html_tags_from_markdownfile', 'HTMLData', 'html_data_from_note', 'tokenize_html_data',
           'def_or_notat_from_html_tag', 'extract_html_tag_indices_from_marked_text', 'html_data_from_marked_text',
           'latex_highlight_parser', 'augment_html_data', 'def_and_notat_preds_by_model',
           'get_def_and_notat_predictions', 'mark_def_and_notat_predictions', 'predict_and_mark_def_and_notats',
           'auto_mark_def_and_notats', 'latex_highlight_formatter', 'AuditResult', 'AuditVoteResult',
           'salvage_audit_result', 'run_strict_audit', 'run_audit_voting', 'run_audit_voting_maker']

# %% ../../../nbs/07_machine_learning_15.tokenize.def_and_notat_token_classification.ipynb 4
from collections.abc import Callable
import copy
from itertools import pairwise
import os 
from os import PathLike
from pathlib import Path
import random
from typing import Literal, Optional, TypedDict, Union
import warnings

import bs4
from openai import OpenAI
from pydantic import BaseModel
import regex
from transformers import BatchEncoding, pipelines, PreTrainedTokenizer, PreTrainedTokenizerFast

from ...helper import is_not_space_and_not_punc, split_string_at_indices
from ...helper.definition_and_notation import double_asterisk_indices, notation_asterisk_indices
from trouver.helper.html import (
    add_HTML_tag_data_to_raw_text, add_space_to_lt_symbols_without_space, remove_html_tags_in_text, StrAndHTMLTagsWithIndices,
    HTMLTagWithIndices)
from ...helper.latex.core import _is_balanced_braces, _first_curly_bracket, _last_curly_bracket
from ...helper.latex.augment import (
    augment_text, change_font_styles_at_random, change_greek_letters_at_random, dollar_sign_manipulation, push_dollar_sign, random_char_modification, random_latex_command_removal, random_word_removal, remove_font_styles_at_random, remove_math_keywords)

from ...helper.regex import latex_indices, replace_string_by_indices
from ...obsidian.file import MarkdownFile, MarkdownLineEnum
from ...personal_vault.note_processing import process_standard_information_note
from ...obsidian.vault import VaultNote




# %% ../../../nbs/07_machine_learning_15.tokenize.def_and_notat_token_classification.ipynb 8
def convert_double_asterisks_to_html_tags(
        text: str
        ) -> str:
    """
    Replace the double asterisks, which signify definitions and notations,
    in `text` with HTML tags.
    """
    double_asts = double_asterisk_indices(text)
    replacement_html_tags = [
        _html_tag_from_double_ast(text[start:end])
        for start, end in double_asts]
    return replace_string_by_indices(
        text, double_asts, replacement_html_tags)


def _html_tag_from_double_ast(
        double_ast_string: str # Starts and ends with double asts
        ) -> str:
    """
    Get the HTML tag representing definition or notation data from
    a string surrounded by double asterisks.

    This is used in the `_convert_double_asterisks_to_html_tags` function.
    """
    no_asts = double_ast_string[2:-2]
    if notation_asterisk_indices(double_ast_string):
        return f'<span notation="">{no_asts}</span>'
    else:
        return f'<b definition="">{no_asts}</b>'


# %% ../../../nbs/07_machine_learning_15.tokenize.def_and_notat_token_classification.ipynb 11
def raw_text_with_html_tags_from_markdownfile(
        mf: MarkdownFile,
        vault: PathLike
        ) -> str:
    """
    Process the `MarkdownFile`, replacing the double asterisk surrounded
    text indicating definitions and notations to be HTML tags instead.
    """
    mf = process_standard_information_note(
        mf, vault, remove_double_asterisks=False,
        remove_html_tags=False)
    return convert_double_asterisks_to_html_tags(str(mf))



# %% ../../../nbs/07_machine_learning_15.tokenize.def_and_notat_token_classification.ipynb 19
class HTMLData(TypedDict):
    note_name: str
    raw_text: str
    tags: list[HTMLTagWithIndices]
    # list[bs4.element.Tag]


# %% ../../../nbs/07_machine_learning_15.tokenize.def_and_notat_token_classification.ipynb 20
def html_data_from_note(
        note_or_mf: Union[VaultNote, MarkdownFile], # Either a `VaultNote`` object to a note or a `MarkdownFile` object from which to extra html data.
        vault: Optional[PathLike] = None, # If vault to use when processing the `MarkdownFile` objects (if `note_of_mf` is a `VaultNote`, then this `MarkdownFile` object is created from the text of the note), cf. the `process_standard_information_note` function.
        note_name: Optional[str] = None, # If `note_or_mf` is a `MarkdownFile`, `note_name` should be the name of the note from which the `MarkdownFile` comes from if applicable. If `note_or_mf` is a `VaultNote` object, then `note_name` is ignored and `note_or_mf.name` is used instead.
        ) -> Union[HTMLData, None]: # The keys to the dict are "note_name", "raw_text", "tags". However, `None` is returned if `note` does not exist or the note is marked with auto-generated, unverified data.
    # TODO: implement obtaining multiple datapoints from a single note
    # Via typos for example.
    # TODO: implement various data augmentation techniques
    """Obtain html data for token classification from the information note.

    Currently, the token types mainly revolve around definitions and
    notations.

    If `note` has the tag `_auto/def_and_notat_identified`, then the data
    in the note is assumed to be auto-generated and not verified and
    `None` is returned.

    **Returns**
    - Union[dict, None]
        - The keys-value pairs are 
            - `"note_name"` - The name of the note
            - `"raw_text"` - The raw text to include in the data.
            - `"tags"` - The list with HTML tags carrying definition/notation
              data and their locations in the Raw text. See the second output to
              the function `remove_html_tags_in_text`.
                - Each element of the list is a tuple consisting of a ``bs4.element.Tag``
                  and two ints.
    """
    if isinstance(note_or_mf, VaultNote) and not note_or_mf.exists():
        return None
    if isinstance(note_or_mf, VaultNote):
        mf = MarkdownFile.from_vault_note(note_or_mf)
        note_name = note_or_mf.name
        if vault is None:
            vault = note_or_mf.vault
    else: # isinstance(note_or_mf, MarkdownFile):
        mf = note_or_mf.copy(deep=False)
    if mf.has_tag('_auto/def_and_notat_identified'):
        return None
    raw_text_with_tags = raw_text_with_html_tags_from_markdownfile(mf, vault)
    raw_text, tags_and_locations = remove_html_tags_in_text(raw_text_with_tags)

    return HTMLData(note_name=note_name, raw_text=raw_text, tags=tags_and_locations)

# %% ../../../nbs/07_machine_learning_15.tokenize.def_and_notat_token_classification.ipynb 31
def tokenize_html_data(
        html_locus: HTMLData, # An output of `html_data_from_note`
        tokenizer: Union[PreTrainedTokenizer, PreTrainedTokenizerFast],
        max_length: int, # Max length for each sequence of tokens
        ner_tag_from_html_tag: Callable[[bs4.element.Tag], Union[str, 'None']], # takes in a bs4.element.Tag and outputs the ner_tag (as a string or `None`)
        label2id: dict[str, int], # The keys are ner_tag's of the form f"I-{output}" or f"B-{output}" where `output` is an output of `ner_tag_from_html_tag`.
        default_label: str = "O", # The default label for the NER tagging.
        ) -> tuple[list[list[str]], list[list[int]]]: # The first list consists of the tokens and the second list consists of the named entity recognition tags.
    """Actually tokenize the html data outputted by `html_data_from_note`.

    To account for the possibility that the raw text is long,
    this function uses the `tokenizer.batch_encode_plus` function
    to tokenize the text into sequences. 
    """
    tokenized = tokenizer.batch_encode_plus(
        [html_locus["raw_text"]], max_length=max_length, return_overflowing_tokens=True,
        return_offsets_mapping=True, truncation=True)

    default_id = label2id[default_label]        
    ner_ids = [[default_id for _ in seq_input_ids]
               for seq_input_ids in tokenized['input_ids']]
    for tag, start, end in html_locus['tags']:
        ner_tag = ner_tag_from_html_tag(tag)
        if ner_tag is None:
            continue  # `ner_tag` is not of relevant data.
        tuppy = _start_end_seqs_indices_for_html_tag(tokenized, start, end - 1)
        (start_seq, start_index_in_seq), (end_seq, end_index_in_seq) = tuppy
        _set_ner_ids_for_tag(
            ner_ids, start_seq, start_index_in_seq, end_seq, end_index_in_seq,
            label2id, ner_tag)
    # return tokenized["input_ids"], ner_ids
    tokens = [tokenizer.convert_ids_to_tokens(tokens_for_seq)
              for tokens_for_seq in tokenized["input_ids"]]
    return tokens, ner_ids


def _start_end_seqs_indices_for_html_tag(
        tokenized: BatchEncoding,
        tag_start_ind: int,
        tag_end_ind: int
        ) -> tuple[tuple[int, int], tuple[int, int]]: # The first tuple is `(a, b)` where `tokenized['input_ids'][a][b]` is the token corresponding to the start of the HTML tag's (raw) text. The second tuple is `(c, d)` where `tokenized['input_ids'][c][d]` is the token corresponding to the end of the HTML tag's (raw) text.
    start_seq = _search_seq_ind_for_char(tokenized['offset_mapping'], tag_start_ind)
    # start_index_in_seq = tokenized.char_to_token(batch_or_char_index=start_seq, char_index=tag_start_ind)
    start_index_in_seq = _search_within_seq_for_char(tokenized['offset_mapping'][start_seq], tag_start_ind)
    end_seq = _search_seq_ind_for_char(tokenized['offset_mapping'], tag_end_ind)
    # end_index_in_seq = tokenized.char_to_token(batch_or_char_index=end_seq, char_index=tag_end_ind)
    end_index_in_seq = _search_within_seq_for_char(tokenized['offset_mapping'][end_seq], tag_end_ind)
    return (start_seq, start_index_in_seq), (end_seq, end_index_in_seq)


def _min_max_char_ind_for_seq(
        offset_for_seq: list[tuple[int,int]] # An item in tokenized['offset_mapping']
        ):
    min_char_ind, max_char_ind = 0, 0
    for inds in offset_for_seq:
        if inds != (0,0):
            min_char_ind = inds[0]
            break
    for inds in reversed(offset_for_seq):
        if inds != (0,0):
            max_char_ind = inds[1]
            break
    return min_char_ind, max_char_ind

def _char_is_in_seq(
        offset_for_seq: list[int], # An item in tokenized['offset_mapping']
        char: int # The index of a character in the original raw text
        ) -> bool:
    min_char_ind, max_char_ind = _min_max_char_ind_for_seq(offset_for_seq)
    return min_char_ind <= char and char < max_char_ind

def _search_seq_ind_for_char(
        offsets: list[tuple[int, int]], # tokenized['offset_mapping']
        char: int # The index of a character in the original raw text
        ) -> int:
    """
    Binary search the index of the sequence containing the token at the 
    location of the index `char` within the original (raw) text.

    Based on pseudocode from https://pseudoeditor.com/guides/binary-search
    """
    left = 0
    right = len(offsets) - 1
    while left <= right:
        mid = (left + right) // 2
        min_char_ind, max_char_ind = _min_max_char_ind_for_seq(offsets[mid])
        if min_char_ind <= char and char < max_char_ind:
            return mid
        elif max_char_ind <= char:
            left = mid + 1
        else:
            right = mid - 1
    return -1  # This should not be returned under normal use.


def _search_within_seq_for_char(
        seq_offset: list[tuple[int, int]],
        char: int
    ) -> int:
    """
    Binary search for the index within the sequence corresponding
    to the token at the location of the index `char` within the
    original (raw) text.

    Based on pseudocode from https://pseudoeditor.com/guides/binary-search
    """
    left = 0
    right = len(seq_offset) - 1
    while left <= right:
        mid = (left + right) // 2
        min_char_ind, max_char_ind = seq_offset[mid] 
        if min_char_ind <= char and char < max_char_ind:
            return mid
        elif max_char_ind <= char:
            left = mid + 1
        else:
            right = mid - 1
    return -1  # This should not be returned under normal use.


def _set_ner_ids_for_tag(
        ner_ids: list[list[int]],
        start_seq: int, 
        start_index_in_seq: int,
        end_seq: int,
        end_index_in_seq: int,
        label2id: dict[str, int],
        ner_tag: str
        ) -> None:
    """
    After the locations of the tokens corresponding to a HTML tag have been found, 
    mark within `ner_ids` the appropriate NER tags at the locations corresponding
    to the tokens' locations.
    """
    ner_ids[start_seq][start_index_in_seq] = label2id[f"B-{ner_tag}"]
    i_ner_id = label2id[f"I-{ner_tag}"]
    seq, ind = start_seq, start_index_in_seq + 1
    while seq < end_seq or ind <= end_index_in_seq:
        if len(ner_ids[seq]) <= ind:
            seq += 1
            ind = 0
        else:
            ner_ids[seq][ind] = i_ner_id 
            ind += 1
    


def def_or_notat_from_html_tag(
        tag: bs4.element.Tag
        ) -> Union[str, None]:
    """
    Can be passed as the `ner_tag_from_html_tag` argument in `tokenize_html_data`
    for the purposes of compiling a dataset for definition and notation
    identification.

    The strings f"I-{output}" and f"B-{output}" are valid ner_tags. To use for 
    """
    if "definition" in tag.attrs:
        return "definition"
    elif "notation" in tag.attrs:
        return "notation"
    return None  # If the HTML tag carries neither definition nor notation data.

# %% ../../../nbs/07_machine_learning_15.tokenize.def_and_notat_token_classification.ipynb 49
def _calculate_clean_indices_and_create_tags(
        original_text: str,
        markings: list[tuple[str, int, int, dict]]
        ) -> tuple[str, list[HTMLTagWithIndices]]:
    """Helper: returns (clean_text, list_of_tags)."""
    results = []
    clean_text_parts = []
    current_raw_idx = 0
    clean_text_len = 0
    soup = bs4.BeautifulSoup("", 'html.parser')

    for content, raw_start, raw_end, attrs in markings:
        # Append text BEFORE the mark
        pre_text = original_text[current_raw_idx:raw_start]
        clean_text_parts.append(pre_text)
        
        pre_text_len = len(pre_text)
        clean_text_len += pre_text_len
        
        # Calculate indices
        tag_start = clean_text_len
        tag_end = tag_start + len(content)
        
        # Create Tag
        tag_name = "b" if "definition" in attrs else "span"
        tag = soup.new_tag(tag_name, **attrs)
        tag.string = content
        results.append(HTMLTagWithIndices(tag, tag_start, tag_end))
        
        # Append the CONTENT of the mark (clean text)
        clean_text_parts.append(content)
        
        clean_text_len += len(content)
        current_raw_idx = raw_end

    # Append remaining text
    clean_text_parts.append(original_text[current_raw_idx:])
    
    return "".join(clean_text_parts), results

# %% ../../../nbs/07_machine_learning_15.tokenize.def_and_notat_token_classification.ipynb 50
def extract_html_tag_indices_from_marked_text(
        text: str, # The text containing custom markings (e.g. "Let [NOT:G] be a [DEF:group]").
        marker_parser: Callable[[str], list[tuple[str, int, int, dict]]] # A function that parses the text and returns a list of tuples. Each tuple should contain: (1) The *inner content* of the marked section, (2) The *start index* of the marking in `text`, (3) The *end index* of the marking in `text`, and (4) A dictionary of *attributes* for the HTML tag.
        ) -> list[HTMLTagWithIndices]: # A list of `HTMLTagWithIndices` objects. The start/end indices corresponds to the *inner content's* location in a "clean" version of the text (where markings are removed).
    """
    Extracts a list of HTML tags and their indices from marked text.
    """
    markings = marker_parser(text)
    markings.sort(key=lambda x: x[1])
    _, tags = _calculate_clean_indices_and_create_tags(text, markings)
    return tags

# %% ../../../nbs/07_machine_learning_15.tokenize.def_and_notat_token_classification.ipynb 52
def html_data_from_marked_text(
        text: str, # The text containing custom markings (e.g. "Let [NOT:G] be a [DEF:group]").
        marker_parser: Callable[[str], list[tuple[str, int, int, dict]]] # A function that parses the text and returns a list of tuples. Each tuple should contain: (1) The *inner content* of the marked section, (2) The *start index* of the marking in `text`, (3) The *end index* of the marking in `text`, and (4) A dictionary of *attributes* for the HTML tag.
        ) -> StrAndHTMLTagsWithIndices: # An object containing the "clean" text (markings removed) and the list of HTML tags with their indices in that clean text.
    """
    Creates an `StrAndHTMLTagsWithIndices` object from text with custom markings.
    """
    markings = marker_parser(text)
    markings.sort(key=lambda x: x[1])
    clean_text, tags = _calculate_clean_indices_and_create_tags(text, markings)
    
    return StrAndHTMLTagsWithIndices(clean_text, tags)

# %% ../../../nbs/07_machine_learning_15.tokenize.def_and_notat_token_classification.ipynb 56
def latex_highlight_parser(text: str) -> list[tuple[str, int, int, dict]]:
    """
    Parses LaTeX highlighting commands (\\hldef, \\hl, \\hlin, \\hlalign) to identify
    definitions and notations.
    
    This function is designed to be passed as the `marker_parser` argument to
    `html_data_from_marked_text`.

    It handles nested braces using recursive regex and strips surrounding whitespace 
    from the inner content (e.g., stripping padding newlines in `\\hlalign{\\n ... \\n}`).
    It also implements special logic for `\\hlin` to expand the notation range to 
    include surrounding `$$` delimiters if present.

    Returns
    -------
    list[tuple[str, int, int, dict]]
        A list of tuples representing the found markings. Each tuple contains:
        1. **Content** (`str`): The inner text of the marking (stripped of padding whitespace).
           This is the text that will remain in the "clean" output and be wrapped by the tag.
        2. **Start Index** (`int`): The start index of the *entire marking wrapper* 
           (e.g., the index of `\\`) in the original `text`.
        3. **End Index** (`int`): The end index of the *entire marking wrapper* 
           (e.g., the index after `}`) in the original `text`.
        4. **Attributes** (`dict`): A dictionary of HTML attributes, e.g. 
           `{'definition': ''}` or `{'notation': ''}`.
    """
    pattern = r'\\(hldef|hlalign|hlin|hl)\s*(\{(?:[^{}]++|(?2))*\})'
    results = []
    
    for match in regex.finditer(pattern, text):
        cmd_type = match.group(1)
        full_brace_group = match.group(2)
        raw_inner = full_brace_group[1:-1]
        match_start, match_end = match.span()
        
        # Strip padding whitespace from content
        lstripped = raw_inner.lstrip()
        stripped = lstripped.rstrip()
        content = stripped
        
        start = match_start
        end = match_end
        attrs = {'definition' if cmd_type == 'hldef' else 'notation': ''}
        
        if cmd_type == 'hlin':
            # 1. Scan Backwards for start $$
            p = start - 1
            while p >= 0 and text[p].isspace(): p -= 1
            
            # Check if we hit $$ immediately (ignoring space)
            if p >= 1 and text[p] == '$' and text[p-1] == '$':
                found_start_dollars = True
                new_start = p - 1
                # BUG WAS HERE: prefix = "" 
                # FIX: Capture the text between $$ (p+1) and \hlin (start)
                prefix = text[p+1 : start] 
            else:
                found_start_dollars = False
                prefix = "" # Initialize for safety

            
            # 2. Scan Forwards for end $$
            # We want to allow punctuation like "." or "," between } and $$
            # E.g. $$ \hlin{x}. $$
            
            q = end
            suffix = ""
            found_end_dollars = False
            
            # Heuristic: Scan forward for a limited distance or until $$
            # We capture everything between } and $$ into 'suffix'
            # Stop if we hit a newline (safeguard)
            temp_q = q
            while temp_q < len(text) - 1:
                if text[temp_q] == '\n': break
                
                if text[temp_q] == '$' and text[temp_q+1] == '$':
                    found_end_dollars = True
                    new_end = temp_q + 2
                    # The text between original end and $$ is the suffix
                    suffix = text[end:temp_q]
                    # Clean the suffix? Usually we just want to include it.
                    # e.g. suffix might be ". " (period and space)
                    # We usually trim the space before the $$, but keeping it is safer for fidelity.
                    break
                temp_q += 1
            
            # Only apply expansion if we found BOTH start and end $$
            # AND (optionally) if we found the start $$ directly. 
            # (Does it make sense to have content BEFORE \hlin? e.g. $$ x = \hlin{y} $$?
            #  If so, we should scan backwards for $$ similarly to how we scanned forwards).
            
            # Let's implement symmetric scanning for robustness.
            
            # RE-TRY Backward Scan with content capture
            if not found_start_dollars:
                 temp_p = start - 1
                 while temp_p >= 1:
                     if text[temp_p] == '\n': break
                     if text[temp_p] == '$' and text[temp_p-1] == '$':
                         found_start_dollars = True
                         new_start = temp_p - 1
                         prefix = text[temp_p+1 : start] # Content between $$ and \hlin
                         break
                     temp_p -= 1

            if found_start_dollars and found_end_dollars:
                # We found $$ ... \hlin{...} ... $$
                # We want the tag to cover the WHOLE thing: "$$ prefix content suffix $$"
                # And we want to remove the WHOLE thing from the text.
                
                # Careful: The 'prefix' and 'suffix' currently contain the raw characters 
                # from the marked text (including potentially ignored spaces).
                # We should probably strip excessive padding next to the $$ inside the tag?
                # Standard convention: $$ content $$ -> Tag content "$$ content $$"
                
                # Let's just assemble it raw to preserve user's punctuation/spacing logic,
                # then maybe strip outer edges if needed.
                
                # Current 'content' is stripped inner content of \hlin.
                # 'prefix' is text between $$ and \hlin.
                # 'suffix' is text between \hlin and $$.
                
                # Example: $$ \hlin{K}. $$
                # prefix = " "
                # content = "K"
                # suffix = ". "
                # Result: "$$ K. $$"
                
                full_content = f"$${prefix}{content}{suffix}$$"
                
                content = full_content
                start = new_start
                end = new_end

        results.append((content, start, end, attrs))
        
    return results
    # pattern = r'\\(hldef|hlalign|hlin|hl)\s*(\{(?:[^{}]++|(?2))*\})'
    
    # results = []
    
    # for match in regex.finditer(pattern, text):
    #     cmd_type = match.group(1)
    #     full_brace_group = match.group(2)
        
    #     # Raw content inside braces (e.g. " \n Content \n ")
    #     raw_inner = full_brace_group[1:-1]
        
    #     # Calculate indices relative to the 'text' string
    #     # Match span covers `\hl{...}`
    #     match_start, match_end = match.span()
        
    #     # We want to identify where the "real" content starts/ends inside the match
    #     # Start of raw_inner is: match_end - 1 (closing brace) - len(raw_inner)
    #     # Actually easier: find start of {
    #     brace_start_idx = match.start(2) # Index of {
    #     inner_start_idx = brace_start_idx + 1
        
    #     # Find leading/trailing whitespace length
    #     lstripped = raw_inner.lstrip()
    #     leading_ws_len = len(raw_inner) - len(lstripped)
        
    #     stripped = lstripped.rstrip()
    #     trailing_ws_len = len(lstripped) - len(stripped)
        
    #     content = stripped
    #     attrs = {'definition' if cmd_type == 'hldef' else 'notation': ''}
        
    #     # 1. Base Range: The command wrapper `\cmd{` and `}`.
    #     # We want to effectively say:
    #     # "Remove `\cmd{ \n`", keep `Content`, "Remove `\n }`".
    #     # But our interface only supports "Remove `Range`, Insert `Content`".
        
    #     # If we return `start=match_start`, `end=match_end`, `content=stripped`:
    #     # "Remove `\hl{ \n Content \n }`. Insert `Content`."
    #     # Result Clean Text: `Content`. (Newlines LOST).
        
    #     # If the user WANTS those newlines preserved in the clean text (outside the tag),
    #     # we have a problem. The current architecture assumes "Marked Region" -> "Tag".
    #     # It doesn't support "Marked Region" -> "Prefix + Tag + Suffix".
        
    #     # DECISION: 
    #     # For `\hlalign`, users usually write:
    #     # \hlalign{
    #     # \begin{align}
    #     # ...
    #     # \end{align}
    #     # }
    #     # They EXPECT the clean text to contain the newlines so the align renders correctly?
    #     # Actually, LaTeX doesn't care about the surrounding newlines much.
    #     # `\begin{align}...\end{align}` is valid without extra newlines.
    #     # So losing the newlines inside the braces is probably ACCEPTABLE and cleaner.
        
    #     # HOWEVER, if `$$ \hlin{ x } $$` -> `$$x$$`. Losing spaces is fine.
        
    #     # Special Logic for \hlin ($$) remains...
        
    #     # Let's apply the stripping logic:
        
    #     start = match_start
    #     end = match_end
        
    #     if cmd_type == 'hlin':
    #         # ... (Logic to expand to $$ remains same, but apply to `stripped` content) ...
    #         # Re-implementing simplified logic for clarity in this snippet:
    #         p = start - 1
    #         while p >= 0 and text[p].isspace(): p -= 1
    #         if p >= 1 and text[p] == '$' and text[p-1] == '$':
    #             new_start = p - 1
    #             q = end
    #             while q < len(text) and text[q].isspace(): q += 1
    #             if q < len(text) - 1 and text[q] == '$' and text[q+1] == '$':
    #                 new_end = q + 2
    #                 content = f"$${content}$$"
    #                 start = new_start
    #                 end = new_end

    #     results.append((content, start, end, attrs))
        
    # return results


# %% ../../../nbs/07_machine_learning_15.tokenize.def_and_notat_token_classification.ipynb 67
def _split_text_by_html_data_parts(
        # text_tags_and_locations = StrAndHTMLTagsWithIndices
        datapoint: HTMLData
        ) -> list[tuple[str, Union[bs4.element.Tag, None]]]:
    r"""
    Helper function to `augment_html_data`.
    """
    to_return: list[tuple[str, Union[bs4.element.Tag, None]]] = []
    split_points: list[int] = []
    for tag_ind in datapoint['tags']:
        split_points.append(tag_ind.start)
        split_points.append(tag_ind.end)
    pieces: list[str] = split_string_at_indices(datapoint['raw_text'], split_points)
    for i, piece in enumerate(pieces):
        if i % 2 == 0:
            to_return.append((piece, None))
        else:
            to_return.append((piece, datapoint['tags'][int(i/2)].tag))
    return to_return


# %% ../../../nbs/07_machine_learning_15.tokenize.def_and_notat_token_classification.ipynb 71
def augment_html_data(
        datapoint: HTMLData,
        num_augmentation_sets: int = 1, # Each augmentation set consists of an augmentation with low, medium, and high probability modifications.
        seed: Optional[int] = None
        ) -> list[HTMLData]:
    r"""Augment a given datapoint for HTML tagging.

    """
    augmented_datapoints: list[HTMLData] = []
    pieces: list[tuple[str, Union[bs4.element.Tag, None]]] = _split_text_by_html_data_parts(
        datapoint)
    if seed is not None:
        random.seed(seed)
    for _ in range(num_augmentation_sets):
        augmented_datapoints.append(
            _augment_html_data_once(pieces, 'low', datapoint['note_name']))
        augmented_datapoints.append(
            _augment_html_data_once(pieces, 'mid', datapoint['note_name']))
        augmented_datapoints.append(
            _augment_html_data_once(pieces, 'hi', datapoint['note_name']))
        # augmented_datapoints.append(_augment_html_data_once(pieces, 'high'))
    return augmented_datapoints


def _augment_html_data_once(
        pieces: list[tuple[str, Union[bs4.element.Tag, None]]],
        modification: Literal['low', 'mid', 'high'],
        note_name: str,
        ) -> HTMLData:
    methods = [
        # (push_dollar_signs,0.2),
        (remove_font_styles_at_random, 0.1), (change_font_styles_at_random, 0.2), (change_greek_letters_at_random, 0.1), 
        (remove_math_keywords,0.1), (random_latex_command_removal,0.2),
        (random_word_removal,0.1), (dollar_sign_manipulation,0.05),
        (random_char_modification,0.001)]
    if modification == 'low':
        method_inclusion_chance = 0.3
        scale = 0.5
    elif modification == 'mid':
        method_inclusion_chance = 0.5
        scale = 1.0
    else:
        method_inclusion_chance = 0.8
        scale = 1.5
    
    random_methods = []
    def create_method(method, p, scale):
        return lambda x: method(x, p=p*scale)
    for method, p in methods:
        if random.random() < method_inclusion_chance:
            random_methods.append(create_method(method, p, scale))
    # random_methods = [
    #     lambda x: method(x, p=p*scale) for method, p in methods if random.random() < method_inclusion_chance]
    augmented_pieces = [
        (augment_text(text, random_methods), copy.copy(tag))
        for text, tag in pieces]
    accumulated_len: int = 0
    accumulated_text: str = ""
    tags_with_indices: list[HTMLTagWithIndices] = []
    for text, tag in augmented_pieces:
        accumulated_text = f'{accumulated_text}{text}'
        if tag:
            tag.string = text
            tags_with_indices.append(HTMLTagWithIndices(
                tag, accumulated_len, accumulated_len + len(text)))
        accumulated_len += len(text)
    return HTMLData(note_name=note_name, raw_text=accumulated_text, tags=tags_with_indices)

# %% ../../../nbs/07_machine_learning_15.tokenize.def_and_notat_token_classification.ipynb 85
def _make_tag(
        text: str,
        entity_type: str # 'definition' or 'notation'
        ) -> bs4.element.Tag:
    """
    Helper function to `_html_tag_data_from_part` and `_consolidate_token_preds`.
    """
    soup = bs4.BeautifulSoup('', 'html.parser')
    if entity_type == 'definition':
        tag = soup.new_tag(
            'b',
            style="border-width:1px;border-style:solid;padding:3px",
            definition="")
    else:
        tag = soup.new_tag(
            'span',
            style="border-width:1px;border-style:solid;padding:3px",
            notation="")
    tag.string = text
    return tag


def _html_tag_data_from_part(
        main_text: str,
        # part: list[dict[str]]) -> tuple[bs4.element.Tag, int, int]:
        part: list[dict[str]] # An item of an output of `_divide_token_preds_into_parts`. Each dict likely contains keys such as `'entity'`, `'score'`, `'index'`, `'word'`, `'start'`, and `'end'`, depending on the model used.
        ) -> tuple[HTMLTagWithIndices]:
    """
    Helper function to `_html_tags_from_token_preds`
    """
    start_token: dict[str] = part[0]
    end_token: dict[str] = part[-1]
    start_char_pos: int = start_token['start']
    end_char_pos: int = end_token['end']

    # Depending on the tokenizer starting spaces might be included in a given token.
    # We exclude such a starting space.
    while main_text[start_char_pos].isspace():
        start_char_pos += 1
    while main_text[end_char_pos-1].isspace():
        end_char_pos -= 1

    # the `'entity'` is either 'I-definition', 'B-definition', 'I-notation',
    # or 'B-notation'
    entity_type = start_token['entity'][2:]
    html_text = main_text[start_char_pos:end_char_pos]
    
    # return (_make_tag(html_text, entity_type), start_char, end_char)
    return HTMLTagWithIndices(_make_tag(html_text, entity_type), start_char_pos, end_char_pos)


# %% ../../../nbs/07_machine_learning_15.tokenize.def_and_notat_token_classification.ipynb 88
def _current_token_continues_the_previous_token(
        current_token: dict,
        previous_token: dict,
        note: Optional[VaultNote] = None,
        ) -> bool:
    """
    Helper function to `_divide_token_preds_into_parts`.
    """
    if current_token['entity'].startswith('I-'):
        if current_token['entity'][2:] == previous_token['entity'][2:]:
            return True
        elif note:
            warnings.warn(rf"""
                In the note {note.name} at {note.path()},
                The token '{previous_token['word']}' is marked as '{previous_token['entity']}'
                and the subsequent token '{current_token['word']}' is marked as '{current_token['entity']}',
                which is unusual because the two consecutive tokens seem to be of different
                entities, and yet the latter token does not start with a 'B-'.

                The latter token will be treated like the beginning of a new entity."""
                    )
            return False
    else:
        return False
        

# %% ../../../nbs/07_machine_learning_15.tokenize.def_and_notat_token_classification.ipynb 90
def _divide_token_preds_into_parts(
        token_preds: list[dict[str]],
        excessive_space_threshold: int,
        note: Optional[VaultNote] = None
        ) -> list[list[dict[str]]]:
    """
    Divide `token_preds` into parts so that each part
    represents a single definition/notation marking.

    Helper function to `_html_tags_from_token_preds`.
    """
    token_preds_parts = []
    for current_token in token_preds:
        if not token_preds_parts:
            token_preds_parts.append([current_token])
            continue
        prev_token = token_preds_parts[-1][-1]
        if _current_token_continues_the_previous_token(
                current_token, prev_token, note):
            prev_token_end = prev_token['end']
            cur_token_start = current_token['start']
            if prev_token_end + excessive_space_threshold >= cur_token_start and note:
                Warning(rf"""
                    In the note {note.name} at {note.path()},
                    There seems to be excessive space between the token
                    {prev_token['word']} and {current_token['word']}, which
                    seem to be part of the same entity"""
                        )
            token_preds_parts[-1].append(current_token)
        else:
            token_preds_parts.append([current_token])
    return token_preds_parts


# %% ../../../nbs/07_machine_learning_15.tokenize.def_and_notat_token_classification.ipynb 92
def _ranges_overlap(
        current_1: HTMLTagWithIndices,
        current_2: HTMLTagWithIndices
        # current_1: tuple[bs4.element.Tag, int, int],
        # current_2: tuple[bs4.element.Tag, int, int]
        ) -> bool:
    """
    Based on https://stackoverflow.com/a/64745177

    Helper function to `_collate_html_tags`, `_consolidate_token_preds`.
    """
    return max(current_1.start, current_2.start) < min(current_1.end, current_2.end)


# %% ../../../nbs/07_machine_learning_15.tokenize.def_and_notat_token_classification.ipynb 94
# If the ML model predicts 
# predictions made around 
latex_commands_to_avoid = [
    # Document structure
    "\\documentclass", "\\usepackage", "\\begin", "\\end",
    # Sectioning
    "\\part", "\\chapter", "\\section", "\\subsection", "\\subsubsection", "\\paragraph", "\\subparagraph",
    # Referencing and citation
    "\\ref", "\\pageref", "\\cite", "\\bibitem", "\\bibliography", "\\bibliographystyle",
    
    # Lists
    "\\enumerate", "\\itemize", "\\item", "\\description",
    
    # Footnotes
    "\\footnote", "\\footnotemark", "\\footnotetext",
    
    # Floats and captions
    # "\\figure", "\\table", "\\caption", "\\centering",
    
    # Page formatting
    "\\newpage", "\\clearpage", "\\cleardoublepage", "\\pagebreak", "\\newline", "\\linebreak",
    
    # Definitions and counters
    "\\newcommand", "\\renewcommand", "\\def", "\\let", "\\setcounter", "\\addtocounter", "\\stepcounter", "\\refstepcounter",
    
    # Index and glossary
    "\\index", "\\glossary", "\\printindex", "\\printglossary",
    
    # Hyperlinks (if using hyperref package)
    "\\href", "\\url", "\\hypersetup", "\\hyperlink", "\\hypertarget",
    
    # Spacing
    "\\hspace", "\\vspace", "\\phantom", "\\vphantom", "\\hphantom",
    
    # Alignment environments
    "\\center", "\\flushleft", "\\flushright",
    
    # Verbatim and code
    "\\verb", "\\verbatim", "\\lstlisting",
    
    # Theorems and proofs
    "\\theorem", "\\lemma", "\\proof", "\\corollary",
    
    # Cross-referencing
    "\\cref", "\\Cref", "\\vref", "\\Vref",  # from cleveref package
    
    # Table of contents related
    "\\tableofcontents", "\\listoffigures", "\\listoftables",
    
    # Appendix
    "\\appendix",
    
    # Author and title information
    "\\title", "\\author", "\\date", "\\maketitle",
    
    # Abstract
    "\\abstract", "\\keywords",
    
    # Including external files
    "\\input", "\\include",
    
    # Page numbering
    "\\pagenumbering", "\\thepage",
    
    # Margin adjustments
    "\\marginpar", "\\marginparsep", "\\marginparwidth",
    
    # Columns
    "\\twocolumn", "\\onecolumn", "\\multicolumn",
]


# %% ../../../nbs/07_machine_learning_15.tokenize.def_and_notat_token_classification.ipynb 95
def _str_contains_latex_command_to_avoid(text):
    """
    Helper function to `_consolidate_token_preds`
    """
    # Check if any command from the list is in the text
    for command in latex_commands_to_avoid:
        if command.lower() in text:
            return True
    return False

# %% ../../../nbs/07_machine_learning_15.tokenize.def_and_notat_token_classification.ipynb 97
def _consolidate_token_preds(
        main_text: str,
        tag_data: list[HTMLTagWithIndices]
        ) -> list[HTMLTagWithIndices]:
    """
    Since the model's predictions can yield some odd results
    (e.g. notations not being marked for an entire LaTeX string
    $<span notation="">$S_k := ...</span>$$), this function tries
    to consolidate some oddities.
    
    """
    latex_inds = latex_indices(main_text)
    extended_tag_data = _extend_tag_data_ranges(main_text, latex_inds, tag_data)
    # extended_tag_data = _extend_tag_data_ranges_to_encompass_latex(main_text, latex_inds, tag_data)
    # extended_tag_data = _extend_tag_data_ranges_to_border_spaces(main_text, extended_tag_data)
    tag_data_notats_chopped = _cutoff_notation_tag_data(main_text, extended_tag_data)
    # Go through the extended tag data to throw out overlapping ones.
    ultimate_tag_data: list[HTMLTagWithIndices] = []
    for tag_point in tag_data_notats_chopped:
        if (_no_overlap_with_previous_tag_data(ultimate_tag_data, tag_point)
                and not _str_contains_latex_command_to_avoid(tag_point.tag.text)):
            ultimate_tag_data.append(tag_point)
    return ultimate_tag_data


def _extend_tag_data_ranges(
        main_text: str,
        latex_inds: list[tuple[int, int]],
        tag_data: list[HTMLTagWithIndices],
        ) -> list[HTMLTagWithIndices]:
    """
    Extend tag data so that
    1. the tag data does not start or end within any latex math mode string.
    2. the tag data is immediately preceded by whitespace (or the start/end of line)
       and followed by whitespace or punctuation
    3. the tag data does not start/end in the middle of the arguments of a latex command.

    Helper function to `_consolidate_token_preds`.
    """
    # TODO: make sure that tag has balanced curly braces 
    extended_tag_data = []
    for tag_tuple in tag_data:
        tag_tuple_before_extension = tag_tuple
        while True:
            tag_tuple_after_extension = _extend_tag_data_range_for_math_mode(
                tag_tuple_before_extension, main_text, latex_inds)
            tag_tuple_after_extension = _extend_tag_data_range_to_border_space_or_punc(
                tag_tuple_after_extension, main_text)
            tag_tuple_after_extension = _extend_tag_data_ranges_to_balance_curly_braces(
                tag_tuple_after_extension, main_text)
            tag_tuple_before_extension = tag_tuple_after_extension 
            if (tag_tuple_before_extension[1] == tag_tuple_after_extension[1]
                    and tag_tuple_before_extension[2] == tag_tuple_after_extension[2]):
                break
        extended_tag_data.append(tag_tuple_after_extension)
    return extended_tag_data


def _update_tag_data(
        tag_tuple: HTMLTagWithIndices,
        main_text: str,
        new_start: int,
        new_end: int) -> HTMLTagWithIndices:
    new_text = main_text[new_start:new_end]
    tag_type = 'definition' if 'definition' in tag_tuple.tag.attrs else 'notation'
    new_tag = _make_tag(new_text, tag_type)
    return HTMLTagWithIndices(new_tag, new_start, new_end)


def _extend_tag_data_range_for_math_mode(
        tag_tuple: HTMLTagWithIndices,
        main_text: str,
        latex_inds: list[tuple[int, int]],
        ) -> HTMLTagWithIndices:
    """
    Extend tag data so that the tag data does not start or end within latex math mode string.
    """
    extended_range = [tag_tuple.start, tag_tuple.end]
    for tex_range in latex_inds:
        if not _ranges_overlap(HTMLTagWithIndices(0, tex_range[0], tex_range[1]), tag_tuple):
            continue
        extended_range = (min(extended_range[0], tex_range[0]), max(extended_range[1], tex_range[1]))
    return _update_tag_data(tag_tuple, main_text, extended_range[0], extended_range[1])


def _extend_tag_data_range_to_border_space_or_punc(
        tag_tuple: HTMLTagWithIndices,
        main_text: str,
        ) -> HTMLTagWithIndices:
    """
    Extend tag data so that the tag data borders spaces or punctuations.

    Helper function to `_consolidate_token_preds`.
    """
    combined_range = [tag_tuple.start, tag_tuple.end]
    while combined_range[0] != 0 and not main_text[combined_range[0]-1].isspace():
        combined_range[0] -= 1
    # while combined_range[1] != len(main_text) and not main_text[combined_range[1]].isspace():
    while combined_range[1] != len(main_text) and is_not_space_and_not_punc(main_text[combined_range[1]]):
        combined_range[1] += 1
    return _update_tag_data(tag_tuple, main_text, combined_range[0], combined_range[1])


def _extend_tag_data_ranges_to_balance_curly_braces(
        tag_tuple: HTMLTagWithIndices,
        main_text: str
        ) -> list[HTMLTagWithIndices]:
    """
    Extend tag data to balance curly braces
    """
    combined_range = [tag_tuple.start, tag_tuple.end]
    while not _is_balanced_braces(main_text[combined_range[0]:combined_range[1]]):
        changed = False
        while combined_range[0] != 0 and _first_curly_bracket(main_text[combined_range[0]:combined_range[1]]) == r'}':
            combined_range[0] -= 1
            changed = True
        while combined_range[1] != len(main_text) and _last_curly_bracket(main_text[combined_range[0]:combined_range[1]]) == r'{':
            combined_range[1] += 1
            changed = True
        if not changed:
            break
    return _update_tag_data(tag_tuple, main_text, combined_range[0], combined_range[1])


def _cutoff_notation_tag_data(
        main_text: str,
        tag_data: list[HTMLTagWithIndices],
        ) -> list[HTMLTagWithIndices]:
    """
    Helper function to `_consolidate_token_preds`.

    Guarantees that a notation tag is a pure math mode latex string
    by cutting only the pure math mode string
    that occurs within it. Assumes that `_extend_tag_data_ranges_to_encompass_latex`
    works as intended.
    """
    cutout_notation_tag_data: list[HTMLTagWithIndices] = []
    for tag, start, end in tag_data:
        if not 'notation' in tag.attrs:
            cutout_notation_tag_data.append(HTMLTagWithIndices(tag, start, end))
            continue
        tag_text = main_text[start:end]
        tex_inds_in_tagged = latex_indices(tag_text)
        for sub_start, sub_end in tex_inds_in_tagged:
            tex_str = main_text[start+sub_start:start+sub_end]
            cutout_tag = _make_tag(tex_str, 'notation')
            cutout_notation_tag_data.append(
                HTMLTagWithIndices(cutout_tag, start+sub_start, start+sub_end))
    return cutout_notation_tag_data



def _no_overlap_with_previous_tag_data(
        ultimate_tag_data: list[HTMLTagWithIndices],
        current_tag_data: HTMLTagWithIndices  # Current tag data
        ) -> bool:
    """
    Return `True`, if the `current_tag_data` does not overlap with
    any tag data that will be ultimately added. 

    Helper function for `_consolidate_token_preds`.
    """
    for prev in reversed(ultimate_tag_data):
        if _ranges_overlap(prev, current_tag_data):
            return False
    return True
    

# %% ../../../nbs/07_machine_learning_15.tokenize.def_and_notat_token_classification.ipynb 99
def _html_tags_from_token_preds(
        main_text: str,
        token_preds: list[dict[str]], # An output of `pipeline(text)`; Each dict likely contains keys such as `'entity'`, `'score'`, `'index'`, `'word'`, `'start'`, and `'end'`, depending on the model used.
        excessive_space_threshold: int,
        note: Optional[VaultNote] = None,
        ) -> list[HTMLTagWithIndices]:  # Tag element, start, end, where main_text[start:end] needs to be replaced by the tag element.
        # ) -> list[tuple[bs4.element.Tag, int, int]]:  # Tag element, start, end, where main_text[start:end] needs to be replaced by the tag element.
    """
    Return HTML tags for definition and notation classification.

    Helper function to `auto_mark_def_and_notats`.
    """
    parts: list[list[dict[str]]] = _divide_token_preds_into_parts(
        token_preds, excessive_space_threshold, note)
    return [_html_tag_data_from_part(main_text, part) for part in parts]


# %% ../../../nbs/07_machine_learning_15.tokenize.def_and_notat_token_classification.ipynb 101
def _collate_html_tags(
        tag_data_1: list[HTMLTagWithIndices],
        tag_data_2: list[HTMLTagWithIndices],
    ) -> list[tuple[bs4.element.Tag], int, int]:
    """
    Collates the lists of HTML tags and the indices within a certain text
    (which is not-needed for this function and hence not included)
    that the HTML tags need to replace.

    If there are entries in `tag_data_1` and `tag_data_2` with overlapping
    ranges, then the entry from `tag_data_1` is prioritized and the entry
    from `tag_data_2` is discarded.

    Helper function to `auto_mark_def_and_notats`
    """
    collated_list: list[HTMLTagWithIndices] = []
    i, j = 0, 0
    while i < len(tag_data_1) and j < len(tag_data_2):
        current_1 = tag_data_1[i]
        current_2 = tag_data_2[j]
        if _ranges_overlap(current_1, current_2): # Ignore current_2
            j += 1
            continue
        if current_1[1] > current_2[1]:
            collated_list.append(current_2)
            j += 1
        else:
            collated_list.append(current_1)
            i += 1
    while i < len(tag_data_1):
        collated_list.append(tag_data_1[i])
        i += 1
    while j < len(tag_data_2):
        collated_list.append(tag_data_2[j])
        j += 1
    return collated_list




# %% ../../../nbs/07_machine_learning_15.tokenize.def_and_notat_token_classification.ipynb 103
def _add_nice_boxing_attrs_to_def_and_notat_tags(
        html_tag_data: list[HTMLTagWithIndices]
        ) -> list[HTMLTagWithIndices]:
    """
    Add HTML tag attributes to draw boxes around notation data

    Helper function to `auto_mark_def_and_notats`.
    """
    listy: list[HTMLTagWithIndices] = []
    for tag, start, end in html_tag_data:
        if ('notation' in tag.attrs or 'definition' in tag.attrs) and 'style' not in tag.attrs:
            tag.attrs['style'] = "border-width:1px;border-style:solid;padding:3px"
        listy.append(HTMLTagWithIndices(tag, start, end)) 
    return listy



# %% ../../../nbs/07_machine_learning_15.tokenize.def_and_notat_token_classification.ipynb 105
def def_and_notat_preds_by_model(
        text: str,  
        pipeline # The pipeline object created using the token classification model and its tokenizer
        ) -> list[HTMLTagWithIndices]: # HTMLTAgWithIndices consists of an HTML tag carrying the data of the prediction and ints marking where in `text` the definition or notation is at.
    """
    Predict where definitions and notations occur in `text`

    This function uses some of the same helper functions as
    `auto_mark_def_and_notats`, but does not raise warning messages as
    in `auto_mark_def_and_notats`.
    """
    tag_data = _html_tags_from_token_preds(text, pipeline(text), 2, None)
    return tag_data

# %% ../../../nbs/07_machine_learning_15.tokenize.def_and_notat_token_classification.ipynb 107
# def _process_mf(
#         mf: MarkdownFile) -> None:
#     """
#     Merge display math mode as needed

#     Helper function to `auto_mark_def_and_notats`
#     """
#     # mf.merge_display_math_mode()
#     # mf.merge_display_math_mode_into_preceding_text()
#     mf.

# %% ../../../nbs/07_machine_learning_15.tokenize.def_and_notat_token_classification.ipynb 108
def _get_main_text_lines(
        mf: MarkdownFile) -> tuple[int, int]:
    """Helper function to `auto_mark_def_and_notats`"""
    tuppy = mf.metadata_lines()
    if tuppy is not None:
        first_non_metadata_line = tuppy[1] + 1
    else:
        first_non_metadata_line = 0 
    see_also_line = mf.get_line_number_of_heading('See Also')
    return first_non_metadata_line, see_also_line



# %% ../../../nbs/07_machine_learning_15.tokenize.def_and_notat_token_classification.ipynb 109
def _append_to_pieces_start_and_end(
        pieces_start_and_end: list[tuple[int, int]],
        start_chunk: tuple[str, int, int],
        end_chunk: tuple[str, int, int]
        ) -> None:
    """Helper function to `_find_places_to_divide_from_chunks`"""
    start_char_index = start_chunk[1]
    end_char_index = end_chunk[1] + len(end_chunk[0])
    pieces_start_and_end.append([start_char_index, end_char_index])

# %% ../../../nbs/07_machine_learning_15.tokenize.def_and_notat_token_classification.ipynb 110
def _find_places_to_divide_from_chunks(
        chunks: list[tuple[str, int, int]], # The str is a chunk of text, the first int is the index in `main_text` that the chunk starts at, and the second int is the approximate token length of the text. Appending all the chunks of text as they are should result back in the original text.
        pipeline: pipelines.token_classification.TokenClassificationPipeline, # The token classification pipeline that is used to predict whether tokens are part of definitions or notations introduced in the text. Here, the tokenizer of this pipeline is used to estimate how many tokens a piece of subtext will have.
        ) -> list[tuple[int, int]]: # Each tuple is a start and end range for pieces of `main_text` to be considered for predictions
    """Identify appropriate indices in `main_text` where (overlapping)
    pieces in `main_text` should start/end for predictions with `pipeline`.
    
    Helper function to `_divide_main_text`.

    We describe how this function is implemented: starting at the first chunk
    (chunks are non-overlapping), start to consider consecutive chunks to
    make up a piece. So maybe we have chunks

        A B C D E F ....

    We build a piece chunk-by-chunk, considering the total token length of the
    built sub-piece along the way. The first chunk within a sub-piece 
    that makes the sub-piece of token-length greater than half the max
    token length with respect to `pipeline.tokenizer` will become the start of the
    next piece, unless the very first chunk in the piece is already longer than half the max
    token length with respect to the tokenizer (this is to ensure that the
    piece-building process does not keep starting at the same chunk).
    Moreover, a piece will stop building as soon as its token-length exceeds
    the max length of the tokenizer.

    For instance, maybe the max length (`tokenizer.model_max_length`)
    for the tokenizer is 512, and the chunks
    are of the following length:

        A   B    C  D   E    F     ...
        76  130  70 13  150  140   ...

    We first build the piece starting at A:

        A
        76

    We continue building the piece by "appending" B:

        A   B
        76  130

    Once we append C as well, the piece's length is now 276 and hence over half of 512,
    so the next piece will start at C: 

        A   B    C
        76  130  70

    Subsequently, we continue building the piece. Only once F is appended does the 
    length of the entire piece exceed 512 (the length is 579):

        A   B    C  D   E    F
        76  130  70 13  150  140
    
    And then we begin building the next piece from C.

    Also, consider an example where the first chunk's length exceeds half the max length
    of the tokenizer:

        A   B    C   ...
        300 200  100 ...

    Here, the first piece will consist of the chunks A, B, and C because
    the length of the piece exceeds the max length of 512 only after appending C.
    To guarantee that the next piece does not start with the chunk A again, B is 
    used as the first chunk in the next piece:

        B    C   ...
        200  100 ...

    If any chunk's token length exceeds the tokenizer's max_model_length, then
    the pipeline/model can only predict on the starting tokens in the chunk. 
    As such, the chunks must not be "too long" for best results on the model's predictions.
    """
    tokenizer = pipeline.tokenizer
    start_chunk_index, next_piece_start_chunk_index = 0, 0
    current_piece_token_len = 0
    pieces_start_and_end = []
    i = 0
    while i < len(chunks):
        chunk = chunks[i]
        current_piece_token_len = current_piece_token_len + chunk[2]
        if (current_piece_token_len > tokenizer.model_max_length / 2
                and start_chunk_index == next_piece_start_chunk_index):
            # Mark where the next piece should start
            next_piece_start_chunk_index = i if start_chunk_index != i else i+1
        if (current_piece_token_len > tokenizer.model_max_length):
            _append_to_pieces_start_and_end(
                pieces_start_and_end, chunks[start_chunk_index], chunk)
            i, start_chunk_index = (
                next_piece_start_chunk_index, next_piece_start_chunk_index)
            current_piece_token_len = 0
            continue
        i += 1
    # Add the last chunk at the end
    _append_to_pieces_start_and_end(
        pieces_start_and_end, chunks[start_chunk_index], chunks[-1])
    return pieces_start_and_end



# %% ../../../nbs/07_machine_learning_15.tokenize.def_and_notat_token_classification.ipynb 111
def _divide_main_text(
        main_text: str,
        pipeline: pipelines.token_classification.TokenClassificationPipeline, # The token classification pipeline that is used to predict whether tokens are part of definitions or notations introduced in the text. Here, the tokenizer of this pipeline is used to estimate how many tokens a piece of subtext will have.
        # ) -> list[tuple[str, int, int]]:  # The str is a chunk of text, the first int is the index in `main_text` that the chunk starts at, and the second int is the approximate token length of the text. Appending all the chunks of text as they are should result back in the original text.
        ) -> list[tuple[int, int]]:  # Each tuple is a start and end range for pieces of `main_text` to be considered for predictions
    """Divides `main_text` so that predictions can be made on smaller chunks of text.
    
    Assumes that dividing `main_text` along newline characters `\n` will result in
    pieces that are "not too long".

    Helper function to `_format_main_text_and_add_html_tag_data`.


    """
    main_text.split('\n')
    tokenizer = pipeline.tokenizer
    newline_indices = [i for i, char in enumerate(main_text) if char == '\n']
    newline_indices.insert(0, 0)
    chunks = []  # list[tuple[str, int, int]]  # The str is a chunk of text, the first int is the index in `main_text` that the chunk starts at, and the second int is the approximate token length of the text. Appending all the chunks of text as they are should result back in the original text.
    for start, end in pairwise(newline_indices):
        chunk = main_text[start:end]
        chunks.append((chunk, start, len(tokenizer(chunk)['input_ids'])))
    last_chunk = main_text[newline_indices[-1]:]
    chunks.append((last_chunk, newline_indices[-1], len(tokenizer(last_chunk)['input_ids'])))
    return _find_places_to_divide_from_chunks(chunks, pipeline)

# %% ../../../nbs/07_machine_learning_15.tokenize.def_and_notat_token_classification.ipynb 112
def _get_token_preds_by_dividing_main_text(
        main_text: str,
        pipeline: pipelines.token_classification.TokenClassificationPipeline, # The token classification pipeline that is used to predict whether tokens are part of definitions or notations introduced in the text. Here, the tokenizer of this pipeline is used to estimate how many tokens a piece of subtext will have.
        excessive_space_threshold: int, 
        note: Optional[VaultNote] = None,
        # ) -> list[tuple[bs4.element.Tag, int, int]]:  # Tag element, start, end, where main_text[start:end] needs to be replaced by the tag element.
        ) -> list[HTMLTagWithIndices]:  # Tag element, start, end, where main_text[start:end] needs to be replaced by the tag element.
    """
    Divide the `main_text` into not-too-long pieces to return HTML tag predictions

    Helper function for `_format_main_text_and_add_html_tag_data`.
    """
    pieces_start_and_end = _divide_main_text(main_text, pipeline)
    cumulative_html_tags_in_main = []
    for start_of_piece, end_of_piece in pieces_start_and_end:
        # text = main_text[start_of_piece:end_of_piece]
        text = main_text[start_of_piece:]
        html_tags_in_piece: list[HTMLTagWithIndices] = _html_tags_from_token_preds(
            text, pipeline(text), excessive_space_threshold, note)
        html_tags_in_piece = _consolidate_token_preds(
            text, html_tags_in_piece)
        # start and end indices need to be re-adjusted with respect to their places in `main_text`
        html_tags_for_piece_in_main_text: list[HTMLTagWithIndices] = [
            HTMLTagWithIndices(tag, start_of_piece + start, start_of_piece + end)
            for tag, start, end in html_tags_in_piece]
        cumulative_html_tags_in_main = _collate_html_tags(
            cumulative_html_tags_in_main, html_tags_for_piece_in_main_text)
    return cumulative_html_tags_in_main


# %% ../../../nbs/07_machine_learning_15.tokenize.def_and_notat_token_classification.ipynb 113
def get_def_and_notat_predictions(
        main_text: str,
        pipeline: pipelines.token_classification.TokenClassificationPipeline,
        excessive_space_threshold: int,
        note: Optional[VaultNote] = None
        ) -> list[HTMLTagWithIndices]:
    """
    Identifies definitions and notations in the text using the ML pipeline.

    Returns a list of `HTMLTagWithIndices` containing the predicted tags 
    and their start/end indices in `main_text`.
    """
    # 1. Divide text (if needed) and get raw token predictions
    html_tags_to_add = _get_token_preds_by_dividing_main_text(
        main_text, pipeline, excessive_space_threshold, note)
    
    # 2. Return the structured data directly
    # Note: _get_token_preds_by_dividing_main_text already returns 
    # list[HTMLTagWithIndices] after calling `consolidate_token_preds`.
    return html_tags_to_add


# %% ../../../nbs/07_machine_learning_15.tokenize.def_and_notat_token_classification.ipynb 114
def mark_def_and_notat_predictions(
        main_text: str, # The original text.
        predictions: list[HTMLTagWithIndices], # The list of predictions (ranges and metadata) returned by `get_def_and_notat_predictions`. Assumes these are sorted and non-overlapping (which `get_def_and_notat_predictions` ensures).
        formatter: Callable[[str, HTMLTagWithIndices], str] # A function that takes the *extracted substring* (the text being marked) and the prediction object, and returns the *formatted string* to replace it with.
        ) -> str: # The modified text with predictions marked.
    """
    Applies formatting to the `main_text` based on the definition and notation predictions.
    """
    # Iterate backwards so that index changes don't affect subsequent replacements
    # (HTMLTagWithIndices are typically sorted by start index, so reversing is safe)
    sorted_preds = sorted(predictions, key=lambda x: x.start, reverse=True)
    
    formatted_text_list = list(main_text)
    
    for pred in sorted_preds:
        start, end = pred.start, pred.end
        original_substring = main_text[start:end]
        
        # Apply the user-defined formatting rule
        replacement_text = formatter(original_substring, pred)
        
        # Replace the slice in the text
        # (Using list slicing for efficiency vs repeated string concatenation)
        formatted_text_list[start:end] = list(replacement_text)
        
    return "".join(formatted_text_list)


# %% ../../../nbs/07_machine_learning_15.tokenize.def_and_notat_token_classification.ipynb 118
def predict_and_mark_def_and_notats(
        main_text: str, # The text to run predictions on and format.
        pipeline: pipelines.token_classification.TokenClassificationPipeline, # The token classification pipeline.
        # note: VaultNote, # The note associated with the text (used for debugging/logging).
        formatter: Callable[[str, HTMLTagWithIndices], str], # The function that formats the text based on predictions.
        excessive_space_threshold: int # Threshold for detecting excessive spacing in predictions.
        ) -> str: # The formatted text.
    """
    Runs definition and notation detection on `main_text` and applies the `formatter`.
    """
    predictions = get_def_and_notat_predictions(
        main_text, pipeline, excessive_space_threshold)
    return mark_def_and_notat_predictions(main_text, predictions, formatter)


# %% ../../../nbs/07_machine_learning_15.tokenize.def_and_notat_token_classification.ipynb 120
def _format_main_text_and_add_html_tag_data(
        note: VaultNote,
        pipeline: pipelines.token_classification.TokenClassificationPipeline, # The token classification pipeline that is used to predict whether tokens are part of definitions or notations introduced in the text.
        add_boxing_attr_to_existing_def_and_notat_markings: bool,
        excessive_space_threshold: int,
        main_text: str,  # The main text to format and to add HTML tag data to
        ) -> str:
    """Helper function to `auto_mark_def_and_notats`"""
    # 1. Pre-processing
    main_text = add_space_to_lt_symbols_without_space(main_text)
    main_text = convert_double_asterisks_to_html_tags(main_text)
    main_text, existing_html_tag_data = remove_html_tags_in_text(main_text)

    # 2. Handle existing tag styling
    if add_boxing_attr_to_existing_def_and_notat_markings:
        existing_html_tag_data = _add_nice_boxing_attrs_to_def_and_notat_tags(
            existing_html_tag_data)

    # 3. Get New Predictions
    new_tag_data = get_def_and_notat_predictions(
        main_text, pipeline, excessive_space_threshold, note)

    # 4. Merge old and new tags
    all_tags_to_add = _collate_html_tags(
        existing_html_tag_data, new_tag_data)

    # 5. Apply to text
    return add_HTML_tag_data_to_raw_text(main_text, all_tags_to_add)
    # html_tags_to_add = _get_token_preds_by_dividing_main_text(
    #     main_text, pipeline, note, excessive_space_threshold)

    # html_tags_to_add_back = _collate_html_tags(
    #     existing_html_tag_data, html_tags_to_add)
    # return add_HTML_tag_data_to_raw_text(main_text, html_tags_to_add_back)



# %% ../../../nbs/07_machine_learning_15.tokenize.def_and_notat_token_classification.ipynb 122
def _write_text_with_html_tag_preds_to_note(
        note: VaultNote,
        mf: MarkdownFile,
        main_text: str,
        first_non_metadata_line: int,
        see_also_line: int
        ) -> None:
    """
    Final step of the marking process; the new contents of the note are written.

    Helper function to `auto_mark_def_and_notats`
    """
    mf.remove_lines(first_non_metadata_line, see_also_line)
    mf.insert_line(first_non_metadata_line,
                   {'type': MarkdownLineEnum.DEFAULT, 'line': main_text})
    mf.add_tags('_auto/def_and_notat_identified')
    mf.write(note)

# %% ../../../nbs/07_machine_learning_15.tokenize.def_and_notat_token_classification.ipynb 123
def auto_mark_def_and_notats(
        note: VaultNote,  # The standard information note in which to find the definitions and notations.
        pipeline: pipelines.token_classification.TokenClassificationPipeline, # The token classification pipeline that is used to predict whether tokens are part of definitions or notations introduced in the text.
        # remove_existing_def_and_notat_markings: bool = False,  # If `True`, remove definition and notation markings (both via surrounding by double asterisks `**` as per the legacy method and via HTML tags)
        excessive_space_threshold: int = 2,
        add_boxing_attr_to_existing_def_and_notat_markings: bool = True # If `True`, then nice attributes are added to the existing notation HTML tags, if not already present.
    ) -> None:
    """
    Predict and mark where definitions and notation occur in a note using
    a token classification ML model.

    Assumes that the note is a standard information note that does not
    have a lot of "user modifications", such as footnotes, links,
    and HTML tags. If
    there are many modifications, then these might be deleted.

    Assumes that the paragraphs in the text of the note are "not too long".
    Currently, this means that the paragraphs in the number of tokens
    in the text of the note should (roughly) not exceed 
    `pipeline.tokenizer.model_max_length`.

    Existing markings for definition and notation data (i.e. by
    surrounding with double asterisks or by HTML tags) are preserved
    (and turned into HTML tags), unless the markings overlap with 
    predictions, in which case the original is preserved (and still
    turned into an HTML tag if possible)

    Since the model can make "invalid" predictions (mostly those which
    start or end within a LaTeX math mode str), the actual markings
    are not necessarily direct translates from the model's predictions.
    See the helper function `_consolidate_token_preds` for more details
    on how this is implemented.
    
    **Raises**
    Warning messages (`UserWarning`) are printed in the following situations:

    - There are two consecutive tokens within the `pipeline`'s predictions
      of different entity types (e.g. one is predicted to belong within a
      definition and the other within a notation), but the latter token's
      predicted `'entity'` more specifically begins with `'I-'` (i.e. is
      `'I-definition'` or `'I-notation'`) as opposed to `'B-'`.
        - `note`'s name, and path are included in the warning message in
          this case.
    - There are two consecutive tokens within the `pipeline`'s predictions
      which the pipeline predicts to belong to the same entity, and yet
      there is excessive space (specified by `excessive_space_threshold`)
      between the end of the first token and the start of the second.

    """
    mf = MarkdownFile.from_vault_note(note)
    mf.cleanup_formatting()
    # _process_mf(mf)
    first_non_metadata_line, see_also_line = _get_main_text_lines(mf)
     
    main_text = mf.text_of_lines(first_non_metadata_line, see_also_line)
    main_text = _format_main_text_and_add_html_tag_data(
        note, pipeline, add_boxing_attr_to_existing_def_and_notat_markings,
        excessive_space_threshold, main_text)
    _write_text_with_html_tag_preds_to_note(
        note, mf, main_text, first_non_metadata_line, see_also_line)



# %% ../../../nbs/07_machine_learning_15.tokenize.def_and_notat_token_classification.ipynb 131
def latex_highlight_formatter(text: str, pred: HTMLTagWithIndices) -> str:
    r"""
    Formats definitions and notations with LaTeX highlighting commands.
    
    Rules:
    - Definitions: \hldef{text}
    - Notations:
        - $...$ -> \hl{$...$}
        - $$...$$ -> $$\hlin{...}$$
        - \begin{align*}...\end{align*} -> \hlalign{\begin{align*}...\end{align*}}
    """
    is_definition = 'definition' in pred.tag.attrs
    
    # --- Case 1: Definition ---
    if is_definition:
        return f"\\hldef{{{text}}}"
    
    # --- Case 2: Notation ---
    # We need to detect the type of math environment in `text`
    
    stripped_text = text.strip()
    
    # Subcase 2a: Display Math with $$...$$
    if stripped_text.startswith('$$') and stripped_text.endswith('$$'):
        # Extract content inside $$...$$
        # Content length is len(text) - 4 (for the two $$ pairs)
        # We need to handle potential whitespace, so we use indices based on the strip
        inner_content = stripped_text[2:-2] 
        return f"$$\\hlin{{{inner_content}}}$$"
    
    # Subcase 2b: Align Environment
    # Checks for \begin{align*} or \begin{align}
    if r'\begin{align' in text:
        return f"\\hlalign{{{text}}}"
    
    # Subcase 2c: Inline Math $...$ (Default for notations)
    # Note: We wrap the *entire* text (including $) in \hl{...}
    # User requirement: "if $blah$, then \hl{$blah$}"
    return f"\\hl{{{text}}}"


# %% ../../../nbs/07_machine_learning_15.tokenize.def_and_notat_token_classification.ipynb 136
DEF_NOTAT_VERIFY_SYSTEM_PROMPT = r"""
You are an expert auditor of mathematical texts. Your task is to validate semantic HTML markings (attributes: "definition" or "notation") within an excerpt. You must determine if the current markings correctly identify **newly introduced** terms while ignoring **contextual** objects.

### I. The Core Binary Classification
For every mathematical object or term, apply this binary test.

**1. THE TARGET (Must be Marked)**
A term is a Target if and only if the excerpt **establishes its meaning for the first time** intended for use beyond the immediate sentence.
*   **New Constructions:** "For any field $K$, let <span notation>$G_K$</span> denote..." (Mark $G_K$).
*   **Formal Definitions:** "We say a sheaf is <b definition>flasque</b> if..." (Mark "flasque").
*   **Explicit Assignments:** "We define the <b definition>L-series</b> <span notation>$L(s, \chi)$</span> as..." (Mark "L-series" and $L(s, \chi)$).
*   **Self-Contained "Recalls":** If the text says "Recall that X is called Y if [Definition]", and the excerpt contains the actual definition criteria, Mark Y. The author is establishing the definition for this text.
    *   *Correct:* "Recall that $X$ is <b definition>totally disconnected</b> if connected components are points."
    *   *Incorrect (Don't mark):* "Recall the properties of totally disconnected spaces."

**2. THE CONTEXT (Must NOT be Marked)**
Everything else is Context. This includes:
*   **Generic Variables:** "Let $X$ be a scheme..." ($X$ is a generic placeholder).
*   **Specific Instances/Applications:** "Let $G = \text{Gal}(L/K)$..." ($G$ is just a shorthand for this specific proof).
*   **Reminders/Recalls:** "Recall that $H^i$ denotes cohomology..." (The definition exists outside this excerpt).
*   **Process Variables:** "By induction on $n$...", "It suffices to treat...", "Let $f: X \to Y$ be a morphism..."

### II. Auditing Rules

1.  **The "Defined Here" Rule:** Only mark objects if the text explicitly links the symbol/term to its formal name or construction *in this specific excerpt*. If the text implies the reader should already know it (e.g., "Consider the trace defined above"), do not mark it.
2.  **Construction vs. Instance:**
    *   "Let $f$ be the map defined in Eq 1" $\to$ **Context** (Reference to past).
    *   "Let $f$ denote the map..." $\to$ **Target** (Establishing new notation).
3.  **OCR Robustness:** Treat malformed LaTeX (e.g., missing `$`) as valid text. If `L(s)` is a Target but lacks delimiters, it still requires a mark.

### III. Reference Examples

*   **Correct Definition:** "We call a functor $F$ <b definition>representable</b> if..."
    *   *Reason:* Defines the property "representable". $F$ is Context.
*   **Correct Notation:** "Let <span notation>$\mathbb{A}^n</span>$ denote affine space."
    *   *Reason:* Explicit assignment of global notation.
*   **False Positive (Do not mark):** "Let $C$ be a category. If $C$ has limits..."
    *   *Reason:* $C$ is a generic variable used to set the stage.
*   **False Positive (Do not mark):** "The fiber product $X \times_Y Z$ (see Chapter 1)..."
    *   *Reason:* Reference to a prior definition.

### IV. Output Format
Return a JSON object with:
1. has_incorrect_markings: Set to true if the text contains a <b definition> or <span notation> that violates the Context rules (i.e., a False Positive).
2. has_missing_markings: Set to true if the text contains a term that meets the Target rules but has no HTML tags (i.e., a False Negative).
3. "reasoning": string (Strictly limited to 30 words).

### V. Constraints on Reasoning
- DO NOT provide an introductory "Step-by-step" or "Let's analyze" paragraph.
- DO NOT repeat the rules or definitions of Target/Context.
- DO NOT provide a preamble. Start immediately with the analysis.
- DO NOT quote the excerpt.
- FORMAT: Use a simple list: [Term]: [Target/Context] - [3-word reason].
- If no errors are found, the reasoning should simply be "All markings conform to rules."

Output Requirement: You must start the reasoning string with the character [ and follow the list format. Do not use conversational fillers.
"""


# %% ../../../nbs/07_machine_learning_15.tokenize.def_and_notat_token_classification.ipynb 137
DEF_NOTAT_VERIFY_USER_PROMPT = r"""
Audit the following excerpt of mathematical text for definition and notation marking errors. Return the result in the specified JSON format.

"""


# %% ../../../nbs/07_machine_learning_15.tokenize.def_and_notat_token_classification.ipynb 138
class AuditResult(BaseModel):
    reasoning: str
    has_incorrect_markings: bool
    has_missing_markings: bool

class AuditVoteResult(TypedDict, total=True):
    should_remove: bool
    should_add: bool
    incorrect_tally: int  # Added tally for incorrect markings
    missing_tally: int    # Added tally for missing markings
    error_tally: int    # New: specifically tracks failed LLM calls
    total_votes: int      # Useful for context/percentage
    stats: str

# %% ../../../nbs/07_machine_learning_15.tokenize.def_and_notat_token_classification.ipynb 139
INITIAL_ERROR_MESSAGE = "ERROR during LLM call"
def salvage_audit_result(raw_content: str, error_msg: str) -> AuditResult:
    """
    Attempts to extract boolean flags from a truncated or malformed JSON string.
    Returns a 'Success' AuditResult if flags are found, otherwise returns an Error result.
    """
    if raw_content:
        if len(raw_content.strip()) < 10:
             return AuditResult(reasoning=f"{INITIAL_ERROR_MESSAGE}: Empty response", 
                               has_incorrect_markings=False, has_missing_markings=False)
        # Normalize: remove whitespace and newlines for robust string matching
        clean = raw_content.lower().replace(" ", "").replace("\n", "").replace("\\n", "")
        
        def extract_bool(key):
            if f'"{key}":true' in clean: return True
            if f'"{key}":false' in clean: return False
            return None

        inc = extract_bool("has_incorrect_markings")
        mis = extract_bool("has_missing_markings")

        # If we found BOTH flags, we can salvage the vote even if JSON is broken
        if inc is not None and mis is not None:
            # <--- CHANGE: Extract only the text after the flags if possible
            # This makes the "Reasonings" section of your final report readable
            display_text = raw_content.split('"reasoning":')[-1].strip(' "}').replace('\\n', '\n')
            return AuditResult(
                reasoning=f"TRUNCATED SALVAGE: {display_text[:200]}...",
                has_incorrect_markings=inc,
                has_missing_markings=mis
            )
            # return AuditResult(
            #     # reasoning=f"TRUNCATED SALVAGE: {raw_content[:150]}...",
            #     # has_incorrect_markings=inc,
            #     # has_missing_markings=mis
            # )
            
    # If flags are missing, return the error string to trigger 'ignore' in voting logic
    return AuditResult(
        reasoning=f"{INITIAL_ERROR_MESSAGE}: {error_msg}",
        has_incorrect_markings=False,
        has_missing_markings=False
    )

# %% ../../../nbs/07_machine_learning_15.tokenize.def_and_notat_token_classification.ipynb 143
def run_strict_audit(
        client: OpenAI,
        text_to_check: str,
        system_prompt: str = DEF_NOTAT_VERIFY_SYSTEM_PROMPT,
        user_prompt: str = DEF_NOTAT_VERIFY_USER_PROMPT,
        temperature: float = 0.3,
        verbose: bool = False,
        max_tokens=1024, # Max tokens for 
        ) -> AuditResult:
    """
    Make a client (running on an LLM) audit the definition and notation markings
    in `text_to_check` to see if any definitions/notations should have been marked
    but were not (false negatives) or were marked but should not have been
    (false positives).

    Sends text to the model and forces a JSON response matching AuditResult.
    """
    raw_content = ""
    try:
        response = client.chat.completions.create(
            model="local-model-name",  # Name often ignored by local servers, but required
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"{user_prompt}:\n\n{text_to_check}"},
            ],
            temperature=temperature,  # Low temperature = more deterministic structure
            max_tokens=1024,
            # --- THE FIX: NEW STRUCTURED OUTPUT FORMAT ---
            # response_format={
            #     "type": "json_schema",
            #     "json_schema": {
            #         "name": "audit_result", # Identifying name for the schema
            #         "schema": AuditResult.model_json_schema(), # Auto-generates the schema
            #         "strict": True # Forces the model to adhere exactly
            #     }
            # }
            response_format={
                "type": "json_schema",
                "json_schema": {
                    "name": "audit_result",
                    "strict": True, # Setting this False may allow the model to be longer with its reasoning at the cost of potentially breaking json 
                    # "strict": False,
                    "schema": {
                        "type": "object",
                        "properties": {
                            "has_incorrect_markings": {"type": "boolean"},
                            "has_missing_markings": {"type": "boolean"},
                            "reasoning": {"type": "string"},
                        },
                        "required": ["reasoning", "has_incorrect_markings", "has_missing_markings"],
                        "additionalProperties": False
                    }
                }
            }
        )

        # 1. Get the raw string (The server GUARANTEES this is valid JSON matching schema)
        raw_content = response.choices[0].message.content
        
        # 2. Parse directly into Pydantic
        result = AuditResult.model_validate_json(raw_content)
        
        return result

    # except Exception as e:
    #     # Attempt to extract what was generated before the crash
    #     # raw_content might be partially populated depending on your client's state
    #     partial_reasoning = raw_content if 'raw_content' in locals() else str(e)
    #     if verbose:
    #         print(f"Audit Failed: {e}")
    #     return AuditResult(
    #         reasoning=f"{INITIAL_ERROR_MESSAGE}: {partial_reasoning}",
    #         has_incorrect_markings=False,
    #         has_missing_markings=False
    #     )

    except Exception as e:
        # if verbose: print(f"Audit Failed: {e}")
        # Invoke the factored-out salvage logic
        return salvage_audit_result(raw_content, str(e))
    # except Exception as e:
    #     if 'raw_content' in locals() and raw_content:
    #         # Normalize content for easier searching
    #         clean_content = raw_content.lower().replace(" ", "")
            
    #         # Helper to find boolean values in truncated JSON
    #         def extract_bool(key):
    #             if f'"{key}":true' in clean_content: return True
    #             if f'"{key}":false' in clean_content: return False
    #             return None # Not generated yet

    #         salvaged_inc = extract_bool("has_incorrect_markings")
    #         salvaged_mis = extract_bool("has_missing_markings")

    #         # Only count this as a "Success" if we got both booleans.
    #         # Otherwise, the reasoning is likely so short the logic hasn't started.
    #         if salvaged_inc is not None and salvaged_mis is not None:
    #             return AuditResult(
    #                 reasoning=f"TRUNCATED SALVAGE: {raw_content[:200]}...",
    #                 has_incorrect_markings=salvaged_inc,
    #                 has_missing_markings=salvaged_mis
    #             )
                
    #     # If we couldn't even get the booleans, return the error so the 
    #     # voting logic knows to ignore this attempt and try again.
    #     return AuditResult(
    #         reasoning=f"{INITIAL_ERROR_MESSAGE}: {str(e)}",
    #         has_incorrect_markings=False,
    #         has_missing_markings=False
    #     )

# %% ../../../nbs/07_machine_learning_15.tokenize.def_and_notat_token_classification.ipynb 147
ALL_AUDITS_FAILED_STRING = "All audit attempts failed due to system errors."
def run_audit_voting(
        client: OpenAI,
        text_to_check: str,
        iterations: int = 3,
        guarantee_success_count: bool = False, # New optional argument
        max_attempts: int = 10,                 # Safety break for the guaranteed mode
        verbose: bool = True,
        system_prompt: str = DEF_NOTAT_VERIFY_SYSTEM_PROMPT,
        user_prompt: str = DEF_NOTAT_VERIFY_USER_PROMPT,
        temperature: float = 0.7,
        ) -> AuditVoteResult:
    """
    Run `run_strict_audit` multiple times to let the LLM within `client`     
    vote against itself concerning whether a text has 
    false negatives or false positives for the marked definitions and notations
    """
    audits: list[AuditResult] = []
    total_attempts = 0
    successful_count = 0

    while True:
        audit = run_strict_audit(
            client, text_to_check, system_prompt, user_prompt, temperature, verbose)
        total_attempts += 1

        # Check if this specific audit was a success (didn't return the error message)
        is_error = audit.reasoning.startswith(INITIAL_ERROR_MESSAGE)
        if not is_error:
            successful_count += 1

        audits.append(audit)

        if verbose:
            status = "Error" if is_error else "Success"
            print(f"Attempt {total_attempts} ({status})")
            print(audit, '\n\n')
        # if verbose:
        #     print(f'Vote {i}')
        #     print(audit, '\n\n')
        # Termination conditions
        if guarantee_success_count:
            # Mode 2: Stop when we have enough successes or hit the safety wall
            if successful_count >= iterations or total_attempts >= max_attempts:
                break
        else:
            # Mode 1: Stop when we hit the fixed iteration count
            if total_attempts >= iterations:

                break
    # Filtering and Tallying
    failures = [a for a in audits if a.reasoning.startswith(INITIAL_ERROR_MESSAGE)]
    successes = [a for a in audits if not a.reasoning.startswith(INITIAL_ERROR_MESSAGE)]
    error_count = len(failures)
    total = len(audits)

    # We base the voting threshold only on successful audits
    # If using Mode 2, this will be exactly `iterations`.
    # If using Mode 1, this will be `iterations - error_count`.
    valid_vote_count = len(successes)
    
    if valid_vote_count == 0:
        return AuditVoteResult(
            should_remove=False, 
            should_add=False, 
            incorrect_tally=0,
            missing_tally=0,
            error_tally=error_count,
            total_votes=total,
            stats=ALL_AUDITS_FAILED_STRING
        )

    threshold = (valid_vote_count // 2) + 1
    inc_votes = sum(1 for audit in successes if audit.has_incorrect_markings)
    mis_votes = sum(1 for audit in successes if audit.has_missing_markings)

    reasonings = [f"[{i+1}] {audit.reasoning}" for i, audit in enumerate(audits)]

    truncation_count = sum(1 for a in failures if "json" in a.reasoning.lower() or "eof" in a.reasoning.lower())

    return AuditVoteResult(
        should_remove=inc_votes >= threshold,
        should_add=mis_votes >= threshold,
        incorrect_tally=inc_votes,
        missing_tally=mis_votes,
        error_tally=error_count,
        total_votes=total,
        stats=(
            f"Summary: {inc_votes} inc, {mis_votes} mis, {error_count} failed out of {total} total attempts.\n"
            f"({truncation_count} were JSON/EOF errors) out of {total} total attempts.\n"
            f"Threshold for action (based on {valid_vote_count} successes): {threshold}\n\n"
            f"Reasonings:\n" + "\n\n".join(reasonings)
        )
    )

# %% ../../../nbs/07_machine_learning_15.tokenize.def_and_notat_token_classification.ipynb 153
# def run_audit_voting_maker(
#         client: OpenAI,
#         text_to_check: str,
#         K: int = 3,              # The MAKER "Head" threshold
#         max_samples: int = 15,    # Safety cap to prevent infinite loops
#         verbose: bool = True,
#         system_prompt: str = DEF_NOTAT_VERIFY_SYSTEM_PROMPT,
#         user_prompt: str = DEF_NOTAT_VERIFY_USER_PROMPT,
#         temperature: float = 0.7,
#         ) -> AuditVoteResult:
#     """
#     Implements MAKER voting: 'First to a head by K'.
#     Samples continue until the lead of one outcome over the other reaches K.
#     """
#     audits: list[AuditResult] = []
    
#     # Tallies for the 'First to a head' logic
#     # We track (True votes - False votes) for each boolean
#     inc_diff = 0 
#     mis_diff = 0
    
#     inc_votes = 0
#     mis_votes = 0
#     successful_count = 0
#     total_attempts = 0

#     if verbose:
#         print(f"--- Starting MAKER Audit (K={K}) ---")

#     while total_attempts < max_samples:
#         audit = run_strict_audit(
#             client, text_to_check, system_prompt, user_prompt, temperature, verbose)
#         total_attempts += 1

#         is_error = audit.reasoning.startswith(INITIAL_ERROR_MESSAGE)
#         if is_error:
#             audits.append(audit)
#             continue # Errors don't count towards the lead in MAKER

#         successful_count += 1
#         audits.append(audit)

#         # Update Incorrect Markings Tally
#         if audit.has_incorrect_markings:
#             inc_votes += 1
#             inc_diff += 1
#         else:
#             inc_diff -= 1

#         # Update Missing Markings Tally
#         if audit.has_missing_markings:
#             mis_votes += 1
#             mis_diff += 1
#         else:
#             mis_diff -= 1

#         if verbose:
#             print(f"Sample {total_attempts}: INC_LEAD={inc_diff}, MIS_LEAD={mis_diff}")

#         # MAKER TERMINATION CRITERIA:
#         # We stop if at least categories have reached a lead of absolute K
#         # (Meaning we are confident that the markings in the excerpt need checking)
#         if abs(inc_diff) >= K or abs(mis_diff) >= K:
#             break

#     # Determine final result based on the sign of the lead
#     # If lead is positive (+K), the answer is True. If negative (-K), the answer is False.
#     should_remove = inc_diff >= K
#     should_add = mis_diff >= K

#     error_count = total_attempts - successful_count
#     reasonings = [f"[{i+1}] {a.reasoning}" for i, a in enumerate(audits)]

#     return AuditVoteResult(
#         should_remove=should_remove,
#         should_add=should_add,
#         incorrect_tally=inc_votes,
#         missing_tally=mis_votes,
#         error_tally=error_count,
#         total_votes=total_attempts,
#         stats=(
#             f"MAKER Result: K={K} reached in {total_attempts} attempts.\n"
#             f"Final Leads: Incorrect={inc_diff}, Missing={mis_diff}\n"
#             f"Successes: {successful_count}, Errors: {error_count}\n\n"
#             f"Reasonings:\n" + "\n\n".join(reasonings)
#         )
#     )

#| export
def run_audit_voting_maker(
        client: OpenAI,
        text_to_check: str,
        K: int = 3,
        max_samples: int = 15,
        verbose: bool = True,
        system_prompt: str = DEF_NOTAT_VERIFY_SYSTEM_PROMPT,
        user_prompt: str = DEF_NOTAT_VERIFY_USER_PROMPT,
        temperature: float = 0.7,
        ) -> AuditVoteResult:
    
    audits: list[AuditResult] = []
    
    # Leads for independent categories
    inc_diff = 0 
    mis_diff = 0
    
    # Status flags to stop updating a category once it reaches K
    inc_settled = False
    mis_settled = False

    inc_votes = 0
    mis_votes = 0
    successful_count = 0
    total_attempts = 0

    if verbose:
        print(f"--- Starting Independent MAKER Audit (K={K}) ---")

    while total_attempts < max_samples:
        # Check if both types have reached confidence
        if inc_settled and mis_settled:
            break

        audit = run_strict_audit(
            client, text_to_check, system_prompt, user_prompt, temperature, verbose)
        total_attempts += 1

        if audit.reasoning.startswith(INITIAL_ERROR_MESSAGE):
            audits.append(audit)
            continue 

        successful_count += 1
        audits.append(audit)

        # Update Incorrect Markings Tally ONLY if not yet settled
        if not inc_settled:
            if audit.has_incorrect_markings:
                inc_votes += 1
                inc_diff += 1
            else:
                inc_diff -= 1
            
            if abs(inc_diff) >= K:
                inc_settled = True
                if verbose: print(f"-> Incorrect Markings SETTLED at lead {inc_diff}")

        # Update Missing Markings Tally ONLY if not yet settled
        if not mis_settled:
            if audit.has_missing_markings:
                mis_votes += 1
                mis_diff += 1
            else:
                mis_diff -= 1
            
            if abs(mis_diff) >= K:
                mis_settled = True
                if verbose: print(f"-> Missing Markings SETTLED at lead {mis_diff}")

        if verbose:
            print(f"Sample {total_attempts}: INC_LEAD={inc_diff} (S:{inc_settled}), MIS_LEAD={mis_diff} (S:{mis_settled})")

    # Determine final results based on locked leads
    should_remove = inc_diff >= K
    should_add = mis_diff >= K

    error_count = total_attempts - successful_count
    reasonings = [f"[{i+1}] {a.reasoning}" for i, a in enumerate(audits)]

    return AuditVoteResult(
        should_remove=should_remove,
        should_add=should_add,
        incorrect_tally=inc_votes,
        missing_tally=mis_votes,
        error_tally=error_count,
        total_votes=total_attempts,
        stats=(
            f"MAKER Result: K={K} reached in {total_attempts} attempts.\n"
            f"Final Leads: Incorrect={inc_diff}, Missing={mis_diff}\n"
            f"Settled: Inc={inc_settled}, Mis={mis_settled}\n"
            f"Reasonings:\n" + "\n\n".join(reasonings)
        )
    )
